<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>etcd_raft源码解析-01概述</title>
    <url>/2021/08/31/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/01%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<p><em>本文以及接下来的一系列文章主要就是分析etcd&#x2F;raft的代码。本文的主要内容，主要来自于etcd&#x2F;raft&#x2F;README.md，以及知乎上的<a href="https://zhuanlan.zhihu.com/p/54846720">《etcd raft 设计与实现》</a>一文。</em></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><code>etcd/raft</code>库是Raft算法的开源实现。该Raft库稳定且完备，目前已广泛应用到各种开源软件中，比如cockroachdb、dgraph、etcd、tikv等等。</p>
<p>大部分Raft实现都具有大而全的设计，其中囊括了存储，消息序列化，网络传输等等基础组件。而<code>etcd/raft</code>库采用简约的设计原则，只实现了Raft算法的核心部分，而像网络&#x2F;磁盘IO这种的实现留给了库的使用者实现。因此使用该库时，必须自己实现Raft节点间消息交互的传输层，以及持久化Raft日志和状态的存储层。</p>
<span id="more"></span>
<p><code>etcd/raft</code>库基本实现了论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》中提到的核心算法和大部分扩展特性，并在此基础上做了一些工程实践上必要的优化。该库实现的Raft特性有：</p>
<ul>
<li><code>Leader</code>选举；</li>
<li>日志复制；</li>
<li>日志压缩；</li>
<li>成员变更；</li>
<li>领导权转移；</li>
<li>可由<code>Leader</code>和<code>Follower</code>提供的高效的线性一致性读服务：<ul>
<li><code>Leader</code>在处理读请求之前会检查是否被分区隔离，并且处理读操作时绕过了写日志的过程；</li>
<li><code>Follower</code>处理读请求时会向Leader询问安全的readIndex；</li>
</ul>
</li>
<li>由<code>Leader</code>和<code>Follower</code>提供的更高效的基于租约的线性一致性读服务：<ul>
<li><code>Leader</code>处理读请求时绕过了写日志的操作（同时也省略了检查是否被分区隔离的过程）；</li>
<li><code>Follower</code>处理读请求时会向Leader询问安全的<code>readIndex</code>；</li>
<li>该方法需要集群节点上clock drift有明确的界限；</li>
</ul>
</li>
</ul>
<p><code>etcd/raft</code>库在实现了上面的Raft核心算法的基础上，做了如下优化：</p>
<ul>
<li>Pipeline：Leader 向 Follower 发送日志条目可以 pipeline 发送的（相对的 ping-pong 模式发送和接收），减少日志复制时的延迟（pipeline 是grpc的一重要特性）；</li>
<li>日志复制的流量控制；</li>
<li>Raft网络消息批量处理，以减少网络I&#x2F;O同步调用；</li>
<li>日志条目批量保存，以减少Disk同步IO；</li>
<li>Writing to leader’s disk in parallel；</li>
<li>发给Follower的提案消息会转发给Leader；</li>
<li><code>Leader</code>被分区隔离时会自动转为Follower；</li>
<li>当<code>Leader</code>被分区隔离时，防止其内部的日志条目无限的增长（Protection against unbounded log growth when quorum is lost）；</li>
</ul>
<hr>
<h3 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h3><p>为了保证确定性，<code>etcd/raft</code>库将Raft实现为一个状态机。该状态机以消息作为输入，消息既可以来自于本地的定时器，也可以来自于其他节点发来的消息。状态机的输出是一个<code>Ready</code>结构。下面是<code>etcd/raft</code>库以及外部基础组件形成的架构：</p>
<img src="/img/01整体框架/v2-ef297c4a45c3d20daff24e49b79b5e41_720w.jpg" alt="img" style="zoom: 40%;" />

<h4 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h4><p>图中最上面一层是网络层，负责使用 Grpc 收发 <code>etcd/raft</code> 和 <code>Client</code> 的各种消息， <code>etcd/raft</code>会通过网络层收发各种消息，包括日志追加消息、投票消息、<code>Client</code>发来的各种请求消息，响应消息等等，这些都是由 Grpc 的 Coroutine 完成。</p>
<h4 id="持久化层"><a href="#持久化层" class="headerlink" title="持久化层"></a>持久化层</h4><p>图中最下面一层是持久化层，其提供了对 Raft 各种数据的持久化存储：</p>
<ul>
<li>WAL：持久化Raft日志条目；</li>
<li>Snapshot：持久化Raft快照（snapshot）；</li>
<li>KV： <code>Etcd</code> 是一个分布式的KV存储，所以，对Raft来说，applied的数据就是写入到 KV 中。</li>
</ul>
<h4 id="Raft-层"><a href="#Raft-层" class="headerlink" title="Raft 层"></a>Raft 层</h4><p>中间这一层就是 Raft 层，也就是<code>etcd/raft</code>的核心实现。其核心的思路就是将Raft所有算法逻辑实现封装成一个状态机，也就是图中的 <code>raft StateMachine</code>。类似网络处理实现中也会通过一个状态机来实现网络消息的异步处理。这个状态机中，不包含任何的网络、持久化等逻辑，给其指定的输入，就能驱动<code>raft状态机</code>运转得到相应的输出。</p>
<p>以 <code>Client</code> 发起 一个 put kv 的请求为例，来看看<code>raft状态机</code>的输入、运转和输出，这里从<code>Leader</code>的视角，分为如下阶段：</p>
<ul>
<li>第一阶段：<code>Client</code>发送一个put kv请求给<code>Etcd server</code>，<code>Grpc server</code>解析后，生成一个提案消息作为<code>raft状态机</code>的输入，驱动<code>raft状态机</code>运转，就会生成两个输出，一个需要写入 WAL 的日志条目，另外一个就是要发给其他节点的日志追加消息，这些输出都会封装在 <code>Ready</code> 结构中；</li>
<li>第二阶段：如果<code>Etcd</code>把第一阶段的输出，即<code>Ready</code>结构中的日志条目写到了盘上，并且把日志追加消息发送给了其他节点，那么其他节点就会收到日志追加消息，持久化之后就会给 <code>Leader</code> 返回日志追加的响应消息，<code>Etcd server</code> 收到响应之后，依然作为输入交给<code>raft状态机</code>处理，驱动状态机运转。如果得到了过半节点的响应，那么就会产生相应的输出，即已经提交的日志条目，该输出也封装在<code>Ready</code>结构中传递给<code>Etcd</code>；</li>
<li>第三阶段：<code>Etcd</code>得到上面<code>raft状态机</code>输出的已提交条目后，应用到状态机，然后就可以返回<code>Client</code> put kv 成功的响应了。</li>
</ul>
<p>下面介绍的raft层的三个小模块就是完成这些事情的：</p>
<ul>
<li><p><code>raft StateMachine</code>：就是一个Raft算法的逻辑实现，其输入统一被抽象成了消息，输出则统一封装在<code>Ready</code>结构中；</p>
</li>
<li><p><code>node</code>：该模块提供了如下功能：</p>
</li>
<li><p>提供<code>raft状态机</code>和外界交互的接口，供外部应用，即<code>Etcd</code>向<code>raft状态机</code>提交请求。以上面的put kv请求为例，就是通过<code>node.Propose</code>接口向<code>raft状态机</code>提交一个提案，这个接口就是将外部请求转换成<code>raft状态机</code>认识的提案消息，并通过 Channel 传递给驱动<code>raft状态机</code>运转的 Coroutine；</p>
<ul>
<li>提供驱动<code>raft状态机</code>运转的 Coroutine，其负责监听所有消息 Channel，一旦收到消息就会调用<code>raft状态机</code>处理消息的接口 <code>raft.Step</code> ，并接收<code>raft状态机</code>输出的 <code>Ready</code> 结构，将其通过 Channel 发给外部 Coroutine 处理；</li>
</ul>
</li>
<li><p><code>raftNode</code>（实际上已经不属于<code>etcd/raft</code>库的模块）：该模块会有一个 Coroutine，负责处理<code>raft状态机</code>输出的 <code>Ready</code> 结构。即将其中需要持久化保存的状态进行持久化保存，将需要发出去的消息发给相应节点，将需要应用的日志条目apply到复制状态机。</p>
</li>
</ul>
<hr>
<h3 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h3><p>接下来按照如下的顺序分析<code>etcd/raft</code>的代码：</p>
<ul>
<li><code>quorum</code>包：其中包含三个文件：<code>majority.go, quorum.go joint.go</code>。该包主要实现了基于当前配置的节点集合计算出集群最大<code>CommitIndex</code>和投票结果的功能；</li>
<li><code>tracker</code>包：主要实现了节点日志追加和节点配置相关的底层逻辑；</li>
<li><code>raft</code>包中的<code>storage.go</code>：定义了表示持久化状态的<code>Storage</code>接口，以及该接口的一个实现<code>MemoryStorage</code>；</li>
<li><code>raft</code>包中的<code>log_unstable.go</code>：定义了<code>unstable</code>结构，对应于<code>storage</code>结构，表示尚未保存到持久化存储中（即<code>storage</code>）的快照和日志信息；</li>
<li><code>raft</code>包中的<code>log.go</code>定义了<code>raftLog</code>结构体，实现了<code>RAFT</code>中日志追加相关的底层功能，<code>raftLog</code>结构主要就是封装了<code>Storage</code>和<code>unstable</code>，并对外提供相应的接口。</li>
<li><code>raft</code>包中的<code>node.go</code>和<code>rawnde.go</code>：对应于上面架构中的<code>node</code>模块，提供<code>raft状态机</code>的对外接口和驱动 Coroutine；</li>
<li>剩下的就是Raft算法的具体流程了，包括：<ul>
<li>选举和状态转换：Raft选举算法，以及<code>Leader</code>，<code>Follower</code>，<code>Candidate</code>的状态转换；</li>
<li>日志追加：<code>Leader</code>向<code>Follower</code>追加日志的流程；</li>
<li>日志压缩：snapshot的生成、传输和使用；</li>
<li>成员变更：底层支撑组件<code>confchange</code>包，以及成员变更流程；</li>
<li>CheckQuorum和心跳消息处理：CheckQuorum机制，心跳处理流程；</li>
<li>处理读请求：Raft实现读请求处理的<code>ReadIndex</code>和<code>LeaseRead</code>流程；</li>
<li>领导权转移：转移领导权流程；</li>
<li><code>Ready</code>的处理：<code>Ready</code>的产生、输出和应用；</li>
<li>节点初始化以及剩余细节；</li>
</ul>
</li>
</ul>
<hr>
<h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><p>《etcd&#x2F;raft&#x2F;README.md》</p>
<p><a href="https://zhuanlan.zhihu.com/p/54846720">https://zhuanlan.zhihu.com/p/54846720</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/54970068">https://zhuanlan.zhihu.com/p/54970068</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-03tracker包</title>
    <url>/2021/09/21/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/03tracker%E5%8C%85/</url>
    <content><![CDATA[<p><code>tracker</code>包主要实现了节点日志追加和节点配置相关的底层逻辑。其中主要实现了三个数据结构：</p>
<ul>
<li><code>Inflights</code>结构：实现节点日志追加的流量控制；</li>
<li><code>Progress</code>结构：记录节点追加日志的“进度”；</li>
<li><code>ProgressTracker</code>结构：实现配置变更和日志追加的主要数据结构；</li>
</ul>
<hr>
<h3 id="Inflights结构"><a href="#Inflights结构" class="headerlink" title="Inflights结构"></a>Inflights结构</h3><p><code>tracker</code>包中的<code>Inflights</code>结构用于<code>Leader</code>对某个<code>Follower</code>发起的<code>pb.MsgApp</code>（日志追加消息）做流量控制。其中记录了所有已发出，但未收到回复的日志条目索引信息，该结构主要针对的是<code>Progress</code>处于<code>StateReplicate</code>状态的节点。</p>
<ul>
<li><code>Leader</code>在发送<code>pb.MsgApp</code>消息之前，必须调用<code>Inflights.Full</code>方法，判断是否可以继续发送消息；</li>
<li>发送<code>pb.MsgApp</code>消息时必须调用<code>Inflights.Add</code>方法记录最后一条日志条目的Index；收到<code>pb.MsgAppResp</code>响应消息后调用<code>Inflights.FreeLE</code>接口更新记录信息。<span id="more"></span></li>
</ul>
<h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Inflights <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">// start就是buffer这个循环队列中首元素的索引</span></span><br><span class="line">	start <span class="type">int</span></span><br><span class="line">	<span class="comment">// count表示buffer中元素的个数</span></span><br><span class="line">	count <span class="type">int</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// size表示buffer能包含的元素最大个数，它与len(buffer)有关系，但不完全相同</span></span><br><span class="line">	size <span class="type">int</span></span><br><span class="line">    <span class="comment">//buffer被当做一个循环队列。buffer中的元素，记录的是发出的每个MsgApp消息中最后一条AppendEntry的索引。</span></span><br><span class="line">	buffer []<span class="type">uint64</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>Inflights</code>中比较有迷惑性的就是size成员：</p>
<ul>
<li>它在创建<code>Inflights</code>时指定，之后就再也不能更改；</li>
<li>如果创建时，<code>size &gt; len(buffer)</code>，则随着消息的发送，<code>buffer</code>就需要扩容，这就需要调用<code>grow</code>方法，而该方法保证<code>len(buffer)</code>的最大值就是<code>size</code>。</li>
<li>如果创建时，<code>size &lt; len(buffer)</code>，则<code>buffer</code>中<code>size</code>之外的空间根本就用不到；</li>
</ul>
<hr>
<h4 id="函数和方法"><a href="#函数和方法" class="headerlink" title="函数和方法"></a>函数和方法</h4><h5 id="NewInflights"><a href="#NewInflights" class="headerlink" title="NewInflights"></a>NewInflights</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewInflights</span><span class="params">(size <span class="type">int</span>)</span></span> *Inflights</span><br></pre></td></tr></table></figure>

<p>创建新的<code>Inflights</code>，只指定了其中的<code>size</code>属性；</p>
<hr>
<h5 id="Clone"><a href="#Clone" class="headerlink" title="Clone"></a>Clone</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> Clone() *Inflights</span><br></pre></td></tr></table></figure>

<p>根据<code>in</code>，复制一个完全相同的新<code>Inflights</code>，新<code>Inflights</code>与<code>in</code>不会有共享内存的情况。</p>
<p><font color=red>注：</font>这里复制<code>buffer</code>时，使用的是这种技巧：<code>ins.buffer = append([]uint64(nil), in.buffer...)</code></p>
<hr>
<h5 id="Add"><a href="#Add" class="headerlink" title="Add"></a>Add</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> Add(inflight <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p><code>Leader</code>向<code>Follower</code>发送<code>pb.MsgApp</code>消息时，在<code>raft.maybeSendAppend</code>方法中，如果该<code>Follower</code>对应的<code>Progress</code>状态是<code>StateReplicate</code>，则会调用该接口。</p>
<p>调用时的参数是<code>Leader</code>要发送的日志中，最后一个日志条目的<code>Index</code>。该方法的逻辑是：</p>
<ul>
<li>该接口内部调用<code>Inflights.Full()</code>如果返回<code>true</code>，则直接<code>panic</code>；</li>
<li>要插入的位置<code>next</code>就是<code>start+count</code>，如果<code>next</code>超过了<code>size</code>，则将<code>next -= size</code>，最后将<code>count++</code>;</li>
<li>如果<code>next</code>大于等于<code>len(in.buffer)</code>，则需要调用<code>grow</code>进行扩容；</li>
</ul>
<hr>
<h5 id="grow"><a href="#grow" class="headerlink" title="grow"></a>grow</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> grow()</span><br></pre></td></tr></table></figure>

<p>对<code>buffer</code>进行扩容。但是保证<code>len(buffer)</code>小于等于<code>size</code>。</p>
<hr>
<h5 id="FreeLE和FreeFirstOne"><a href="#FreeLE和FreeFirstOne" class="headerlink" title="FreeLE和FreeFirstOne"></a>FreeLE和FreeFirstOne</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> FreeLE(to <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> FreeFirstOne()</span><br></pre></td></tr></table></figure>

<p>在<code>stepLeader</code>回调函数中，<code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgApp</code>的成功响应消息，即<code>pb.MsgAppResp</code>消息后，并且当前该<code>Follower</code>节点对应的<code>Progress</code>状态是<code>traceker.StateReplicate</code>时，调用<code>Inflights.FreeLE</code>方法，参数是消息中的 Index属性；</p>
<p><code>FreeLE</code>中的<code>LE</code>表示<code>Less Equal</code>。该接口就是当收到<code>MsgApp</code>对于<code>AppendEntry</code>索引为<code>to</code>的回复时，调用该接口，将<code>buffer</code>中，所有小于等于<code>to</code>的元素清除（这里不会对实际元素做任何改动，只是修改<code>start</code>），并且调整<code>start</code>, <code>count</code>。</p>
<p><code>FreeFirstOne</code>就是调用<code>FreeLE</code>，将<code>start</code>元素排除。在<code>stepLeader</code>回调函数中，Leader收到Follower对于心跳消息的回复，即<code>pb.MsgHeartbeatResp</code>消息后，如果该Follower对应的<code>Progress</code>状态是<code>traceker.StateReplicate</code>，并且该节点对应的<code>Inflights.Full</code>为true，则调用该方法，释放一个<code>slot</code>；<font color=red>奇怪的点</font>，这个写法可以追溯到<code>commit: 4a64373</code>，即首次添加<code>Inflights</code>时就存在了；</p>
<hr>
<h5 id="Full"><a href="#Full" class="headerlink" title="Full"></a>Full</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> Full() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>根据<code>count</code>是否等于<code>size</code>，判断<code>Inflights</code>是否已满，不许再发新消息了；</p>
<p>在<code>raft.maybeSendAppend</code>方法，即<code>Leader</code>向<code>Follower</code>发送<code>pb.MsgApp</code>消息时调用，如果该方法返回<code>true</code>，则不会发送<code>pb.MsgApp</code>消息；</p>
<hr>
<h5 id="Count"><a href="#Count" class="headerlink" title="Count"></a>Count</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> Count() <span class="type">int</span></span><br></pre></td></tr></table></figure>

<p>返回<code>count</code>，即当前<code>buffer</code>中记录的<code>MsgApp</code>消息个数；</p>
<hr>
<h5 id="reset"><a href="#reset" class="headerlink" title="reset"></a>reset</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(in *Inflights)</span></span> reset()</span><br></pre></td></tr></table></figure>

<p>将<code>count</code>和<code>start</code>重置为<code>0</code>。</p>
<hr>
<h4 id="附注"><a href="#附注" class="headerlink" title="附注"></a>附注</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line">	smallSizeIn := &amp;Inflights&#123;</span><br><span class="line">		size: <span class="number">5</span>,</span><br><span class="line">		count: <span class="number">0</span>,</span><br><span class="line">		start: <span class="number">0</span>,</span><br><span class="line">		buffer: <span class="built_in">make</span>([]<span class="type">uint64</span>, <span class="number">10</span>),</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">5</span>; i++ &#123;</span><br><span class="line">		smallSizeIn.Add(<span class="type">uint64</span>(i))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	smallSizeIn.FreeLE(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">	smallSizeIn.Add(<span class="number">6</span>)</span><br><span class="line">	smallSizeIn.Add(<span class="number">7</span>)</span><br><span class="line">	</span><br><span class="line"><span class="comment">//此时smallSizeIn是 &amp;&#123;start:4 count:3 size:5 buffer:[6 7 2 3 4 0 0 0 0 0]&#125;</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Progress结构"><a href="#Progress结构" class="headerlink" title="Progress结构"></a>Progress结构</h3><p><code>tracker</code>包中的<code>Progress</code>结构记录了节点追加日志的“进度”，集群中的每个节点（<code>Leader</code>，<code>Follower</code>，<code>Learner</code>）都有对应的<code>Progress</code>结构。<code>Leader</code>会根据<code>Progress</code>里的信息来决定每次同步给<code>Follower</code>（包括<code>Learner</code>）的日志项。<code>Progresss</code>本身也可以看做是一个状态机，他有三种状态：</p>
<ul>
<li><code>StateProbe</code>：这表示该<code>Follower</code>的<code>lastIndex</code>尚不明确，需要探测。<code>Follower</code>对应的<code>Progress</code>处于这种状态时，针对该<code>Follower</code>，<code>Leader</code>只能允许一个<code>pb.MsgApp</code>消息处于”Inflight”状态（已发送但尚未收到回复的状态），收到<code>pb.MsgApp</code>的响应消息后根据其返回信息确定下一次要发的<code>pb.MsgApp</code>消息；</li>
<li><code>StateReplicate</code>：这种状态下，<code>Leader</code>会批量的（<code>pipeline</code>方式）发送<code>pb.MsgApp</code>消息给<code>Follower</code>；这样就能尽快的将日志复制到<code>Follower</code>中；</li>
<li><code>StateSnapshot</code>：这种状态下，<code>Leader</code>会发送<code>snpashot</code>给<code>Follower</code>；</li>
</ul>
<h4 id="数据结构-1"><a href="#数据结构-1" class="headerlink" title="数据结构"></a>数据结构</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Progress <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//Match表示已经复制到Follower的日志最大索引；</span></span><br><span class="line">	<span class="comment">//Next表示下一次要Append到Follower的日志起始索引，StateProbe状态下Next一般等于Match + 1；</span></span><br><span class="line">    <span class="comment">//StateReplicate状态下Next可能会大于Match+1，因为StateReplicate状态下可以有多个pb.MsgApp消息处于</span></span><br><span class="line">    <span class="comment">//&quot;Inflight&quot;状态;</span></span><br><span class="line">	Match, Next <span class="type">uint64</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment">//Progress当前状态</span></span><br><span class="line">	State StateType</span><br><span class="line"></span><br><span class="line">	<span class="comment">// PendingSnapshot用于StateSnapshot状态，表示Snapshot中最后日志条目的索引</span></span><br><span class="line">	PendingSnapshot <span class="type">uint64</span></span><br><span class="line">	</span><br><span class="line">    <span class="comment">//节点加入集群时在Changer.initProgress初始化节点对应的Progress结构时，该属性被置为了true；</span></span><br><span class="line">    <span class="comment">//Leader如果能从Follower处收到pb.MsgAppResp或pb.MsgHeartbeatResp响应消息时，该属性会被置为true；</span></span><br><span class="line">    <span class="comment">//Leader每隔一段时间就会进行CheckQuorum检查，即检查这段时间内集群中是否Majority的节点的</span></span><br><span class="line">    <span class="comment">//RecentActive为true（即当前Leader发出的消息是能得到响应的），在CheckQuorum流程的最后，会把所有节点的</span></span><br><span class="line">    <span class="comment">//Progress.RecentActive置为false，以便为下次CheckQuorum做准备；</span></span><br><span class="line">	RecentActive <span class="type">bool</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//该字段用于StateProbe状态。表示是否已经发送了AppendEntry消息，并且尚未收到回复;</span></span><br><span class="line">    <span class="comment">//ProbeSent为true的情况下就不能继续发送AppendEntry消息</span></span><br><span class="line">	ProbeSent <span class="type">bool</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//用于流量控制</span></span><br><span class="line">	Inflights *Inflights</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 表示Follower是否是Learner</span></span><br><span class="line">	IsLearner <span class="type">bool</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="函数和方法-1"><a href="#函数和方法-1" class="headerlink" title="函数和方法"></a>函数和方法</h4><h5 id="ResetState"><a href="#ResetState" class="headerlink" title="ResetState"></a>ResetState</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> ResetState(state StateType) </span><br></pre></td></tr></table></figure>

<p><code>Progress</code>进行状态转换时（即<code>Progress.BecomeProbe</code>、<code>Progress.BecomeReplicate</code>、<code>Progress.BecomeSnapshot</code>方法中）调用本方法。该函数设置<code>State</code>属性，同时重置<code>ProbeSent、PendingSnapshot</code>和<code>Inflights</code>为<code>0</code>或者<code>false</code>；</p>
<hr>
<h5 id="ProbeAcked"><a href="#ProbeAcked" class="headerlink" title="ProbeAcked"></a>ProbeAcked</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> ProbeAcked()</span><br></pre></td></tr></table></figure>

<p>当<code>Leader</code>自身追加日志时，或者<code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgApp</code>消息的成功回复时，就会调用调用本方法。该函数将<code>ProbeSent</code>置为<code>false</code>；</p>
<hr>
<h5 id="BecomeProbe"><a href="#BecomeProbe" class="headerlink" title="BecomeProbe"></a>BecomeProbe</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> BecomeProbe()</span><br></pre></td></tr></table></figure>

<p>状态转移到<code>StateProbe</code>，并设置<code>Next</code>属性。该函数的逻辑是：</p>
<ul>
<li>调用<code>Progress.ResetState</code>重置状态；</li>
<li>如果当前状态为<code>StateSnapshot</code>，则将<code>Next</code>置为<code>max(pr.Match+1, pendingSnapshot+1)</code>；</li>
<li>否则，将<code>Next</code>置为<code>Match+1</code>；</li>
</ul>
<p>该方法的调用时机是：</p>
<ul>
<li>在<code>stepLeader</code>函数中，<code>Leader</code>收到<code>Follower</code>关于<code>pb.MsgApp</code>消息的失败回复，并且该失败回复不是过期消息（即<code>Progress.MaybeDecrTo</code>返回<code>true</code>），并且当前状态是<code>StateReplicate</code>，则调用该方法将状态转为<code>StateProbe</code>；</li>
<li>在<code>stepLeader</code>函数中，<code>Leader</code>收到<code>Follower</code>关于<code>pb.MsgApp</code>消息的成功回复后，如果该<code>Follower</code>对应的<code>Progress</code>当前状态为<code>StateSnapshot</code>，则先调用本方法将状态转为<code>StateProbe</code>，然后调用<code>Progress.BecomeReplicate</code>方法将状态转为<code>StateReplicate</code>；</li>
<li>在<code>stepLeader</code>函数中，<code>Leader</code>如果收到<code>pb.MsgUnreachable</code>消息，并且该<code>Follower</code>对应的<code>Progress</code>当前状态为<code>StateReplicate</code>，则调用该方法将状态转为<code>StateProbe</code>；<font color=red><code>pb.MsgUnreachable</code>消息</font>表示<code>Leader</code>向<code>Follower</code>发送日志条目或snapshot时出现了问题，外部模块会调用<code>node.ReportUnreachable</code>通知<code>etcd/raft</code>，在<code>stepLeader</code>回调函数中，收到该消息后，如果<code>Follower</code>对应的<code>Progress</code>正处于<code>StateReplicate</code>状态，则调用<code>Progress.BecomeProbe</code>转为<code>StateProbe</code>状态；</li>
<li>在<code>stepLeader</code>函数中，<code>Leader</code>如果收到<code>pb.MsgSnapStatus</code>消息，即快照在网络上的发送结果，不管快照是否发送成功，都会调用该方法将状态转为<code>StateProbe</code>；</li>
</ul>
<hr>
<h5 id="BecomeReplicate"><a href="#BecomeReplicate" class="headerlink" title="BecomeReplicate"></a>BecomeReplicate</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> BecomeReplicate()</span><br></pre></td></tr></table></figure>

<p>状态转移到<code>StateReplicate</code>，设置<code>Next</code>为<code>Match+1</code>。该函数的调用时机是：</p>
<ul>
<li>在<code>raft.becomeLeader</code>方法中，即<code>Follower</code>转为<code>Leader</code>之时，对于自身对应的<code>Progress</code>结构会调用本方法，即<code>Leader</code>本身都是处于<code>StateReplicate</code>的；</li>
<li>在<code>stepLeader</code>函数中，<code>Leader</code>收到<code>Follower</code>关于<code>pb.MsgApp</code>消息的成功回复后，如果该<code>Follower</code>对应的<code>Progress</code>当前状态为<code>StateProbe</code>，则调用本方法；</li>
<li>在<code>stepLeader</code>函数中，<code>Leader</code>收到<code>Follower</code>关于<code>pb.MsgApp</code>消息的成功回复后，如果该<code>Follower</code>对应的<code>Progress</code>当前状态为<code>StateSnapshot</code>，则先调用<code>Progress.BecomeProbe</code>方法将状态转为<code>StateProbe</code>，然后调用本方法将状态转为<code>StateReplicate</code>；</li>
</ul>
<hr>
<h5 id="BecomeSnapshot"><a href="#BecomeSnapshot" class="headerlink" title="BecomeSnapshot"></a>BecomeSnapshot</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> BecomeSnapshot(snapshoti <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>状态转移到<code>StateSnapshot</code>，并且将<code>PendingSnapshot</code>设置为<code>snapshoti</code>，即<code>snapshot</code>最后日志条目的索引。</p>
<p>该函数的调用时机是：</p>
<ul>
<li><code>Leader</code>在向<code>Follower</code>追加日志条目时，即在<code>raft.maybeSendAppend</code>方法中，需要根据<code>Follower</code>对应的<code>Progress.Next</code>，调用<code>r.raftLog.term(pr.Next - 1)</code>获取<code>preLogTerm</code>，调用<code>r.raftLog.entries(pr.Next, r.maxMsgSize)</code>获取日志条目。如果<code>raftLog.term</code>或<code>raftLog.entries</code>方法返回的err不为<code>nil</code>，说明<code>pr.Next-1</code>对应的日志条目在本地日志中已经找不到了，已经压缩到快照中去了，所以此时会调用<code>Progress.BecomeSnapshot</code>转为<code>StateSnapshot</code>状态，并向<code>Follower</code>发送<code>snapshot</code>；</li>
</ul>
<hr>
<h5 id="MaybeUpdate"><a href="#MaybeUpdate" class="headerlink" title="MaybeUpdate"></a>MaybeUpdate</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> MaybeUpdate(n <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法尝试更新<code>Progress.Match</code>和<code>Progress.Next</code>；</p>
<p>当<code>Leader</code>收到<code>Follower</code>关于<code>pb.MsgApp</code>消息的成功回复时，或者<code>Leader</code>自己追加完本地日志时调用该函数：</p>
<ul>
<li><code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgApp</code>消息的成功回复时，参数<code>n</code>就表示<code>Follower</code>追加日志成功后的<code>lastIndex</code>，</li>
<li><code>Leader</code>追加本地日志时，<code>n</code>表示追加日志后最新的<code>lastIndex</code>；</li>
</ul>
<p>正常情况下，<code>Progress.Match</code>小于<code>n</code>，因此将<code>Progress.Match</code>更新为<code>n</code>，调用<code>ProbeAcked</code>将<code>ProbeSent</code>置为<code>false</code>；然后将<code>Progress.Next</code>更新为<code>max(Progress.Next, n+1)</code>（如果处于<code>StateReplicate</code>状态，因有多个<code>pb.MsgApp</code>消息处于”Inflight”状态，所以一般<code>n</code>要小于<code>Progress.Next</code>），并返回<code>true</code>。如果<code>n</code>小于等于<code>Progress.Match</code>，说明这是过期消息，返回<code>false</code>；</p>
<hr>
<h5 id="OptimisticUpdate"><a href="#OptimisticUpdate" class="headerlink" title="OptimisticUpdate"></a>OptimisticUpdate</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> OptimisticUpdate(n <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该函数直接将<code>Progress.Next</code>置为<code>n+1</code>。</p>
<p>在<code>raft.maybeSendAppend</code>方法中，<code>Leader</code>向<code>Follower</code>发送<code>pb.MsgApp</code>消息时，如果<code>Follower</code>对应的<code>Progress</code>处于<code>StateReplicate</code>状态，调用该方法。其中的参数<code>n</code>为此次要发送的<code>pb.MsgApp</code>消息中最后一条日志条目的Index，后续在未收到<code>pb.MsgApp</code>消息的响应时，也能根据<code>Progress.Next</code>继续发送新的<code>pb.MsgApp</code>消息。所以，处于<code>StateReplicate</code>状态时，Index在<code>(Progress.Match, Progress.Next)</code>之前的日志都处于”Inflight”状态。</p>
<hr>
<h5 id="MaybeDecrTo"><a href="#MaybeDecrTo" class="headerlink" title="MaybeDecrTo"></a>MaybeDecrTo</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> MaybeDecrTo(rejected, matchHint <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>当<code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgApp</code>消息的失败回复时，调动该函数，用于设置<code>Progress.Next</code>。</p>
<ul>
<li><code>rejected</code>表示<code>Follower</code>对于哪条日志的失败回复，实际上就是<code>pb.MsgApp</code>消息中的<code>preLogIndex</code>字段，<code>matchHint</code>是<code>Leader</code>在<code>pb.MsgApp</code>消息的失败回复中的<code>RejectHint</code>基础上，调用<code>raftLog.findConflictByTerm</code>根据自己的日志匹配出的下一次要发送的日志条目的<code>preIndex</code>，它用于设置<code>Next</code>。</li>
<li>该函数返回<code>false</code>，表示<code>rejected</code>是过期消息，<code>Leader</code>不会继续处理；返回<code>true</code>，则<code>Leader</code>会尝试继续发送<code>pb.MsgApp</code>消息；</li>
</ul>
<p>该函数的逻辑：</p>
<ul>
<li>如果当前状态为<code>StateReplicate</code>，如果<code>rejected</code>小于等于<code>Match</code>，说明是过期消息，直接返回<code>false</code>；如果不是过期消息，则将<code>Next</code>置为<code>Match+1</code>。这里根据注释，实际上用<code>matchHint</code>，而非<code>Match</code>应该更好。最后返回<code>true</code>；</li>
<li>剩下的逻辑表示状态为<code>StateProbe</code>，如果<code>rejected</code>不是<code>Next-1</code>，说明是过期消息，直接返回<code>false</code>；如果不是过期消息，则将<code>Next</code>置为<code>max(min(rejected, matchHint+1), 1)</code>，将<code>ProbeSent</code>置为<code>false</code>，返回<code>true</code>；</li>
</ul>
<hr>
<h5 id="IsPaused"><a href="#IsPaused" class="headerlink" title="IsPaused"></a>IsPaused</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> IsPaused() <span class="type">bool</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>Leader</code>在向<code>Follower</code>追加日志条目时，即在<code>raft.maybeSendAppend</code>方法中调用该方法，判断是否需要暂停向<code>Follower</code>发送<code>pb.MsgApp</code>消息：</p>
<ul>
<li>如果状态为<code>StateProbe</code>，则返回<code>Progress.ProbeSent</code>的值；</li>
<li>如果状态为<code>StateReplicate</code>，则返回<code>Inflights.Full()</code>的值；</li>
<li>如果状态为<code>StateSnapshot</code>，则直接返回<code>true</code>；</li>
</ul>
<hr>
<h3 id="ProgressTracker结构"><a href="#ProgressTracker结构" class="headerlink" title="ProgressTracker结构"></a>ProgressTracker结构</h3><p><code>tracker</code>包中的<code>tracker.go</code>主要实现了支撑配置变更和日志追加的底层结构<code>Config</code>和<code>ProgressTracker</code>。</p>
<h4 id="Config结构"><a href="#Config结构" class="headerlink" title="Config结构"></a>Config结构</h4><p><code>Config</code>表示系统中节点配置相关的属性，它的定义如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Config <span class="keyword">struct</span> &#123;</span><br><span class="line">    <span class="comment">//表示系统当前的Voters节点集合。为了适配Joint Consensus配置变更算法，所以这里使用</span></span><br><span class="line">    <span class="comment">//quorum.JointConfig表示系统中的Voters节点; JointConfig实际类型是[2]MajorityConfig，即表示Cold,new; </span></span><br><span class="line">    <span class="comment">//这里的节点主要是指系统中有投票权的节点，不包含Learner;</span></span><br><span class="line">	Voters quorum.JointConfig</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//AutoLeave为true，表示配置变更时，Cold,new配置会自动过渡到Cnew配置；</span></span><br><span class="line">    <span class="comment">//为false的话，则需要手动介入才会由Cold,new过渡到Cnew;</span></span><br><span class="line">	AutoLeave <span class="type">bool</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//Learners是系统中Learner节点的ID形成的集合；</span></span><br><span class="line">    <span class="comment">//不变式：Learners ∩ Voters == ∅.</span></span><br><span class="line">	Learners <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//LearnersNext用于临时存放Voters中降级为Learner的节点ID；</span></span><br><span class="line">    <span class="comment">//在节点配置变更时，在Joint Consensus过程中，被降级的Voter不能直接加入到Learners中，</span></span><br><span class="line">    <span class="comment">//否则就会破坏不变式：Learners ∩ Voters == ∅. 比如开始时的配置是：</span></span><br><span class="line">	<span class="comment">//   voters:   &#123;1 2 3&#125;,     learners: &#123;&#125;</span></span><br><span class="line">	<span class="comment">//现在要把节点3降级，进入到joint configuration状态, 所以，配置是：</span></span><br><span class="line">	<span class="comment">//   voters:   &#123;1 2&#125; &amp; &#123;1 2 3&#125;,   learners: &#123;&#125;,    next_learners: &#123;3&#125;</span></span><br><span class="line">	<span class="comment">//最后再过渡到最终状态：</span></span><br><span class="line">	<span class="comment">//   voters:   &#123;1 2&#125;,    learners: &#123;3&#125;,    next_learners: &#123;&#125;</span></span><br><span class="line">	LearnersNext <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="ProgressTracker结构-1"><a href="#ProgressTracker结构-1" class="headerlink" title="ProgressTracker结构"></a>ProgressTracker结构</h4><p><code>ProgressTracker</code>结构是实现配置变更和日志追加的主要数据结构。其中包含：</p>
<ul>
<li>集群中所有的节点配置，即<code>Config</code>结构；</li>
<li>每个节点的日志追加进度，即所有节点的<code>Progress</code>结构；</li>
<li>记录选举时的投票结果，即<code>Votes</code>这个<code>map</code>；</li>
</ul>
<p>它的定义如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ProgressTracker <span class="keyword">struct</span> &#123;</span><br><span class="line">    Config</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ProgressMap是progress.go中定义的类型：type ProgressMap map[uint64]*Progress.</span></span><br><span class="line">    <span class="comment">//该map以节点ID为key，以*Progress为value，即表示每个节点的Append情况；</span></span><br><span class="line">	Progress ProgressMap</span><br><span class="line"></span><br><span class="line">    <span class="comment">//该map以节点ID为key，表示每个节点的投票情况</span></span><br><span class="line">	Votes <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="type">bool</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//用于设置Progress中的Inflights.size</span></span><br><span class="line">	MaxInflight <span class="type">int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="函数和方法-2"><a href="#函数和方法-2" class="headerlink" title="函数和方法"></a>函数和方法</h4><h5 id="MakeProgressTracker函数"><a href="#MakeProgressTracker函数" class="headerlink" title="MakeProgressTracker函数"></a>MakeProgressTracker函数</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MakeProgressTracker</span><span class="params">(maxInflight <span class="type">int</span>)</span></span> ProgressTracker </span><br></pre></td></tr></table></figure>

<p>创建一个新的<code>ProgressTracker</code>。</p>
<p>实现中，在创建<code>ProgressTracker.Config.Voters</code>时，该数组仅仅包含一个元素<code>Voters[0]</code>，而<code>Voters[1]</code>为<code>nil</code>；</p>
<hr>
<h5 id="IsSingleton方法"><a href="#IsSingleton方法" class="headerlink" title="IsSingleton方法"></a>IsSingleton方法</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> IsSingleton() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>判断当前系统是否为单节点系统，即仅仅包含<code>Leader</code>自身。</p>
<p>该方法返回<code>len(p.Voters[0]) == 1 &amp;&amp; len(p.Voters[1]) == 0</code> 的结果；</p>
<hr>
<h5 id="matchAckIndexer类型"><a href="#matchAckIndexer类型" class="headerlink" title="matchAckIndexer类型"></a>matchAckIndexer类型</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> matchAckIndexer <span class="keyword">map</span>[<span class="type">uint64</span>]*Progress</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> _ quorum.AckedIndexer = matchAckIndexer(<span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// AckedIndex implements IndexLookuper.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l matchAckIndexer)</span></span> AckedIndex(id <span class="type">uint64</span>) (quorum.Index, <span class="type">bool</span>) &#123;</span><br><span class="line">	pr, ok := l[id]</span><br><span class="line">	<span class="keyword">if</span> !ok &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>, <span class="literal">false</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> quorum.Index(pr.Match), <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>matchAckIndexer</code>实际类型是<code>map[uint64]*Progress</code>，定义该类型主要是为了适配<code>quorum.AckedIndexer</code>接口。</p>
<p>该类型实现了<code>AckedIndexer</code>接口的<code>AckedIndex</code>方法，该方法返回特定节点的<code>Progress.Match</code>，即该节点已经<code>Append</code>成功的最大日志Index；</p>
<p>该类型是<code>tracker</code>包的私有类型，定义该类型主要是为了方便调用<code>JointConfig</code>的<code>CommittedIndex</code>方法；</p>
<hr>
<h5 id="Committed方法"><a href="#Committed方法" class="headerlink" title="Committed方法"></a>Committed方法</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> Committed() <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p><code>Leader</code>调用该函数得到集群当前Majority节点的最大<code>CommitIndex</code>。该方法就是调用<code>JointConfig.CommittedIndex</code>方法，得到<code>Voters</code>中，<code>Majority</code>节点都已<code>Append</code>成功（即<code>Progress.Match</code>）的索引最大值；</p>
<hr>
<h5 id="Visit方法"><a href="#Visit方法" class="headerlink" title="Visit方法"></a>Visit方法</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> Visit(f <span class="function"><span class="keyword">func</span><span class="params">(id <span class="type">uint64</span>, pr *Progress)</span></span>)</span><br></pre></td></tr></table></figure>

<p>该方法就是先将<code>p.Progress</code>按照<code>节点ID</code>从小到大的顺序进行排序，然后依次在每个元素上调用函数<code>f</code>。</p>
<p>实现中，为了性能优化，使用了<code>MajorityConfig.CommittedIndex</code>方法中，栈上分配<code>Slice</code>以及插入排序的方法；</p>
<hr>
<h5 id="QuorumActive方法"><a href="#QuorumActive方法" class="headerlink" title="QuorumActive方法"></a>QuorumActive方法</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> QuorumActive() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>该函数判断<code>p.Progress</code>中，是否有<code>Majority</code>个节点都是<code>RecentActive</code>的；</p>
<p>该方法通过调用<code>p.Visit</code>方法，以及<code>JointConfig.VoteResult</code>方法实现。所以<code>JointConfig.VoteResult</code>方法不止用于统计选票。</p>
<p>该方法是<code>Leader</code>进行<code>CheckQuorum</code>检查的核心函数，主要用于判断<code>Leader</code>一段时间内向所有<code>Follower</code>发出的消息是否得到了响应，如果有超过Majority的节点都响应了，则当前<code>Leader</code>继续维持其领导地位，否则说明该<code>Leader</code>被分区隔离了，该<code>Leader</code>会主动降级为<code>Follower</code>，具体可以参考《15CheckQuorum和心跳消息》。</p>
<hr>
<h5 id="VoterNodes方法"><a href="#VoterNodes方法" class="headerlink" title="VoterNodes方法"></a>VoterNodes方法</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> VoterNodes() []<span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法，按照从小到大的顺序，返回<code>p.Voters</code>中的<code>节点ID</code>形成的<code>Slice</code>；</p>
<hr>
<h5 id="LearnerNodes方法"><a href="#LearnerNodes方法" class="headerlink" title="LearnerNodes方法"></a>LearnerNodes方法</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> LearnerNodes() []<span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法，按照从小到大的顺序，返回<code>p.Learners</code>中的<code>节点ID</code>形成的<code>Slice</code>；</p>
<hr>
<h5 id="选票相关ResetVotes-RecordVote-TallyVotes"><a href="#选票相关ResetVotes-RecordVote-TallyVotes" class="headerlink" title="选票相关ResetVotes, RecordVote, TallyVotes"></a>选票相关ResetVotes, RecordVote, TallyVotes</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> ResetVotes()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> RecordVote(id <span class="type">uint64</span>, v <span class="type">bool</span>)</span><br><span class="line"><span class="number">67</span> <span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> TallyVotes() (granted <span class="type">int</span>, rejected <span class="type">int</span>, _ quorum.VoteResult)</span><br></pre></td></tr></table></figure>

<p><code>ResetVotes</code>方法用于重置<code>p.Votes</code>，以便接下来的选票统计和唱票；</p>
<p><code>RecordVote</code>方法用于将节点的投票结果记录到<code>p.Votes</code>中；</p>
<p><code>TallyVotes</code>通过调用<code>p.Voters.VoteResult</code>方法，返回投票结果，同时返回投票中的<code>granted</code>和<code>rejected</code>数量；</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-02quorum包</title>
    <url>/2021/09/02/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/02quorum%E5%8C%85/</url>
    <content><![CDATA[<p><code>quorum</code>包中包含三个文件：<code>majority.go, quorum.go joint.go</code>。该包主要实现了基于当前配置的节点集合计算出集群最大<code>CommitIndex</code>和投票结果的功能：</p>
<ul>
<li><code>majority.go</code>：定义了<code>MajorityConfig</code>类型及其方法，它实际上是个<code>节点ID</code>形成的<code>set</code>，并在此基础上实现了<code>CommittedIndex</code>（计算<code>Majority</code>节点的最大提交索引）和<code>VoteResult</code>（计算<code>Majority</code>节点的投票结果）方法；</li>
<li><code>quorum.go</code>：定义了<code>AckedIndexer</code>接口，该接口唯一的方法<code>AckedIndex</code>就是提供<code>节点ID</code>到日志<code>Index</code>的转换功能；</li>
<li><code>joint.go</code>：在<code>MajorityConfig</code>的基础上定义了<code>JointConfig</code>类型，即联合共识（Joint Consensus）节点变更算法中<code>Cold,new</code>的配置。并在此基础上实现了<code>CommittedIndex</code>和<code>VoteResult</code>方法；<span id="more"></span></li>
</ul>
<h3 id="MajorityConfig"><a href="#MajorityConfig" class="headerlink" title="MajorityConfig"></a>MajorityConfig</h3><p><code>MajorityConfig</code>实际类型是<code>map[uint64]struct&#123;&#125;</code>，即由系统中所有<code>节点ID</code>形成的<code>set</code>。</p>
<p>该类型主要实现了<code>CommittedIndex</code>和<code>VoteResult</code>方法。</p>
<h4 id="CommittedIndex方法"><a href="#CommittedIndex方法" class="headerlink" title="CommittedIndex方法"></a>CommittedIndex方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c MajorityConfig)</span></span> CommittedIndex(l AckedIndexer) Index</span><br></pre></td></tr></table></figure>

<p>该方法就是根据系统中所有节点反馈的<code>Index</code>，计算出最大<code>commitIndex</code>。当<code>Leader</code>收到<code>Follower</code>发来的<code>pb.MsgApp</code>消息的成功响应时就会调用本方法。</p>
<p>如果某个<code>Index</code>在<code>Majority</code>节点上都已经<code>Append</code>成功了，则<code>Leader</code>就可以将其提交了，所以最大<code>commitIndex</code>，就是找出<code>Majority</code>节点上<code>Append</code>成功的<code>Index</code>的最大值。</p>
<p>比如<code>8</code>节点的系统，如果节点反馈的<code>Append</code>成功的<code>Index</code>是<code>[1, 2, 2, 1, 2, 2, 1, 3]</code>，则计算得到的最大<code>commitIndex</code>就是<code>2</code>。因为<code>8</code>个节点中，有5个节点的<code>Index</code>是大于等于<code>2</code>的。</p>
<p><code>CommittedIndex</code>的实现逻辑是：</p>
<ul>
<li>通过参数<code>l</code>的<code>AckedIndex</code>方法，将所有节点的<code>Index</code>记录到<code>srt</code>这个<code>Slice</code>中；</li>
<li>对<code>srt</code>进行从小到大的排序，然后返回<code>srt[n - (n/2 + 1)]</code>的值；<code>n</code>表示节点数量；</li>
<li>这里的基本思想就是：<ul>
<li><code>srt</code>排序之后，<code>srt[n-1]</code>就是系统中只有1个节点认可的<code>Index</code>，<code>srt[n-2]</code>是有<code>2</code>个节点认可的<code>Index</code>，以此类推，<code>srt[n - (n/2 + 1)]</code>就是有<code>n/2+1</code>个节点认可的<code>Index</code>，<code>n/2+1</code>超过了半数，形成了<code>Majority</code>，并且是<code>srt</code>中满足条件的最大值；</li>
<li>比如上面的例子，[1, 2, 2, 1, 2, 2, 1, 3]排序之后就是[1, 1, 1, 2, 2, 2, 2, 3]。从右往左看，最后一个元素3，只有一个节点认可，倒数第2个元素2，目前有2个节点认可，一直到倒数第5个元素2，表示有5个元素认可，所以结果是2；</li>
</ul>
</li>
</ul>
<p>这里有一些实现细节：</p>
<ul>
<li><p>当节点数<code>n</code>小于等于7的时候，<code>srt</code>使用栈上内存，否则使用堆上内存。这是为了性能考虑。这通过下面的方式实现：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> stk [<span class="number">7</span>]<span class="type">uint64</span></span><br><span class="line"><span class="keyword">var</span> srt []<span class="type">uint64</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(stk) &gt;= n &#123;</span><br><span class="line">	srt = stk[:n]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">	srt = <span class="built_in">make</span>([]<span class="type">uint64</span>, n)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对<code>srt</code>使用的排序算法，也是从<code>stdlib</code>中复制过来的插入排序，这也是为了让保证<code>srt</code>一直在栈上；</p>
</li>
</ul>
<h4 id="VoteResult方法"><a href="#VoteResult方法" class="headerlink" title="VoteResult方法"></a>VoteResult方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> VoteResult <span class="type">uint8</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	VotePending VoteResult = <span class="number">1</span> + <span class="literal">iota</span></span><br><span class="line">	VoteLost</span><br><span class="line">	VoteWon</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c MajorityConfig)</span></span> VoteResult(votes <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="type">bool</span>) VoteResult</span><br></pre></td></tr></table></figure>

<p><code>VoteResult</code>方法的实现逻辑：</p>
<ul>
<li>参数<code>votes</code>记录了节点的投票情况，其中不包括没有投票的节点；</li>
<li>轮训<code>votes</code>，对投票情况进行统计：<ul>
<li>如果投”yes”的节点数，大于等于<code>n/2+1</code>，则投票结果是<code>VoteWon</code>；</li>
<li>如果投”yes”的节点数，加上尚未投票的节点数，大于等于<code>n/2+1</code>，则投票结果是<code>VotePending</code>，表示尚未获得准确结果；</li>
<li>其他情况，投票结果是<code>VoteLost</code>；</li>
</ul>
</li>
</ul>
<p><code>VoteResult</code>方法不一定只用来统计投票结果，它还可以判断系统中是否有<code>Majority</code>的节点都成为了某种状态，因为参数<code>votes</code>实际上是一个<code>节点ID</code>到<code>bool</code>的映射。</p>
<hr>
<h3 id="JointConfig"><a href="#JointConfig" class="headerlink" title="JointConfig"></a>JointConfig</h3><p><code>JointConfig</code>实际类型是<code>[2]MajorityConfig</code>，主要表示Joint Consensus配置变更算法中，<code>Cold,new</code>的配置。</p>
<p>该类型主要实现了三个方法：<code>IDs, CommittedIndex</code>和<code>VoteResult</code>方法。</p>
<h4 id="IDs方法"><a href="#IDs方法" class="headerlink" title="IDs方法"></a>IDs方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c JointConfig)</span></span> IDs() <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="keyword">struct</span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<p>该方法，就是返回<code>c[0]</code>和<code>c[1]</code>中，<code>节点ID</code>的并集；</p>
<h4 id="CommittedIndex方法-1"><a href="#CommittedIndex方法-1" class="headerlink" title="CommittedIndex方法"></a>CommittedIndex方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c JointConfig)</span></span> CommittedIndex(l AckedIndexer) Index</span><br></pre></td></tr></table></figure>

<p>该方法通过调用<code>MajorityConfig.CommittedIndex</code>方法实现，实际上就是返回<code>c[0].CommittedIndex(l)</code>和<code>c[1].CommittedIndex(l)</code>中的较小值；</p>
<h4 id="VoteResult方法-1"><a href="#VoteResult方法-1" class="headerlink" title="VoteResult方法"></a>VoteResult方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c JointConfig)</span></span> VoteResult(votes <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="type">bool</span>) VoteResult</span><br></pre></td></tr></table></figure>

<p>该方法通过调用<code>MajorityConfig.VoteResult</code>方法实现，实际逻辑是：</p>
<ul>
<li>如果<code>c[0].VoteResult(votes)</code>和<code>c[1].VoteResult(votes)</code>的返回值相等，表示<code>Cold</code>和<code>Cnew</code>达成了一致，因此最终结果就是其中之一即可；</li>
<li>如果其中有一个的返回结果是<code>VoteLost</code>，则最终结果就是<code>VoteLost</code>；</li>
<li>其他情况，最终结果就是<code>VotePending</code></li>
</ul>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><code>quorum.go</code>中定义了一些简单的类型和接口。他们包括：</p>
<ul>
<li><code>Index</code>类型，表示日志索引，实际类型就是<code>uint64</code>；</li>
<li><code>AckedIndexer</code>接口，其唯一的方法就是<code>AckedIndex(voterID uint64) (idx Index, found bool)</code>，用于将<code>节点ID</code>转为日志<code>Index</code>；</li>
<li><code>mapAckIndexer</code>类型，它实现了<code>AckedIndexer</code>接口，它的实际类型就是<code>map[uint64]Index</code>，它实现的<code>AckedIndex</code>方法就是在<code>map</code>中查询<code>ID</code>，返回得到的<code>Index</code>；</li>
<li><code>VoteResult</code>类型，表示投票结果，实际类型是<code>uint8</code>，实际上就是个枚举，取值有：<code>VotePending、VoteLost</code>和<code>VoteWon</code>；</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-04raft包--storage</title>
    <url>/2021/10/07/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/04raft%E5%8C%85--storage/</url>
    <content><![CDATA[<p><code>raft</code>包中的<code>storage.go</code>定义了表示持久化状态的<code>Storage</code>接口，以及该接口的一个实现<code>MemoryStorage</code>。</p>
<p>定义<code>Storage</code>接口，主要是为了让使用<code>raft</code>库的应用程序实现该接口，以便<code>raft</code>可以通过调用<code>Storage</code>的方法获取<code>NVS</code>中的持久化snapshot和日志条目等信息。<code>Storage</code>接口的定义如下：</p>
<span id="more"></span>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面的HardState和ConfState，定义在raft/v3/raftpb中</span></span><br><span class="line"><span class="comment">//HardState表示保存在NVS中的持久化状态，与论文中的描述一致，即包含：Term，投票节点ID以及提交日志的最大index</span></span><br><span class="line">type HardState <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">	Term   uint64 `protobuf:<span class="string">&quot;varint,1,opt,name=term&quot;</span> json:<span class="string">&quot;term&quot;</span>`</span><br><span class="line">	Vote   uint64 `protobuf:<span class="string">&quot;varint,2,opt,name=vote&quot;</span> json:<span class="string">&quot;vote&quot;</span>`</span><br><span class="line">	Commit uint64 `protobuf:<span class="string">&quot;varint,3,opt,name=commit&quot;</span> json:<span class="string">&quot;commit&quot;</span>`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//ConfState表示NVS中保存的节点配置</span></span><br><span class="line">type ConfState <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">	<span class="comment">//表示Cnew中的voter</span></span><br><span class="line">	Voters []uint64 `protobuf:<span class="string">&quot;varint,1,rep,name=voters&quot;</span> json:<span class="string">&quot;voters,omitempty&quot;</span>`</span><br><span class="line">	<span class="comment">//表示Cnew中的learner</span></span><br><span class="line">	Learners []uint64 `protobuf:<span class="string">&quot;varint,2,rep,name=learners&quot;</span> json:<span class="string">&quot;learners,omitempty&quot;</span>`</span><br><span class="line"></span><br><span class="line">	<span class="comment">//表示Cold中的voter，如果当前没有处于joint consensus状态，则VotersOutgoing为空</span></span><br><span class="line">	VotersOutgoing []uint64 `protobuf:<span class="string">&quot;varint,3,rep,name=voters_outgoing,json=votersOutgoing&quot;</span> json:<span class="string">&quot;voters_outgoing,omitempty&quot;</span>`</span><br><span class="line"></span><br><span class="line">	<span class="comment">//表示在joint consensus过程中，Cold中被降级为learner的voter，</span></span><br><span class="line">	LearnersNext []uint64 `protobuf:<span class="string">&quot;varint,4,rep,name=learners_next,json=learnersNext&quot;</span> json:<span class="string">&quot;learners_next,omitempty&quot;</span>`</span><br><span class="line"></span><br><span class="line">	<span class="comment">//为true，表示当前正处于joint consensus过程，并且Raft会自动过渡到joint consensus的第二阶段</span></span><br><span class="line">	AutoLeave <span class="type">bool</span> `protobuf:<span class="string">&quot;varint,5,opt,name=auto_leave,json=autoLeave&quot;</span> json:<span class="string">&quot;auto_leave&quot;</span>`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Storage interface &#123;</span><br><span class="line">    <span class="comment">//该方法返回NVS中的HardState和ConfState信息</span></span><br><span class="line">	InitialState() (pb.HardState, pb.ConfState, error)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//调用该方法，获取索引范围在[lo,hi)中的日志条目。maxSize限制了返回日志条目的总大小，</span></span><br><span class="line">    <span class="comment">//本方法在[lo,hi)符合条件的基础上，至少会返回一条日志（即使一条日志的大小超过了maxSize）。</span></span><br><span class="line">	Entries(lo, hi, maxSize uint64) ([]pb.Entry, error)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//该方法返回索引为i的日志的Term。i必须在[FirstIndex()-1, LastIndex()]范围内</span></span><br><span class="line">    <span class="comment">//之所以保留索引FirstIndex()-1的日志term信息，是为了AppendEntry时匹配日志，但其日志内容已不复存在了。</span></span><br><span class="line">	Term(i uint64) (uint64, error)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//该方法返回NVS中最后一条日志条目的索引值</span></span><br><span class="line">	LastIndex() (uint64, error)</span><br><span class="line">    <span class="comment">//该方法返回NVS中第一条日志条目的索引值。该索引之前的日志已经被合入到Snapshot中了;</span></span><br><span class="line">	FirstIndex() (uint64, error)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//该方法返回最新的Snapshot。如果当前Snapshot暂时不可得，则返回ErrSnapshotTemporarilyUnavailable错误；</span></span><br><span class="line">    <span class="comment">//因此raft状态机可以知道当前Storage正在准备Snapshot，从而过段时间再重试；</span></span><br><span class="line">	Snapshot() (pb.Snapshot, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据<code>Storage</code>的方法，可知<code>NVS</code>中保存了持久化状态<code>HardState</code>，节点配置状态<code>ConfState</code>，以及日志项。而日志项又包括<code>Snapshot</code>和日志条目。</p>
<hr>
<h3 id="MemoryStorage结构"><a href="#MemoryStorage结构" class="headerlink" title="MemoryStorage结构"></a>MemoryStorage结构</h3><p><code>MemoryStorage</code>是<code>Storage</code>接口的一种实现，实际上它的信息都保存在内存中。从代码中看来<code>etcdserver</code>和<code>raftexample</code>都是直接用的该结构来提供<code>log</code>的查询功能。</p>
<p><code>MemoryStorage</code>的定义如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面的Snapshot、SnapshotMetadata和Entry都定义在raft/v3/raftpb中</span></span><br><span class="line"><span class="keyword">type</span> Snapshot <span class="keyword">struct</span> &#123;</span><br><span class="line">    <span class="comment">//snapshot的内容</span></span><br><span class="line">	Data     []<span class="type">byte</span>           <span class="string">`protobuf:&quot;bytes,1,opt,name=data&quot; json:&quot;data,omitempty&quot;`</span></span><br><span class="line">	<span class="comment">//元数据，包括快照中最后的节点配置，snapshot包含的最后日志条目index和term</span></span><br><span class="line">    Metadata SnapshotMetadata <span class="string">`protobuf:&quot;bytes,2,opt,name=metadata&quot; json:&quot;metadata&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> SnapshotMetadata <span class="keyword">struct</span> &#123;</span><br><span class="line">	ConfState ConfState <span class="string">`protobuf:&quot;bytes,1,opt,name=conf_state,json=confState&quot; json:&quot;conf_state&quot;`</span></span><br><span class="line">	Index     <span class="type">uint64</span>    <span class="string">`protobuf:&quot;varint,2,opt,name=index&quot; json:&quot;index&quot;`</span></span><br><span class="line">	Term      <span class="type">uint64</span>    <span class="string">`protobuf:&quot;varint,3,opt,name=term&quot; json:&quot;term&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Entry <span class="keyword">struct</span> &#123;</span><br><span class="line">	Term  <span class="type">uint64</span>    <span class="string">`protobuf:&quot;varint,2,opt,name=Term&quot; json:&quot;Term&quot;`</span></span><br><span class="line">	Index <span class="type">uint64</span>    <span class="string">`protobuf:&quot;varint,3,opt,name=Index&quot; json:&quot;Index&quot;`</span></span><br><span class="line">	Type  EntryType <span class="string">`protobuf:&quot;varint,1,opt,name=Type,enum=raftpb.EntryType&quot; json:&quot;Type&quot;`</span></span><br><span class="line">	Data  []<span class="type">byte</span>    <span class="string">`protobuf:&quot;bytes,4,opt,name=Data&quot; json:&quot;Data,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> MemoryStorage <span class="keyword">struct</span> &#123;</span><br><span class="line">	sync.Mutex</span><br><span class="line"></span><br><span class="line">	hardState pb.HardState</span><br><span class="line">	snapshot  pb.Snapshot</span><br><span class="line"></span><br><span class="line">    <span class="comment">//ents[1:]表示尚未合入到snapshot中的日志条目；</span></span><br><span class="line">	ents []pb.Entry</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据<code>MemoryStorage</code>的定义，可以总结出<code>snapshot</code>和<code>ents</code>的一般结构（<font color=red>有例外</font>）如下：</p>
<img src="/img/06storage/1.png" style="zoom:20%;" />

<p>其中，<code>ents[0]</code>中只包含了<code>Term</code>和<code>Index</code>信息，<font color=red>一般情况下</font>对应的是<code>snapshot</code>中包含的最后日志条目的<code>Index</code>和<code>Term</code>，即<code>snapshot.Metadata.Index</code>和<code>snapshot.Metadata.Term</code>（<font color=red>也有例外情况下<code>ents[0] &lt; snapshot.Metadata.Index</code>的情况</font>），注释中称之为<code>dummy entry</code>。</p>
<p>根据示例代码，可以得到<code>MemoryStorage.snapshot</code>来源：</p>
<ul>
<li><p>外部模块收到<code>Ready</code>结构后，如果其中包含了从<code>Leader</code>节点发来的<code>snapshot</code>，则以<code>Ready.snapshot</code>为参数，调用<code>MemoryStorage.ApplySnapshot</code>方法，替换掉现有的<code>MemoryStorage.snapshot</code>，并将所有日志条目<code>MemoryStorage.ents</code>清空，只保留<code>ents[0]</code>的信息，设置<code>ents[0]</code>的<code>Index</code>和<code>Term</code>分别为最新的<code>snapshot.Metadata.Index</code>和<code>snapshot.Metadata.Term</code>；</p>
</li>
<li><p>外部模块在满足一定的条件后，会调用<code>MemoryStorage.CreateSnapshot</code>方法主动创建<code>MemoryStorage.snapshot</code>，然后外部模块将<code>snapshot</code>保存到<code>NVS</code>中，然后会调用<code>MemoryStorage.Compact</code>来摒弃一些<code>MemoryStorage.ents</code>中的日志条目，即<code>compactlndex</code>之前的日志。但是调用该方法的参数<code>compactlndex</code>并不一定等于调用<code>CreateSnapshot</code>时传递的<code>Metadata.Index</code>，其值有可能比当前的新<code>snapshot.Metadata.Index</code>要小，即保留一些已经压缩到<code>snapshot</code>中的日志条目。根据注释，这里是为了照顾一些比较慢的Follower，以便不发送非必要的<code>snapshot</code>，节省带宽。<font color=red>注意</font>：这就表示<code>ents[0].Index</code>并不总是等于<code>snapshot.Metadata.Index</code>，其值可能更小。</p>
</li>
</ul>
<p><code>MemoryStorage.ents</code>的来源：外部模块收到<code>Ready</code>结构，将其中的<code>Entries</code>，调用<code>MemoryStorage.Append</code>，追加到<code>MemoryStorage.ents</code>中。</p>
<hr>
<h3 id="方法和函数"><a href="#方法和函数" class="headerlink" title="方法和函数"></a>方法和函数</h3><h4 id="NewMemoryStorage"><a href="#NewMemoryStorage" class="headerlink" title="NewMemoryStorage"></a>NewMemoryStorage</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewMemoryStorage</span><span class="params">()</span></span> *MemoryStorage</span><br></pre></td></tr></table></figure>

<p><code>NewMemoryStorage</code>函数创建并返回一个空的<code>MemoryStorage</code>结构。该函数内部只初始化了<code>ents</code>成员，将其初始化为只包含一个元素，即<code>ents[0]</code>，其中的<code>Index</code>和<code>Term</code>都是默认值<code>0</code>.</p>
<hr>
<h4 id="FirstIndex和LastIndex"><a href="#FirstIndex和LastIndex" class="headerlink" title="FirstIndex和LastIndex"></a>FirstIndex和LastIndex</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> FirstIndex() (<span class="type">uint64</span>, <span class="type">error</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> LastIndex() (<span class="type">uint64</span>, <span class="type">error</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> firstIndex() <span class="type">uint64</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> lastIndex() <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>在<code>ents</code>中，<code>ents[0]</code>中保存的<code>Index</code>，对应的是最小日志条目的<code>Index-1</code>，从<code>ents[1]</code>开始才是保存的真正的日志条目。所以，<code>ents[i]</code>日志项，其日志的<code>Index</code>是<code>i+ents[0].Index</code>。所以，有下面的<font color=red>等式</font>成立，下面的<code>offset</code>指的就是<code>ents[0].Index</code>：</p>
<ul>
<li><code>ents[i].Index == i + offset</code>;</li>
<li><code>ents[i-offset].Index == i</code>;</li>
</ul>
<p><code>firstIndex</code>和<code>lastIndex</code>，返回<code>ents</code>中，实际保存的日志条目，第一条和最后一条的<code>Index</code>，所以<code>firstIndex</code>实际上返回的是<code>ents[0].Index + 1</code>，而<code>lastIndex</code>返回的是<code>ms.ents[0].Index + uint64(len(ms.ents)) - 1</code>。</p>
<p><code>FirstIndex</code>和<code>LastIndex</code>，就是在加锁的基础上，对<code>firstIndex</code>和<code>lastIndex</code>的简单封装。</p>
<p>eg:</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">ents := []pb.Entry&#123;&#123;Index: <span class="number">3</span>, Term: <span class="number">3</span>&#125;, &#123;Index: <span class="number">4</span>, Term: <span class="number">4</span>&#125;, &#123;Index: <span class="number">5</span>, Term: <span class="number">5</span>&#125;&#125;</span><br><span class="line">s := &amp;MemoryStorage&#123;ents: ents&#125;</span><br><span class="line"><span class="comment">//s.FirstIndex() is 4</span></span><br><span class="line"><span class="comment">//s.LastIndex() is 5</span></span><br></pre></td></tr></table></figure>

<hr>
<h4 id="ApplySnapshot方法"><a href="#ApplySnapshot方法" class="headerlink" title="ApplySnapshot方法"></a>ApplySnapshot方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> ApplySnapshot(snap pb.Snapshot) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>外部模块收到<code>Ready</code>结构后，如果其中包含了从Leader节点发来的<code>snapshot</code>，则以<code>Ready.snapshot</code>为参数，调用<code>ApplySnapshot</code>方法，替换掉现有的<code>MemoryStorage.snapshot</code>，并将所有日志条目<code>MemoryStorage.ents</code>清空，只保留<code>ents[0]</code>的信息，设置<code>ents[0]</code>的<code>Index</code>和<code>Term</code>分别为最新的<code>snap.Metadata.Index</code>和<code>snap.Metadata.Term</code>；</p>
<p>该方法在替换<code>MemoryStorage.snapshot</code>之前，需<font color=red>要保证新的<code>snapshot.Metadata.Index</code>必须大于旧的<code>snapshot.Metadata.Index</code></font>，否则就表示新<code>snapshot</code>不能完全覆盖旧<code>snapshot</code>，这种情况下直接报错返回；</p>
<hr>
<h4 id="CreateSnapshot"><a href="#CreateSnapshot" class="headerlink" title="CreateSnapshot"></a>CreateSnapshot</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> CreateSnapshot(i <span class="type">uint64</span>, cs *pb.ConfState, data []<span class="type">byte</span>) (pb.Snapshot, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>外部模块在满足一定的条件后，会调用<code>MemoryStorage.CreateSnapshot</code>方法主动创建<code>MemoryStorage.snapshot</code>。</p>
<ul>
<li><p>该方法的参数<code>i</code>表示新<code>snapshot</code>的<code>Metadata.Index</code>；参数<code>data</code>表示新<code>snapshot</code>的二进制数据（也就是说新<code>snapshot</code>的数据来源于外部模块，实际上就是当前系统状态的快照）；参数<code>cs</code>表示当前的集群成员配置；</p>
</li>
<li><p>该方法会检查<code>i</code>是否在 <code>(ms.snapshot.Metadata.Index, lastIndex ]</code>范围内，然后设置新的<code>snapshot</code>，下面的<code>offset</code>就是<code>原ms.ents[0].Index</code>：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">ms.snapshot.Metadata.Index = i</span><br><span class="line">ms.snapshot.Metadata.Term = ms.ents[i-offset].Term</span><br><span class="line"><span class="keyword">if</span> cs != <span class="literal">nil</span> &#123;</span><br><span class="line">	ms.snapshot.Metadata.ConfState = *cs</span><br><span class="line">&#125;</span><br><span class="line">ms.snapshot.Data = data</span><br></pre></td></tr></table></figure></li>
</ul>
<p><font color=red>注意</font>，本方法并没有对<code>ms.ents</code>做任何改动；</p>
<hr>
<h4 id="Compact方法"><a href="#Compact方法" class="headerlink" title="Compact方法"></a>Compact方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> Compact(compactIndex <span class="type">uint64</span>) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>外部模块调用<code>CreateSnapshot</code>设置新的<code>snapshot</code>后，将<code>snapshot</code>保存到<code>NVS</code>中，然后会调用<code>MemoryStorage.Compact</code>来摒弃一些<code>MemoryStorage.ents</code>中的日志条目，即<code>Index</code>为<code>compactIndex</code>及其之前的的日志条目。</p>
<p><font color=red>但是</font>调用该方法的参数<code>compactlndex</code>并不一定等于调用<code>CreateSnapshot</code>时传递的<code>Metadata.Index</code>，其值有可能比当前的新<code>snapshot.Metadata.Index</code>要小，即保留一些已经压缩到<code>snapshot</code>中的日志条目。根据注释，这里是为了照顾一些比较慢的Follower，以便不发送非必要的<code>snapshot</code>，节省带宽。<font color=red>注意</font>：这就表示<code>ents[0].Index</code>并不总是等于<code>snapshot.Metadata.Index</code>，其值可能更小。</p>
<p>该方法会检查参数<code>compactlndex</code>，查看其是否在<code>(ents[0].Index, lastlndex]</code>范围内，然后设置新的<code>MemoryStorage.ents</code>，并且<font color=red>设置<code>MemoryStorage.ents[0].Index</code>为<code>compactlndex</code>，丢掉原<code>ents[compactIndex - offset]</code>及其之前的部分</font>；</p>
<hr>
<h4 id="Entries方法"><a href="#Entries方法" class="headerlink" title="Entries方法"></a>Entries方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> Entries(lo, hi, maxSize <span class="type">uint64</span>) ([]pb.Entry, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p><code>Entries</code>方法返回的是<code>ents</code>中，<code>Index</code>在<code>[lo, hi)</code>范围内日志条目，同时保证返回的日志条目的总字节数小于<code>maxSize</code>，但是该方法至少会返回一个条目。注意，<code>Index</code>为<code>i</code>，对应的<code>ents</code>元素是<code>ents[i-offset]</code>，这里<code>offset</code>是<code>ents[0].Index</code>。<code>lo</code>和<code>hi</code>的约束条件如下：</p>
<ul>
<li><code>lo</code>如果小于等于<code>offset</code>，表示<code>lo</code>对应的日志项已经合入到<code>snapshot</code>中了，因此返回<code>ErrCompacted</code>错误，表示对应的日志项已经合入到<code>snapshot</code>中了；</li>
<li><code>hi</code>如果大于<code>lastIndex() + 1</code>，则直接<code>Panic</code>；</li>
<li>如果当前<code>ents</code>长度为1，即只包含<code>ents[0]</code>，则返回<code>ErrUnavailable</code>错误，表示请求的日志上不可得；</li>
</ul>
<hr>
<h4 id="Term方法"><a href="#Term方法" class="headerlink" title="Term方法"></a>Term方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> Term(i <span class="type">uint64</span>) (<span class="type">uint64</span>, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>Index</code>为<code>i</code>的日志条目的<code>Term</code>。注意，<code>Index</code>为<code>i</code>，对应的是<code>ents[i-offset]</code>，其中<code>offset</code>是<code>ents[0].Index</code>。<code>i</code>需要满足的约束条件是：</p>
<ul>
<li>如果<code>i</code>小于<code>offset</code>，则返回<code>ErrCompacted</code>错误，表示对应的日志项已经合入到<code>snapshot</code>中了；</li>
<li>如果<code>i-offset</code>大于等于<code>len(ms.ents)</code>，则返回<code>ErrUnavailable</code>错误，表示请求的日志上不可得；</li>
</ul>
<hr>
<h4 id="Append方法"><a href="#Append方法" class="headerlink" title="Append方法"></a>Append方法</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> Append(entries []pb.Entry) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>外部模块收到<code>Ready</code>结构，将其中的<code>Entries</code>，调用<code>MemoryStorage.Append</code>方法，追加到<code>MemoryStorage.ents</code>中。</p>
<p>该方法将<code>entries</code>的内容附加到<code>ms.ents</code>中。<code>entries</code>的内容有可能与<code>ms.ents</code>有重叠的部分，附加时需要用<code>entries</code>的内容覆盖掉重叠部分，并且保证最后<code>ms.ents</code>中的日志连续性。函数的逻辑如下，注意<code>entries</code>中没有<code>dummy entry</code>，每个元素都对应真正的日志条目：</p>
<ul>
<li>如果<code>entries</code>最后一条日志的<code>Index</code>，小于<code>ms.firstIndex()</code>，说明没有任何可附加的新日志，直接返回<code>nil</code>；</li>
<li>如果<code>ms.firstIndex()</code>大于<code>entries</code>第一条日志的<code>Index</code>，则需要将<code>entries</code>中，前<code>ms.firstIndex()-entries[0].Index</code>个日志条目删除；排除之后，可以保证<code>entries[0].Index</code> 大于等于 <code>ms.firstIndex()</code>；</li>
<li>经过上面的步骤之后，根据<code>entries[0].Index - ms.ents[0].Index</code>的值的大小决定附加的策略。以下称该值为<code>offset</code>：<ul>
<li>如果<code>offset</code>小于<code>len(ms.ents)</code>，说明<code>entries</code>和<code>ms.ents</code>有重叠的部分，所以先保留<code>ms.ents[:offset]</code>原有部分，然后将<code>entries</code>内容直接追加到<code>ms.ents</code>后面，即用<code>entries</code>的内容覆盖掉重叠部分；</li>
<li>如果<code>offset</code>等于<code>len(ms.ents)</code>，说明<code>entries</code>的内容可以与<code>ms.ents</code>无缝衔接，因此直接将<code>entries</code>附加到<code>ms.ents</code>后面即可；</li>
<li>如果<code>offset</code>大于<code>len(ms.ents)</code>，说明<code>entries</code>的内容与<code>ms.ents</code>之间有“空隙”，这种情况下直接<code>Panic</code>；</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-05raft包--log_unstable</title>
    <url>/2021/10/24/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/05raft%E5%8C%85--log_unstable/</url>
    <content><![CDATA[<p><code>raft</code>包中的<code>log_unstable.go</code>定义了<code>unstable</code>结构，对应于<code>storage</code>结构，表示尚未保存到持久化存储中（即<code>storage</code>）的快照和日志信息。</p>
<span id="more"></span>
<h3 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> unstable <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//表示从外部接收到的快照</span></span><br><span class="line">	snapshot *pb.Snapshot</span><br><span class="line"></span><br><span class="line">	<span class="comment">//尚未写入到storage中的日志。注意entries[i]的Index为offset+i，所以，offset == entries[0]。</span></span><br><span class="line">	entries []pb.Entry</span><br><span class="line">	<span class="comment">//表示entries[0]的Index。注意其值可能小于storage中的日志条目的最大索引，所以将entries写入storage时，</span></span><br><span class="line">	<span class="comment">//需要将entries中前几条条目截断</span></span><br><span class="line">	offset  <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">	logger Logger</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据<code>unstable</code>的定义和使用方法，可以总结出<code>snapshot</code>和<code>entries</code>的一般情况下的结构（<font color=red>有例外，即entries[0].Index &lt;&#x3D; snapshot.Metadata.Index</font>）如下，其中<code>entries[0]</code>日志条目<code>Index</code>是紧接着<code>snapshot.Metadata.Index</code>的：</p>
<p><img src="/img/07log_unstable/image-20220316190647592.png" alt="image-20220316190647592"></p>
<p><code>unstable.snapshot</code>是<code>Follower</code>通过<code>Leader</code>发来的<code>pb.MsgSnap</code>消息中收到的；而<code>entries</code>日志条目是<code>Leader</code>收到提案后生成的，或者是<code>Follower</code>通过<code>Leader</code>发来的<code>pb.MsgApp</code>消息中收到的。不管是<code>snapshot</code>，还是<code>entries</code>日志条目，都是尚未，但最终会保存到<code>storage</code>中的。</p>
<p>下面的恒等式是始终成立的：</p>
<ul>
<li><code>offset == entries[0].Index</code></li>
<li><code>entries[i].Index == i + offset</code></li>
</ul>
<hr>
<h3 id="snapshot和entries的来源和去处"><a href="#snapshot和entries的来源和去处" class="headerlink" title="snapshot和entries的来源和去处"></a>snapshot和entries的来源和去处</h3><p><code>unstable.snapshot</code>来源和去处：</p>
<ul>
<li><code>Follower/Candidate</code>收到<code>Leader</code>发来的<code>pb.MsgSnap</code>消息后，调用<code>raft.handleSnapshot</code> ，该函数中，会以消息中的<code>snapshot</code>为参数，调用<code>unstable.restore</code>方法，设置<code>unstable.snapshot</code>，并将<code>unstable.entries</code>置为<code>nil</code>，并设置<code>unstable.offset</code>为<code>snapshot.Metadata.Index+1</code>，即后续的日志条目的<code>Index</code>是紧接着<code>u.snapshot.Metadata.Index</code>的;</li>
<li>外部模块处理完<code>Ready</code>结构后，调用<code>raft.advance</code>方法，其中，如果<code>Ready.Snapshot</code>不为空，则说明其中的<code>snapshot</code>已经保存到<code>NVS</code>中了，则以其为参数调用<code>raftlog.stableSnapTo</code>方法，最终调用到<code>unstable.stableSnapTo</code>方法 ，将<code>unstable.snapshot</code>置为<code>nil</code>。</li>
<li>所以，<code>unstable</code>中的<code>snapshot</code>是个临时状态，它最终是要保存到<code>storage</code>中去的。</li>
</ul>
<p><code>unstable.entries</code>的来源和去处：</p>
<ul>
<li>节点追加日志时，会调用<code>unstable.truncateAndappend</code>，使用参数<code>ents</code>，覆盖并且追加到<code>unstable.entries</code>中。这里，如果参数<code>ents[0].Index</code>小于等于当前的<code>unstable.offset</code>，则需要删除所有<code>unstable.entries</code>的内容，重新赋值<code>unstable.offset</code>为<code>ents[0].Index</code>，并设置<code>unstable.entries</code>为参数<code>ents</code>。所以就有可能出现所谓的<font color=red>例外情况</font>，即<code>unstable.entries[0].Index</code>小于等于<code>snapshot.Metadata.Index</code>；</li>
<li>外部模块处理完<code>Ready</code>结构后，调用<code>raft.advance</code>方法，其中，如果<code>Ready.Entries</code>不为空，则使用<code>Ready.Entries</code>最后一个条目的<code>Index</code>和<code>Term</code> ，调用<code>raftLog.stableTo</code>方法，清理掉<code>Index</code>以及之前的日志，并将<code>offset</code>设置为<code>Index+1</code>；</li>
</ul>
<hr>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="restore"><a href="#restore" class="headerlink" title="restore"></a>restore</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> restore(s pb.Snapshot)</span><br></pre></td></tr></table></figure>

<p>节点崩溃后重启时，或者<code>Follower/Candidate</code>收到<code>Leader</code>发来的<code>pb.MsgSnap</code>消息后，调用该方法。根据参数<code>pb.Snapshot</code>，设置<code>u.snapshot</code>，并将<code>u.entries</code>置为<code>nil</code>，而<code>u.offset</code>置为<code>s.Metadata.Index + 1</code>，即后续的日志条目的<code>Index</code>是紧接着<code>u.snapshot.Metadata.Index</code>的。</p>
<hr>
<h4 id="stableSnapTo"><a href="#stableSnapTo" class="headerlink" title="stableSnapTo"></a>stableSnapTo</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> stableSnapTo(i <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>在将<code>unstable</code>中<code>snapshot</code>保存到<code>storage</code>中后，调用该方法，丢弃<code>u.snapshot</code>。即在<code>i</code>等于<code>u.snapshot.Metadata.Index</code>的情况下，将<code>u.snapshot</code>置为<code>nil</code>；</p>
<hr>
<h4 id="stableTo"><a href="#stableTo" class="headerlink" title="stableTo"></a>stableTo</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> stableTo(i, t <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>在将<code>unstable</code>中日志保存到<code>storage</code>中后，调用该方法，丢弃<code>Index</code>为<code>i</code>以及之前的日志条目。逻辑如下：</p>
<ul>
<li>首先调用<code>u.maybeTerm(i)</code>，得到<code>Index</code>为<code>i</code>的日志条目对应的<code>Term</code>值，只有在<code>Term</code>值等于参数<code>t</code>，并且<code>i</code>大于等于<code>u.offset</code>的情况下，才能丢弃部分日志条目；</li>
<li>将<code>u.entries</code>中，<code>i-u.offset</code>及其之前的内容丢弃，然后将<code>u.offset</code>设置为<code>i+1</code>，保持恒等式<code>u.offset == u.entries[0].Index</code>；</li>
<li>最后调用<code>shrinkEntriesArray</code>，尝试回收<code>u.entries</code>的空间；</li>
</ul>
<hr>
<h4 id="maybeFirstIndex和maybeLastIndex"><a href="#maybeFirstIndex和maybeLastIndex" class="headerlink" title="maybeFirstIndex和maybeLastIndex"></a>maybeFirstIndex和maybeLastIndex</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> maybeFirstIndex() (<span class="type">uint64</span>, <span class="type">bool</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> maybeLastIndex() (<span class="type">uint64</span>, <span class="type">bool</span>)</span><br></pre></td></tr></table></figure>

<p><code>maybeFirstIndex</code>在有<code>u.snapshot</code>的情况下，返回<code>u.snapshot.Metadata.Index + 1</code>，一般情况下（<font color=red>有例外</font>）表示<code>unstable</code>中保存的日志条目的最小Index。如果没有<code>u.snapshot</code>，则直接返回<code>0, false</code>；</p>
<p><code>maybeLastIndex</code>返回<code>unstable</code>中记录的日志条目的最大<code>Index</code>：</p>
<ul>
<li>如果<code>u.entries</code>中有条目，则返回<code>u.entries</code>中最后一个条目的<code>Index</code>，即<code>u.offset + len(u.entries) - 1</code>；</li>
<li>否则，如果有<code>u.snapshot</code>，则返回<code>u.snapshot.Metadata.Index</code>；</li>
<li>否则，返回<code>0, false</code>；</li>
</ul>
<hr>
<h4 id="maybeTerm"><a href="#maybeTerm" class="headerlink" title="maybeTerm"></a>maybeTerm</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> maybeTerm(i <span class="type">uint64</span>) (<span class="type">uint64</span>, <span class="type">bool</span>)</span><br></pre></td></tr></table></figure>

<p>该函数返回<code>Index</code>为<code>i</code>的日志条目的<code>Term</code>值。注意，<code>i</code>的取值范围是<code>[u.offset, u.maybeLastIndex()]</code>，或者是<code>u.snapshot.Metadata.Index</code>：</p>
<ul>
<li>在<code>i</code>小于<code>u.offset</code>的情况下，如果存在<code>u.snapshot</code>，并且<code>i</code>等于<code>u.snapshot.Metadata.Index</code>，则直接返回<code>u.snapshot.Metadata.Term</code>，否则返回<code>0, false</code>；</li>
<li>如果<code>i</code>取值范围在<code>[u.offset, u.maybeLastIndex()]</code>内，则返回<code>u.entries[i-u.offset].Term</code>；</li>
<li>其他情况，返回<code>0, false</code>；</li>
</ul>
<hr>
<h4 id="shrinkEntriesArray"><a href="#shrinkEntriesArray" class="headerlink" title="shrinkEntriesArray"></a>shrinkEntriesArray</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> shrinkEntriesArray()</span><br></pre></td></tr></table></figure>

<p>该方法用于清理<code>u.entries</code>这个<code>slice</code>的空间。即当 <code>2 * len(u.entries)</code> 小于 <code>cap(u.entries)</code>时，重新构造<code>u.entries</code>：</p>
<ul>
<li>这里没有直接清理<code>entries</code>，根据注释，如果直接清理<code>entries</code>的话不安全，因为<code>client</code>可能正在使用<code>entries</code>。</li>
<li>因此，这里是满足条件的情况下，构造新的<code>entries</code>，然后将当前<code>u.entries</code>内容复制到新<code>entries</code>中，然后再用新<code>entries</code>替换<code>u.entries</code>；</li>
<li>如果当前<code>u.entries</code>长度为0，则直接将<code>u.entries</code>置为<code>nil</code>；</li>
</ul>
<hr>
<h4 id="mustCheckOutOfBounds"><a href="#mustCheckOutOfBounds" class="headerlink" title="mustCheckOutOfBounds"></a>mustCheckOutOfBounds</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> mustCheckOutOfBounds(lo, hi <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法检查<code>lo</code>和<code>hi</code>是否满足不等式：<code>u.offset &lt;= lo &lt;= hi &lt;= u.offset+len(u.entries)</code>，如果不满足，则直接<code>panic</code>；</p>
<p>注意<code>u.entries</code>中日志条目的Index范围是<code>[ u.offset, u.offset+len(u.entries) )</code>。</p>
<hr>
<h4 id="slice"><a href="#slice" class="headerlink" title="slice"></a>slice</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> slice(lo <span class="type">uint64</span>, hi <span class="type">uint64</span>) []pb.Entry</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>u.entries</code>中，<code>Index</code>范围在<code>[lo, hi)</code>之间的日志条目。因<code>Index</code>为<code>i</code>，对应的是<code>u.entries[i-u.offset]</code>，所以该方法返回的是<code>u.entries[lo-u.offset : hi-u.offset]</code>；该方法会首先通过<code>mustCheckOutOfBounds</code>方法检查<code>lo, hi</code>满足不等式：<code>u.offset &lt;= lo &lt;= hi &lt;= u.offset+len(u.entries)</code>；</p>
<hr>
<h4 id="truncateAndAppend"><a href="#truncateAndAppend" class="headerlink" title="truncateAndAppend"></a>truncateAndAppend</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> truncateAndAppend(ents []pb.Entry)</span><br></pre></td></tr></table></figure>

<p>当需要将外部日志条目<code>ents</code>，附加到<code>unstable</code>中的<code>entries</code>中时，调用本方法。该方法以<code>ents</code>的日志条目为准，根据<code>ents</code>中<code>Index</code>的范围，可能需要覆盖源<code>u.entries</code>中的日志条目：</p>
<ul>
<li>如果<code>ents[0].Index</code>刚好等于<code>u.offset+uint64(len(u.entries))</code>，说明<code>ents</code>和<code>u.entries</code>刚好可以无缝衔接，因此直接将<code>ents append</code>到<code>u.entries</code>即可；</li>
<li>如果<code>ents[0].Index</code>小于等于<code>u.offset</code>，则需要完全丢弃原有的<code>u.entries</code>，而用<code>ents</code>进行替代，同时将<code>u.offset</code>置为<code>ents[0].Index</code>；</li>
<li>如果<code>ents[0].Index</code>在<code>(u.offset, u.offset+uint64(len(u.entries)))</code>范围内，则首先保留<code>u.entries</code>中<code>Index</code>在<code>[u.offset, ents[0].Index)</code>之间的部分，然后用<code>ents</code>覆盖剩下的部分；</li>
<li>如果<code>ents[0].Index</code>大于<code>u.offset+uint64(len(u.entries)))</code>，则会在调用<code>u.slice -&gt; u.mustCheckOutOfBounds</code>方法时<code>panic</code>；</li>
</ul>
<p>比如：<code>u.entries</code>原来为<code>[]pb.Entry&#123;&#123;Index: 5, Term: 1&#125;&#125;</code>, <code>u.offset</code>为<code>5</code>，调用<code>truncateAndAppend</code>时传入参数为<code>[]pb.Entry&#123;&#123;Index: 4, Term: 2&#125;, &#123;Index: 5, Term: 2&#125;, &#123;Index: 6, Term: 2&#125;&#125;</code>，则最后<code>u.entries</code>为<code>[]pb.Entry&#123;&#123;Index: 4, Term: 2&#125;, &#123;Index: 5, Term: 2&#125;, &#123;Index: 6, Term: 2&#125;&#125;</code>，<code>u.offset</code>为<code>4</code>；</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-06raft包--log</title>
    <url>/2021/11/15/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/06raft%E5%8C%85--log/</url>
    <content><![CDATA[<p><code>raft</code>包中的<code>log.go</code>定义了<code>raftLog</code>结构体，实现了<code>RAFT</code>中日志追加相关的底层功能，<code>raftLog</code>结构主要就是封装了<code>Storage</code>和<code>unstable</code>，并对外提供相应的接口。</p>
<span id="more"></span>
<h3 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> raftLog <span class="keyword">struct</span> &#123;</span><br><span class="line">	storage Storage</span><br><span class="line"></span><br><span class="line">    <span class="comment">//unstable是尚未保存到storage中的日志</span></span><br><span class="line">	unstable unstable</span><br><span class="line"></span><br><span class="line">	<span class="comment">//committed是在过半节点上已经保存到storage中的日志条目的最大Index值</span></span><br><span class="line">	committed <span class="type">uint64</span></span><br><span class="line">	<span class="comment">//applied是已经应用到状态机中的日志条目的最大Index值。不变式：applied &lt;= committed</span></span><br><span class="line">	applied <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">	logger Logger</span><br><span class="line"></span><br><span class="line">	<span class="comment">//表示nextEnts方法中，返回的日志条目ents的总字节数的最大限制，</span></span><br><span class="line">	<span class="comment">//具体而言就是返回的可以应用到状态机中的日志条目的总字节数的最大限制；</span></span><br><span class="line">	maxNextEntsSize <span class="type">uint64</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据之前对<code>Storage</code>和<code>unstable</code>的分析，可知<code>Storage</code>中保存的就是已经存储到<code>NVS</code>中的<code>snapshot</code>和日志条目；而<code>unstable</code>中的<code>snapshot</code>则是从外部接收到的，最终会保存到<code>NVS</code>中，并替换掉<code>Storage</code>的<code>snapshot</code>和日志条目；而<code>unstable</code>中的<code>entries</code>，则是最新追加的日志条目，最终也会保存到<code>Storage</code>中。</p>
<hr>
<h3 id="函数和方法"><a href="#函数和方法" class="headerlink" title="函数和方法"></a>函数和方法</h3><h4 id="newLogWithSize和newLog"><a href="#newLogWithSize和newLog" class="headerlink" title="newLogWithSize和newLog"></a>newLogWithSize和newLog</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newLogWithSize</span><span class="params">(storage Storage, logger Logger, maxNextEntsSize <span class="type">uint64</span>)</span></span> *raftLog</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newLog</span><span class="params">(storage Storage, logger Logger)</span></span> *raftLog</span><br></pre></td></tr></table></figure>

<p><code>newLogWithSize</code>函数根据<code>storage</code>构造新的<code>raftLog</code>。新的<code>raftLog</code>结构，<code>unstable</code>中不包含任何日志，其<code>offset</code>值初始化为<code>storage.LastIndex()+1</code>，所以初始状态下<code>unstable</code>中的日志是紧跟着<code>storage</code>中的日志的。新<code>raftLog</code>的成员初始化为：</p>
<ul>
<li><code>log.unstable.offset = storage.LastIndex() + 1</code>；</li>
<li><code>log.committed = storage.FirstIndex() - 1</code>；</li>
<li><code>log.applied = storage.FirstIndex() - 1</code>；</li>
</ul>
<p><code>newLog</code>函数通过调用<code>newLogWithSize(storage, logger, noLimit)</code>实现。</p>
<hr>
<h4 id="firstIndex"><a href="#firstIndex" class="headerlink" title="firstIndex"></a>firstIndex</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> firstIndex() <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法是<code>raftLog</code>内部需要调用的方法。它尝试返回当前节点<code>raftLog</code>中记录的日志条目的<code>Index</code>最小值（有可能是<code>snapshot</code>中的<code>Index</code>）。</p>
<p>按照该方法的逻辑，如果有<code>unstable.snapshot</code>的话，则该<code>snapshot</code>会替换<code>storage.snapshot</code>，并且将<code>storage.ents</code>清空，所以优先查看<code>unstable.snapshot</code>；如果没有<code>unstable.snapshot</code>，则<code>Index</code>最小的日志肯定在<code>storage</code>中。</p>
<p>所以它<font color=red>首先调用<code>l.unstable.maybeFirstIndex()</code>，如果调动失败则调用<code>l.storage.FirstIndex()</code>。</font>即依次尝试获取如下的值，只要获取成功则返回，否则进行下一个尝试：</p>
<ol>
<li>存在<code>l.unstable.snapshot</code>的情况下，<code>l.unstable.snapshot.Metadata.Index + 1</code>;</li>
<li><code>storage.ents[0].Index + 1</code>;</li>
</ol>
<hr>
<h4 id="lastIndex"><a href="#lastIndex" class="headerlink" title="lastIndex"></a>lastIndex</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> lastIndex() <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法是<code>raftLog</code>内部，以及外部<code>raft</code>模块都会使用的方法。它尝试返回当前节点<code>raftLog</code>中记录的日志条目的<code>Index</code>最大值（有可能是<code>snapshot</code>中的<code>Index</code>）。</p>
<p>根据<code>raftLog</code>中包含的<code>storage</code>和<code>unstable</code>，可以判定，如果<code>unstable</code>中有日志的话，则<code>Index</code>最大的日志条目肯定就在其中；否则，就只能在<code>storage</code>中。</p>
<p>该方法首先调用<code>l.unstable.maybeLastIndex()</code>，如果调动失败则调用<code>l.storage.LastIndex()</code>。即依次尝试获取如下的值，只要获取成功则返回，否则进行下一个尝试：</p>
<ul>
<li><code>l.unstable.offset + uint64(len(l.unstable.entries)) - 1</code>;</li>
<li><code>l.unstable.snapshot.Metadata.Index</code>;</li>
<li><code>l.storage.ents[0].Index + uint64(len(l.storage.ents)) - 1</code>;</li>
</ul>
<hr>
<h4 id="term"><a href="#term" class="headerlink" title="term"></a>term</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> term(i <span class="type">uint64</span>) (<span class="type">uint64</span>, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>raftLog</code>中记录的<code>Index</code>为<code>i</code>的日志条目的<code>Term</code>：</p>
<ul>
<li>首先判断<code>i</code>的取值是否在范围<code>[l.firstIndex() - 1, l.lastIndex()]</code>内，不在这个范围内的直接返回<code>0, nil</code>；</li>
<li>该方法先调用<code>l.unstable.maybeTerm(i)</code>，查看是否能从<code>l.unstable</code>中获取<code>i</code>对应的<code>Term</code>；</li>
<li>不能的话则调用<code>l.storage.Term(i)</code>，查看能否能从<code>l.storage</code>中获取<code>i</code>对应的<code>Term</code>；</li>
</ul>
<hr>
<h4 id="matchTerm"><a href="#matchTerm" class="headerlink" title="matchTerm"></a>matchTerm</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> matchTerm(i, term <span class="type">uint64</span>) <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>该方法判断<code>raftLog</code>中<code>Index</code>为<code>i</code>的日志条目的<code>Term</code>值，是否等于<code>term</code>。该方法通过调用<code>term</code>实现。</p>
<hr>
<h4 id="findConflict"><a href="#findConflict" class="headerlink" title="findConflict"></a>findConflict</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> findConflict(ents []pb.Entry) <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法只在<code>raftLog.maybeAppend</code>中调用，即<code>Follower/Canididate</code>收到<code>Leader</code>发来<code>pb.MsgApp</code>消息时，在<code>raft.handleAppendEntries</code>方法中调用<code>raftLog.maybeAppend</code>方法。而<code>raftLog.maybeAppend</code>方法在<code>pb.MsgApp</code>消息中的<code>preLogIndex</code>和<code>preLogTerm</code>与本地日志匹配的情况下调用<code>raftLog.findConflict</code>，尝试在日志<code>ents</code>中，找到第一个与<code>raftLog</code>中现有日志条目不一致的日志<code>Index</code>，以便后续找到合适的位置进行日志Append。</p>
<p>该方法通过依次轮训<code>ents</code>每个日志条目<code>ne</code>，找到第一个<code>l.matchTerm(ne.Index, ne.Term)</code>返回<code>false</code>的<code>ne.Index</code>。具体而言：</p>
<ul>
<li>如果<code>raftLog</code>现有日志包含了<code>ents</code>所有条目，并且现有日志条目的<code>Index</code>和<code>Term</code>都与<code>ents</code>的匹配，则返回<code>0</code>。比如，当前<code>raftLog</code>中的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>；如果调用<code>findConflict</code>时传递的参数是下面的内容，则<code>findConflict</code>返回值为<code>0</code>：<ul>
<li><code>[]pb.Entry&#123;&#125;</code>, </li>
<li><code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>, </li>
<li><code>[]pb.Entry&#123;&#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code> 或 </li>
<li><code>[]pb.Entry&#123;&#123;Index: 3, Term: 3&#125;&#125;</code>，</li>
</ul>
</li>
<li>如果<code>ents</code>中的日志条目的<code>Index</code>和<code>Term</code>与<code>raftLog</code>中现有日志条目的都一致，但是<code>ents</code>还包含了新的日志条目，则该方法返回找到的第一条新条目的<code>Index</code>。比如，当前<code>raftLog</code>中的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>；如果调用<code>findConflict</code>时传递的参数是下面的内容，则<code>findConflict</code>返回值为<code>4</code>：<ul>
<li><code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;, &#123;Index: 4, Term: 4&#125;, &#123;Index: 5, Term: 4&#125;&#125;</code>, </li>
<li><code>[]pb.Entry&#123;&#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;, &#123;Index: 4, Term: 4&#125;, &#123;Index: 5, Term: 4&#125;&#125;</code>, </li>
<li><code>[]pb.Entry&#123;&#123;Index: 3, Term: 3&#125;, &#123;Index: 4, Term: 4&#125;, &#123;Index: 5, Term: 4&#125;&#125;</code> 或</li>
<li><code>[]pb.Entry&#123;&#123;Index: 4, Term: 4&#125;, &#123;Index: 5, Term: 4&#125;&#125;</code>,</li>
</ul>
</li>
<li>如果<code>ents</code>中的日志条目的<code>Index</code>和<code>Term</code>有与<code>raftLog</code>中现有日志条目不一致的条目，则该方法返回找到的第一条不一致的条目。比如，当前<code>raftLog</code>中的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>；如果调用<code>findConflict</code>时传递的参数是下面的内容：<ul>
<li>传入<code>[]pb.Entry&#123;&#123;Index: 1, Term: 4&#125;, &#123;Index: 2, Term: 4&#125;&#125;</code>, 则返回值为<code>1</code>；</li>
<li>传入<code>[]pb.Entry&#123;&#123;Index: 2, Term: 1&#125;, &#123;Index: 3, Term: 4&#125;, &#123;Index: 4, Term: 4&#125;&#125;</code>, 则返回值为<code>2</code>；</li>
<li>传入<code>[]pb.Entry&#123;&#123;Index: 3, Term: 1&#125;, &#123;Index: 4, Term: 2&#125;, &#123;Index: 5, Term: 4&#125;, &#123;Index: 6, Term: 4&#125;&#125;</code>, 则返回值是<code>3</code>；</li>
</ul>
</li>
</ul>
<p>该方法的调用者需要保证<code>ents</code>中的日志条目的<code>Index</code>是连续递增的。</p>
<hr>
<h4 id="findConflictByTerm"><a href="#findConflictByTerm" class="headerlink" title="findConflictByTerm"></a>findConflictByTerm</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> findConflictByTerm(index <span class="type">uint64</span>, term <span class="type">uint64</span>) <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法会在<code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgApp</code>消息的失败回应时，或者<code>Follower</code>收到<code>Leader</code>发来的<code>pb.MsgApp</code>消息但其中的<code>preLogIndex</code>和<code>preLogTerm</code>与本地日志不匹配时调用：</p>
<ul>
<li><code>Follower</code>收到<code>Leader</code>发来的<code>pb.MsgApp</code>消息但其中的<code>preLogIndex</code>和<code>preLogTerm</code>与本地日志不匹配时，参数<code>(index, term)</code>表示<code>pb.MsgApp</code>消息中的<code>m.Index</code>和<code>m.LogTerm</code>，即<code>preLogIndex</code>和<code>preLogTerm</code>，<code>Follower</code>根据该信息调用本方法，决定<code>pb.MsgApp</code>消息失败响应中的<code>m.RejectHint</code>和<code>m.LogTerm</code>；</li>
<li><code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgApp</code>消息的失败回应时，参数<code>(index, term)</code>表示<code>pb.MsgApp</code>消息回应消息中的<code>m.RejectHint</code>和<code>m.LogTerm</code>，<code>Leader</code>根据该信息调用本方法决定下一次发送的<code>pb.MsgApp</code>消息中的日志条目；</li>
<li>该方法针对<code>(index, term)</code>，找到<code>raftLog</code>中，符合<code>Term</code>小于等于<code>term</code>，并且<code>Index</code>小于等于<code>index</code>的日志条目的最大<code>Index</code>。如果找不到，则返回<code>raftLog</code>中第一个日志条目的<code>Index</code>。</li>
</ul>
<p>该方法的具体逻辑是：</p>
<ul>
<li><p>如果<code>index</code>大于<code>l.lastIndex()</code>，则直接返回<code>index</code>。在<code>Follower</code>收到<code>Leader</code>发来的<code>pb.MsgApp</code>消息时调用的<code>handleAppendEntries</code>方法中，传递给<code>findConflictByTerm</code>的<code>index</code>实际上是<code>min(m.Index, r.raftLog.lastIndex())</code>，所以这种情况一般不会出现；</p>
</li>
<li><p>否则：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">	logTerm, err := l.term(index)</span><br><span class="line">	<span class="keyword">if</span> logTerm &lt;= term || err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">	&#125;</span><br><span class="line">	index--</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> index</span><br></pre></td></tr></table></figure></li>
</ul>
<p>注释：</p>
<ul>
<li><p><code>pb.MsgAppResp</code>消息（即<code>pb.MsgApp</code>的响应消息）中的<code>RejectHint</code>，用于<code>Follower</code>在拒绝<code>pb.MsgApp</code>消息时向<code>Leader</code>提示下次发送的日志条目（即<code>Leader</code>下次会尝试发送<code>Index</code>为<code>RejectHint+1</code>的日志条目），<code>LogTerm</code>就是<code>RejectHint</code>对应的<code>Term</code>。注意：旧版本的<code>raft</code>实现中，<code>pb.MsgAppResp</code>消息中没有<code>LogTerm</code>字段；如果Follower的日志为空的话，则<code>LogTerm</code>为0；</p>
</li>
<li><p>正常情况下，<code>Leader</code>的日志要比<code>Follower</code>的日志要长，而且<code>Follower</code>的日志就是<code>Leader</code>日志的前缀，即不会出现分叉的情况。这种情况下，<code>RejectHint</code>就会是<code>Follower</code>最后一条日志的<code>Index</code>；</p>
</li>
<li><p>但是如果发生网络分区，系统负载过高等情况，则<code>Follower</code>的日志和<code>Leader</code>的日志就会分叉，这种情况下如果逐个根据<code>RejectHint</code>试探的话，可能会发生很长的时间，所以才有了下面的优化算法。</p>
</li>
<li><p>考虑<code>Leader</code>的场景：</p>
<pre><code>| idx      | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    |
</code></pre>
<p>| ——– | —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| term (L) | 1    | 3    | 3    | 3    | 5    | 5    | 5    | 5    | 5    |<br>| term (F) | 1    | 1    | 1    | 1    | 2    | 2    |      |      |      |</p>
<p><code>Leader</code>在发送了<code>(idx=9,term=5)</code>后的日志条目后，收到<code>Follower</code>发来的<code>(RejectHint=6, LogTerm=2)</code>的失败响应消息。如果单纯按照<code>RejectHint</code>，<code>Leader</code>下次发<code>(idx=6,term=5)</code>后的日志条目依然会失败。但是实际上<code>Leader</code>只要观察自己的日志，就能知道<code>Index</code>为<code>6,5,4,3,2</code>的日志条目的<code>Term</code>都要比<code>LogTerm(2)</code>要大，肯定会失败。</p>
<p><code>Follower</code>告知<code>Leader</code>其<code>Index</code>为<code>6</code>的日志<code>Term</code>为<code>2</code>，因为<font color=red>日志的<code>Term</code>值只能是递增的</font>，所以在<code>6</code>之前的日志的<code>Term</code>肯定小于等于<code>2</code>，所以<code>Leader</code>需要调用<code>findConflictByTerm(RejectHint, LogTerm)</code>，找到自己日志的日志中，<code>Term</code>小于等于<code>2</code>，<code>Index</code>小于等于<code>6</code>的最大的<code>Index</code>，最终找到的是<code>1</code>，这样下次发送<code>(idx=1,term=1)</code>后的日志条目就会成功。</p>
</li>
<li><p>考虑<code>Follower</code>的场景：</p>
<pre><code>| idx      | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    |
</code></pre>
<p>| ——– | —- | —- | —- | —- | —- | —- | —- | —- | —- |<br>| term (L) | 1    | 3    | 3    | 3    | 3    | 3    | 3    | 3    | 7    |<br>| term (F) | 1    | 3    | 3    | 4    | 4    | 5    | 5    | 6    | 6    |</p>
<p><code>Leader</code>发送<code>(idx=9, term=7)</code>后的日志条目后，<code>Leader</code>收到<code>Follower</code>发来的<code>(RejectHint=9, LogTerm=6)</code>的失败响应消息。因为<code>Leader</code>前一个的日志条目<code>(idx=8, term=3)</code>，其<code>Term</code>已经小于<code>6</code>，所以<code>Leader</code>会继续发送<code>(idx=8, term=3)</code>后的日志条目，但是还是收到<code>(RejectHint=8, LogTerm=6)</code>的失败响应，<code>Leader</code>继续按照上面的方法，发送<code>(idx=7, term=3)</code>后的日志条目，然后继续失败下去，直到发送<code>(idx=3, term=3)</code>后的日志条目才会成功。</p>
<p>所以，这种情况下需要在<code>Follower</code>端进行优化。当<code>Follower</code>收到<code>(idx=8, term=3)</code>后的日志条目，它可以得出结论：<code>Leader</code>的日志中，<code>Index</code>小于<code>8</code>的条目它们的<code>Term</code>值肯定是小于等于<code>3</code>的，所以<code>Follower</code>需要调用<code>findConflictByTerm(8, 3)</code>，找到自己的日志中，<code>Term</code>小于等于<code>3</code>，<code>Index</code>小于等于<code>8</code>的最大<code>Index</code>，得到返回值<code>3</code>，发回<code>(RejectHint=3, LogTerm=3)</code>的失败响应，这样<code>Leader</code>下次发送<code>AppendEntry</code>消息就会成功。</p>
</li>
</ul>
<hr>
<h4 id="lastTerm"><a href="#lastTerm" class="headerlink" title="lastTerm"></a>lastTerm</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> lastTerm() <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法返回<code>raftLog</code>中最后一条日志条目的<code>Term</code>值。该方法通过调用<code>l.term(l.lastIndex())</code>实现。</p>
<hr>
<h4 id="isUpToDate"><a href="#isUpToDate" class="headerlink" title="isUpToDate"></a>isUpToDate</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> isUpToDate(lasti, term <span class="type">uint64</span>) <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>该方法根据给定的<code>lasti</code>和<code>term</code>判断，其是否和<code>raftLog</code>中最后一条日志条目一样新或者更新。与论文中描述的一致，该方法返回<code>true</code>的条件是：</p>
<ul>
<li>如果<code>term</code>大于<code>l.lastTerm()</code>，则返回<code>true</code>；</li>
<li>如果<code>term</code>等于<code>l.lastTerm()</code>，则判断<code>lasti</code>是否大于等于<code>l.lastIndex()</code>；</li>
<li>如果<code>term</code>小于<code>l.lastTerm()</code>，则返回<code>false</code>；</li>
</ul>
<hr>
<h4 id="mustCheckOutOfBounds"><a href="#mustCheckOutOfBounds" class="headerlink" title="mustCheckOutOfBounds"></a>mustCheckOutOfBounds</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> mustCheckOutOfBounds(lo, hi <span class="type">uint64</span>) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>该方法检查<code>lo</code>和<code>hi</code>是否满足不等式：<code>l.firstIndex &lt;= lo &lt;= hi &lt;= l.lastIndex()+1</code>;</p>
<hr>
<h4 id="slice"><a href="#slice" class="headerlink" title="slice"></a>slice</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> slice(lo, hi, maxSize <span class="type">uint64</span>) ([]pb.Entry, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>raftLog</code>中，<code>Index</code>在<code>[lo, hi)</code>范围内的日志条目，返回的日志条目的总字节数要小于<code>maxSize</code>。该方法首先尝试获取<code>l.storage</code>中的条目，如果尚有空余，则获取<code>l.unstable</code>中的日志条目。函数逻辑如下：</p>
<ul>
<li>调用<code>mustCheckOutOfBounds</code>，保证不等式：<code>l.firstIndex &lt;= lo &lt;= hi &lt;= l.lastIndex()+1</code>；</li>
<li>如果<code>lo</code>小于<code>l.unstable.offset</code>，则首先从<code>l.storage</code>中获取部分日志条目：<ul>
<li>首先调用<code>l.storage.Entries(lo, min(hi, l.unstable.offset), maxSize)</code>尝试获取日志条目，获取到的日志放在<code>storedEnts</code>中，注意这里<code>Index</code>的上届是<code>min(hi, l.unstable.offset)</code>；</li>
<li>如果调用<code>l.storage.Entries</code>没有报错，则在满足条件<code>uint64(len(storedEnts)) &lt; min(hi, l.unstable.offset)-lo</code>的情况下，直接返回从<code>l.storage</code>中获取的结果。这里判断的依据是，正常情况下，如果<code>l.storage.Entries</code>返回的日志条目没有超过<code>maxSize</code>的限制，则<code>uint64(len(storedEnts))</code> 等于 <code>min(hi, l.unstable.offset)-lo</code>，如果小于<code>min(hi, l.unstable.offset)-lo</code>，说明已经达到了<code>maxSize</code>的限制，因此直接返回即可；而不满足该条件，说明尚未达到<code>maxSize</code>的限制，可以继续获取<code>l.unstable</code>中的日志条目；</li>
</ul>
</li>
<li>接下来，如果<code>hi</code>大于<code>l.unstable.offset</code>的情况下，可以继续从<code>l.unstable</code>中获取日志条目：调用<code>l.unstable.slice(max(lo, l.unstable.offset), hi)</code>获取，注意这里的下界是<code>max(lo, l.unstable.offset)</code>。如果调用<code>l.unstable.slice</code>没有报错，则将函数拿到的日志条目，与之前调用<code>l.storage.Entries</code>得到的日志条目整合；</li>
<li>最后，对整合后的结果进行<code>maxSize</code>限制判断，并返回相应的结果；</li>
</ul>
<hr>
<h4 id="entries"><a href="#entries" class="headerlink" title="entries"></a>entries</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> entries(i, maxsize <span class="type">uint64</span>) ([]pb.Entry, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>raftLog</code>中，<code>Index</code>在<code>[i, l.lastIndex()+1)</code>范围内的日志条目，返回的日志条目的总字节数要小于<code>maxSize</code>。该方法通过调用<code>l.slice(i, l.lastIndex()+1, maxsize)</code>实现；</p>
<hr>
<h4 id="allEntries"><a href="#allEntries" class="headerlink" title="allEntries"></a>allEntries</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> allEntries() []pb.Entry</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>raftLog</code>中所有的日志条目，目前正式代码中没有调用该方法的地方。该方法通过调用<code>l.entries(l.firstIndex(), noLimit)</code>实现，如果第一次调用<code>l.entries</code>时返回的错误是<code>ErrCompacted</code>，说明可能正在保存快照，因此需要在调用一次<code>allEntries</code>方法，直到返回的错误不是<code>ErrCompacted</code>为止；<font color=red>奇怪的点</font></p>
<hr>
<h4 id="append"><a href="#append" class="headerlink" title="append"></a>append</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> <span class="built_in">append</span>(ents ...pb.Entry) <span class="type">uint64</span></span><br></pre></td></tr></table></figure>

<p>该方法尝试将<code>ents</code>日志<code>append</code>到<code>l.unstable</code>中，该方法调用<code>l.unstable.truncateAndAppend(ents)</code>实现，最后返回新的<code>l.lastIndex()</code>。</p>
<p>而<code>l.unstable.truncateAndAppend(ents)</code>以<code>ents</code>的日志条目为准，根据<code>ents</code>中<code>Index</code>的范围，可能需要覆盖源<code>l.unstable.entries</code>中的日志条目：</p>
<ul>
<li>如果<code>ents[0].Index</code>刚好等于<code>l.unstable.offset+uint64(len(l.unstable.entries))</code>，说明<code>ents</code>和<code>l.unstable.entries</code>刚好可以无缝衔接，因此直接将<code>ents append</code>到<code>l.unstable.entries</code>即可；</li>
<li>如果<code>ents[0].Index</code>小于等于<code>l.unstable.offset</code>，则需要完全丢弃原有的<code>l.unstable.entries</code>，而用<code>ents</code>进行替代，同时将<code>l.unstable.offset</code>置为<code>ents[0].Index</code>；</li>
<li>如果<code>ents[0].Index</code>在<code>(l.unstable.offset, l.unstable.offset+uint64(len(u.entries)))</code>范围内，则首先保留<code>l.unstable.entries</code>中<code>Index</code>在<code>[l.unstable.offset, ents[0].Index)</code>之间的部分，然后用<code>ents</code>覆盖剩下的部分；</li>
<li>如果<code>ents[0].Index</code>大于<code>l.unstable.offset+uint64(len(l.unstable.entries)))</code>，则会在调用<code>l.unstable.slice</code>方法时<code>panic</code>；</li>
</ul>
<p>比如当前<code>raftLog</code>中的<code>storage</code>中的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;&#125;</code>，而<code>unstable</code>中日志为空，且<code>unstable.offset</code>为<code>3</code>。则：</p>
<ul>
<li>如果<code>append</code>的日志是<code>[]pb.Entry&#123;&#125;</code>，则<code>append</code>之后，<code>unstable</code>中日志为空，<code>unstable.offset</code>为<code>3</code>，<code>append</code>返回值，即新的<code>lastIndex()</code>为<code>2</code>；</li>
<li>如果<code>append</code>的日志是<code>[]pb.Entry&#123;&#123;Index: 3, Term: 2&#125;&#125;</code>，则<code>append</code>之后，<code>unstable</code>中日志为<code>[]pb.Entry&#123;&#123;Index: 3, Term: 2&#125;&#125;</code>，<code>unstable.offset</code>为<code>3</code>，<code>append</code>返回值，即新的<code>lastIndex()</code>为<code>3</code>；</li>
<li>如果<code>append</code>的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 2&#125;&#125;</code>，则<code>append</code>之后，<code>unstable</code>中日志为<code>[]pb.Entry&#123;&#123;Index: 1, Term: 2&#125;&#125;</code>，<code>unstable.offset</code>为<code>1</code>，<code>append</code>返回值，即新的<code>lastIndex()</code>为<code>1</code>；</li>
<li>如果<code>append</code>的日志是<code>[]pb.Entry&#123;&#123;Index: 2, Term: 3&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>，则<code>append</code>之后，<code>unstable</code>中日志为<code>[]pb.Entry&#123;&#123;Index: 2, Term: 3&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>，<code>unstable.offset</code>为<code>2</code>，<code>append</code>返回值，即新的<code>lastIndex()</code>为<code>3</code>；</li>
</ul>
<hr>
<h4 id="maybeAppend"><a href="#maybeAppend" class="headerlink" title="maybeAppend"></a>maybeAppend</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> maybeAppend(index, logTerm, committed <span class="type">uint64</span>, ents ...pb.Entry) (lastnewi <span class="type">uint64</span>, ok <span class="type">bool</span>)</span><br></pre></td></tr></table></figure>

<p><code>Follower/Canididate</code>收到<code>Leader</code>发来<code>pb.MsgApp</code>消息时，即在<code>raft.handleAppendEntries</code>方法中会调用本方法。该方法尝试调用<code>raftLog.append</code>将<code>ents</code>日志追加到<code>l.unstable</code>中，并更新<code>l.committed</code>。该方法如果在追加成功的情况下，返回<code>ents</code>最后一个条目的<code>Index</code>，即<code>index+uint64(len(ents))</code>，该信息会附加到<code>pb.MsgAppResp</code>类型的消息中，回复给<code>Leader</code>；否则返回<code>0, false</code>。</p>
<ul>
<li><code>index</code>和<code>logTerm</code>实际上就是论文中的<code>prevLogIndex</code>和<code>prevLogTerm</code>，即要追加的日志<code>ents</code>的上一条的日志的<code>Index</code>和<code>Term</code>，可以认为是<code>ents[-1]</code>的<code>Index</code>和<code>Term</code>，所以有：<code>ents[i].Index == index+i+1</code>；<code>index</code>和<code>logTerm</code>可用于标识附加的位置，他们必须与<code>raftLog</code>现有日志中的某日志条目的<code>Index</code>和<code>Term</code>匹配。</li>
<li>参数<code>ents</code>表示要<code>append</code>到<code>raftLog</code>中的日志；</li>
<li><code>committed</code>是<code>AppendEntry</code>中附带的<code>Leader</code>的<code>committed</code>，用于更新<code>Follower</code>的<code>l.committed</code>。</li>
</ul>
<p>该方法的逻辑是：</p>
<ul>
<li>首先调用<code>l.matchTerm(index, logTerm)</code>判断<code>index</code>和<code>logTerm</code>与<code>raftLog</code>的现有日志条目的<code>Index</code>和<code>Term</code>匹配，如果该函数返回<code>false</code>，则直接返回<code>0, false</code>；</li>
<li>调用<code>l.findConflict(ents)</code>，找到第一个与<code>raftLog</code>中现有日志条目不一致的日志<code>Index：ci</code>；</li>
<li>如果<code>ci</code>等于<code>0</code>，则<code>raftLog</code>现有日志已经完全包含<code>ents</code>，所以不会追加日志，直接返回<code>0, false</code>；</li>
<li>如果<code>ci</code>小于等于现在的<code>l.committed</code>，则直接<code>Panic</code>；</li>
<li>其他情况下，直接调用<code>l.append(ents[ci-index-1]...)</code>，即将<code>ents</code>中<code>ci</code>对应的日志条目之前的内容都丢弃后，追加到<code>l.unstable</code>中；</li>
<li>最后，调用<code>l.commitTo</code>，更新<code>l.committed</code>，将其置为<code>committed</code>和<code>index+uint64(len(ents))</code>的较小值，即更新的<code>commiIndex</code>一定是在<code>ents</code>的范围内的。最后返回<code>index+len(ents), true</code>；</li>
</ul>
<p>比如当前<code>raftLog</code>中的unstable中的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 2&#125;, &#123;Index: 3, Term: 3&#125;&#125;</code>，调用<code>maybeAppend(1, 1, 3, []pb.Entry&#123;&#123;Index: 2, Term: 4&#125;, &#123;Index: 3, Term: 4&#125;&#125;...)</code>后，该函数返回<code>3, true</code>，此时<code>raftLog</code>中的<code>unstable</code>中的日志是<code>[]pb.Entry&#123;&#123;Index: 1, Term: 1&#125;, &#123;Index: 2, Term: 4&#125;, &#123;Index: 3, Term: 4&#125;&#125;</code>，<code>committed</code>为<code>3</code>。</p>
<hr>
<h4 id="commitTo"><a href="#commitTo" class="headerlink" title="commitTo"></a>commitTo</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> commitTo(tocommit <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法将<code>l.committed</code>置为<code>tocommit</code>，该方法保证不会减少当前<code>l.committed</code>的值。设置之前对参数<code>tocommit</code>进行检查：如果<code>tocommit</code>大于当前<code>l.committed</code>，并且大于<code>l.lastIndex()</code>的话，则直接<code>Panic</code>；</p>
<hr>
<h4 id="appliedTo"><a href="#appliedTo" class="headerlink" title="appliedTo"></a>appliedTo</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> appliedTo(i <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法将<code>l.applied</code>置为<code>i</code>，该方法保证不会减少当前<code>l.applied</code>的值。设置之前对参数<code>i</code>进行检查：如果<code>i</code>等于<code>0</code>，则直接return；如果<code>i</code>大于当前<code>l.committed</code>，或者小于当前<code>l.applied</code>的话，则直接<code>Panic</code>；</p>
<hr>
<h4 id="stableTo"><a href="#stableTo" class="headerlink" title="stableTo"></a>stableTo</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> stableTo(i, t <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法直接调用 <code>l.unstable.stableTo(i, t)</code>实现。即在将<code>unstable</code>中日志保存到<code>storage</code>中后，调用该方法，丢弃<code>unstable</code>中<code>Index</code>为<code>i</code>以及之前的日志条目，同时更新<code>l.unstable.offset</code>。</p>
<hr>
<h4 id="stableSnapTo"><a href="#stableSnapTo" class="headerlink" title="stableSnapTo"></a>stableSnapTo</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> stableSnapTo(i <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法直接调用 <code>l.unstable.stableSnapTo(i)</code>。在将<code>unstable</code>中<code>snapshot</code>保存到<code>storage</code>中后，调用该方法，丢弃<code>l.unstable.snapshot</code>，即在<code>i</code>等于<code>u.snapshot.Metadata.Index</code>的情况下，将<code>u.snapshot</code>置为<code>nil</code>；</p>
<hr>
<h4 id="unstableEntries"><a href="#unstableEntries" class="headerlink" title="unstableEntries"></a>unstableEntries</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> unstableEntries() []pb.Entry</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>l.unstable.entries</code>。</p>
<hr>
<h4 id="hasNextEnts和nextEnts"><a href="#hasNextEnts和nextEnts" class="headerlink" title="hasNextEnts和nextEnts"></a>hasNextEnts和nextEnts</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> hasNextEnts() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> nextEnts() (ents []pb.Entry)</span><br></pre></td></tr></table></figure>

<p><code>hasNextEnts</code>判断当前<code>raftLog</code>中是否有可以应用到状态机中的日志，在判断是否要发送到外部模块的<code>Ready</code>结构时会调用本方法。具体的判断逻辑是：</p>
<ul>
<li><code>off := max(l.applied+1, l.firstIndex())</code>，即如果<code>l.applied</code>小于等于快照的<code>Index</code>，即<code>l.unstable.snapshot.Metadata.Index</code>或<code>l.ents[0].Index</code>，则<code>off</code>等于快照的<code>Index + 1</code>；否则<code>off</code>为<code>l.applied + 1</code>；</li>
<li>如果<code>l.committed+1</code>大于<code>off</code>，返回<code>true</code>，否则返回<code>false</code>；</li>
</ul>
<p><code>nextEnts</code>在<code>hasNextEnts</code>为<code>true</code>的情况下，调用<code>l.slice</code>方法返回<code>Index</code>在<code>[off, l.committed]</code>范围内的日志条目，返回的日志条目的总字节数要小于<code>l.maxNextEntsSize</code>。</p>
<hr>
<h4 id="hasPendingSnapshot"><a href="#hasPendingSnapshot" class="headerlink" title="hasPendingSnapshot"></a>hasPendingSnapshot</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> hasPendingSnapshot() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>该方法判断当前<code>raftLog</code>中是否有需要应用的快照，具体的判断逻辑是：<code>l.unstable.snapshot</code> 不是<code>nil</code>，并且 <code>l.unstable.snapshot.Metadata.Index</code>不等于<code>0</code>；</p>
<hr>
<h4 id="snapshot"><a href="#snapshot" class="headerlink" title="snapshot"></a>snapshot</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> snapshot() (pb.Snapshot, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>该方法返回<code>l.unstable.snapshot</code>或<code>l.storage.Snapshot()</code>，即<code>l.unstable.snapshot</code>为<code>nil</code>的情况下返回<code>l.storage.Snapshot()</code>；</p>
<hr>
<h4 id="zeroTermOnErrCompacted"><a href="#zeroTermOnErrCompacted" class="headerlink" title="zeroTermOnErrCompacted"></a>zeroTermOnErrCompacted</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> zeroTermOnErrCompacted(t <span class="type">uint64</span>, err <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>该方法在<code>err</code>为<code>nil</code>的情况下返回<code>t</code>；如果<code>err</code>为<code>ErrCompacted</code>，则返回<code>0</code>；其他情况直接<code>Panic</code>；</p>
<hr>
<h4 id="maybeCommit"><a href="#maybeCommit" class="headerlink" title="maybeCommit"></a>maybeCommit</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> maybeCommit(maxIndex, term <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>该方法在满足一定条件的情况下，调用<code>l.commitTo</code>将<code>l.committed</code>改为<code>maxIndex</code>，并返回<code>true</code>，这里的条件是：<code>maxIndex</code>大于<code>l.committed</code>，并且<code>l.zeroTermOnErrCompacted(l.term(maxIndex)) == term</code>，第二个条件可以简化为<code>l.term(maxIndex) == term</code>；</p>
<hr>
<h4 id="restore"><a href="#restore" class="headerlink" title="restore"></a>restore</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> restore(s pb.Snapshot)</span><br></pre></td></tr></table></figure>

<p>节点崩溃后重启时，或者<code>Follower</code>收到<code>Leader</code>发来的快照时，调用该方法。根据参数<code>pb.Snapshot</code>，将<code>l.committed</code>设置为<code>s.Metadata.Index</code>，并且将<code>l.unstable.snapshot</code>设置为<code>s</code>，并将<code>l.unstable.entries</code>置为<code>nil</code>，而<code>l.unstable.offset</code>置为<code>s.Metadata.Index + 1</code>。</p>
<hr>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-07raft包--node和rawnode</title>
    <url>/2021/12/01/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/07raft%E5%8C%85--node%E5%92%8Crawnode/</url>
    <content><![CDATA[<h3 id="架构回顾"><a href="#架构回顾" class="headerlink" title="架构回顾"></a>架构回顾</h3><p>如《概述》中所述，<code>etcd/raft</code>将Raft所有算法逻辑实现为一个状态机，该状态机由输入，即消息来驱动，并输出<code>Ready</code>结构。</p>
<span id="more"></span>
<img src="/img/01整体框架/v2-ef297c4a45c3d20daff24e49b79b5e41_720w.jpg" alt="img" style="zoom: 40%;" />

<p>上面的架构图中，<code>node</code> 模块提供了如下功能：</p>
<ul>
<li>提供<code>raft状态机</code>和外界交互的接口，供外部应用，即<code>Etcd</code>向<code>raft状态机</code>提交请求。比如外部<code>Client</code>发起的请求，经过<code>etcd</code>的处理后，就是通过<code>node.Propose</code>接口向<code>raft状态机</code>提交一个提案，这个接口就是将外部请求转换成<code>raft状态机</code>认识的提案消息，并通过 Channel 传递给驱动<code>raft状态机</code>运转的 Coroutine；</li>
<li>提供驱动<code>raft状态机</code>运转的 Coroutine，其负责监听所有消息 Channel，一旦收到消息就会调用<code>raft状态机</code>处理消息的接口 <code>raft.Step</code> ，并接收<code>raft状态机</code>输出的 <code>Ready</code> 结构，将其通过 Channel 发给外部<code>etcd</code>的 Coroutine 处理；</li>
</ul>
<p><code>raft</code>包中的<code>node.go</code>和<code>rawnde.go</code>，实现了上面架构中的<code>node</code>模块的功能，提供<code>raft状态机</code>的对外接口和驱动 Coroutine；</p>
<h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><p><code>node.go</code>中首先定义了<code>Node</code>接口，其中定义了<code>node</code>模块应该实现的所有函数接口，这些接口实际上就是提供给外部使用的驱动<code>raft状态机</code>运转的对外接口。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Node <span class="keyword">interface</span> &#123;</span><br><span class="line">	<span class="comment">//该接口用于递增raft状态机内部的逻辑时钟，即Raft中的定时器。逻辑时钟以tick为单位，</span></span><br><span class="line">    <span class="comment">//目前Raft中的定时器主要是选举超时和心跳超时两种。</span></span><br><span class="line">	Tick()</span><br><span class="line"></span><br><span class="line">	<span class="comment">//该接口用于将节点由Follower转为Candidate角色，并开始选举流程（该接口绕过了选举超时时间）；</span></span><br><span class="line">	Campaign(ctx context.Context) <span class="type">error</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment">//该接口用于向raft状态机发起提案消息，追加新的日志条目。</span></span><br><span class="line">    <span class="comment">//注意proposals可能会默默的丢掉，调用者自己负责重试；</span></span><br><span class="line">	Propose(ctx context.Context, data []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//该接口用于触发配置变更。配置变更proposal和其他proposal一样，也有可能被默默丢掉；特别是</span></span><br><span class="line">	<span class="comment">//当Leader发现当前日志中有尚未完成提交的配置变更条目时，对于新的配置变更proposal，直接丢掉；</span></span><br><span class="line">	<span class="comment">//该接口接收pb.ConfChange(已废弃) 或 pb.ConfChangeV2 消息。后者就是joint consensus方法</span></span><br><span class="line">	<span class="comment">//对应的消息，允许一次变更多个成员。使用这种消息的前提是集群中所有节点的RAFT版本都认识V2的API；</span></span><br><span class="line">	ProposeConfChange(ctx context.Context, cc pb.ConfChangeI) <span class="type">error</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//该接口通过特定消息驱动raft状态机运转；</span></span><br><span class="line">	Step(ctx context.Context, msg pb.Message) <span class="type">error</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//该接口返回一个channel，通过该channel可以接收Ready结构。Ready就是当前状态的集合；</span></span><br><span class="line">	<span class="comment">//使用Node的Client在检索Ready中的状态后，需要调用Advance接口。</span></span><br><span class="line">	<span class="comment">//注意：在上一个Ready中的已提交日志和快照完成应用之前，不能应用下一个Ready中的已提交日志和快照；</span></span><br><span class="line">	Ready() &lt;-<span class="keyword">chan</span> Ready</span><br><span class="line"></span><br><span class="line">	<span class="comment">//该接口用于Client通知Node，其已经完成了上一个Ready的保存，可以接收下一个Ready了；</span></span><br><span class="line">	<span class="comment">//应用程序在将上一个Ready中的日志条目应用之后，应该调用本接口；</span></span><br><span class="line">	<span class="comment">//然而，作为一种优化手段，应用程序可以在应用命令期间调用该接口。比如上一个Ready中包含了一个快照，</span></span><br><span class="line">	<span class="comment">//应用快照需要耗费很长时间，为了能在不阻塞Raft进程的情况下继续接收Ready，他可以在应用快照完成之前调用本</span></span><br><span class="line">	<span class="comment">//接口；</span></span><br><span class="line">	Advance()</span><br><span class="line"></span><br><span class="line">	<span class="comment">//在调用ProposeConfChange接口触发配置变更后，Client可以调用本接口应用配置变更。当Client在Ready的</span></span><br><span class="line">	<span class="comment">//Ready.CommittedEntries中看到配置变更条目时，它就必须调用本接口。如果应用程序决定放弃本次配置变更，</span></span><br><span class="line">	<span class="comment">//即将其当做noop，则无需调用本接口；</span></span><br><span class="line">	<span class="comment">//本接口返回一个不透明的*pb.ConfState结构，表示当前的节点配置，该结构必须记录到快照中；</span></span><br><span class="line">	ApplyConfChange(cc pb.ConfChangeI) *pb.ConfState</span><br><span class="line"></span><br><span class="line">	<span class="comment">//Client调用本接口，驱动Leader将领导权转移到transferee节点；</span></span><br><span class="line">	TransferLeadership(ctx context.Context, lead, transferee <span class="type">uint64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Client调用本接口发起一个读请求。raft状态机处理读请求后，结果封装到ready结构。结果中包含了</span></span><br><span class="line">    <span class="comment">// 一个readIndex，只要etcd状态机应用日志条目超过了readIndex，则此时处理该读请求就可以满足</span></span><br><span class="line">    <span class="comment">// 线性一致性，从而可以安全的返回给外部了。注意：读请求也可能会丢失，所以需要Client进行重发；</span></span><br><span class="line">	ReadIndex(ctx context.Context, rctx []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//该接口返回raft状态机的当前状态，包括Term值，当前节点的角色、日志复制的进度，集群的节点配置等等；</span></span><br><span class="line">	Status() Status</span><br><span class="line"></span><br><span class="line">	<span class="comment">//本接口通知上次发送给某节点的消息不可达；</span></span><br><span class="line">	ReportUnreachable(id <span class="type">uint64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 该接口用于外部应用向Raft中的Leader报告snapshot的发送情况。参数id表示接收snapshot的Follower的ID，</span></span><br><span class="line">    <span class="comment">// SnapshotStatus表示snapshot的发送结果，可以是发送完成：SnapshotFinish，或者是</span></span><br><span class="line">    <span class="comment">// 发送失败：SnapshotFailure。只要snapshot在Leader向Follower传输过程中发生了错误，就需要调用该</span></span><br><span class="line">    <span class="comment">// 接口告知Leader。原因是：当Leader向Follower发送snapshot时，它就不会向其同步任何日志条目了，直到</span></span><br><span class="line">    <span class="comment">// Follower应用了该snapshot，并发回了响应消息为止。因此当传输发生了问题，Leader需要知道该问题才能</span></span><br><span class="line">    <span class="comment">// 采取后续的动作，否则Follower就卡主了。</span></span><br><span class="line">	ReportSnapshot(id <span class="type">uint64</span>, status SnapshotStatus)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//调用本接口，通知Node停止raft状态机的运转；</span></span><br><span class="line">	Stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>node.go</code>中接着定义了实现了<code>Node</code>接口的<code>node</code>结构：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// node is the canonical implementation of the Node interface</span></span><br><span class="line"><span class="keyword">type</span> node <span class="keyword">struct</span> &#123;</span><br><span class="line">	propc      <span class="keyword">chan</span> msgWithResult</span><br><span class="line">	recvc      <span class="keyword">chan</span> pb.Message</span><br><span class="line">	confc      <span class="keyword">chan</span> pb.ConfChangeV2</span><br><span class="line">	confstatec <span class="keyword">chan</span> pb.ConfState</span><br><span class="line">	readyc     <span class="keyword">chan</span> Ready</span><br><span class="line">	advancec   <span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">	tickc      <span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">	done       <span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">	stop       <span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;</span><br><span class="line">	status     <span class="keyword">chan</span> <span class="keyword">chan</span> Status</span><br><span class="line">	rn *RawNode</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Tick()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Campaign(ctx context.Context) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Propose(ctx context.Context, data []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Step(ctx context.Context, m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ProposeConfChange(ctx context.Context, cc pb.ConfChangeI) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Ready() &lt;-<span class="keyword">chan</span> Ready</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Advance()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ApplyConfChange(cc pb.ConfChangeI) *pb.ConfState</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Status() Status</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ReportUnreachable(id <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ReportSnapshot(id <span class="type">uint64</span>, status SnapshotStatus)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> TransferLeadership(ctx context.Context, lead, transferee <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ReadIndex(ctx context.Context, rctx []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Stop()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br></pre></td></tr></table></figure>

<p>从结构定义就可以看出，<code>node</code>结构大部分成员都是Channel。除了实现了<code>Node</code>接口的方法之外，<code>node</code>结构还实现了一个驱动<code>raft状态机</code>运转的 Coroutine，<code>node.run</code>方法就是该Coroutine的入口函数。</p>
<p><code>node</code>结构实现的<code>Node</code>接口，本质上就是向对应的Channel发送消息，最终在<code>node.run</code>中接收各个Channel上的消息，然后调用相应的方法，驱动状态机的运转。这就实现了消息处理的串行化。</p>
<hr>
<h3 id="Rawnode"><a href="#Rawnode" class="headerlink" title="Rawnode"></a>Rawnode</h3><p><code>node.run</code>方法中，收到各个Channel上的消息后，需要驱动<code>raft状态机</code>的转动，这是通过调用<code>rawnode.go</code>中实现的<code>Rawnode</code>方法实现的，<code>node</code>结构中包含一个指向<code>Rawnode</code>的指针<code>rn</code>。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> raft <span class="keyword">struct</span> &#123;</span><br><span class="line">	id <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">	Term <span class="type">uint64</span></span><br><span class="line">	Vote <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">	readStates []ReadState</span><br><span class="line"></span><br><span class="line">	<span class="comment">// the log</span></span><br><span class="line">	raftLog *raftLog</span><br><span class="line"></span><br><span class="line">	maxMsgSize         <span class="type">uint64</span></span><br><span class="line">	maxUncommittedSize <span class="type">uint64</span></span><br><span class="line">	<span class="comment">// TODO(tbg): rename to trk.</span></span><br><span class="line">	prs tracker.ProgressTracker</span><br><span class="line"></span><br><span class="line">	state StateType</span><br><span class="line"></span><br><span class="line">	<span class="comment">// isLearner is true if the local raft node is a learner.</span></span><br><span class="line">	isLearner <span class="type">bool</span></span><br><span class="line"></span><br><span class="line">	msgs []pb.Message</span><br><span class="line"></span><br><span class="line">	<span class="comment">// the leader id</span></span><br><span class="line">	lead <span class="type">uint64</span></span><br><span class="line">	<span class="comment">// leadTransferee is id of the leader transfer target when its value is not zero.</span></span><br><span class="line">	<span class="comment">// Follow the procedure defined in raft thesis 3.10.</span></span><br><span class="line">	leadTransferee <span class="type">uint64</span></span><br><span class="line">	<span class="comment">// Only one conf change may be pending (in the log, but not yet</span></span><br><span class="line">	<span class="comment">// applied) at a time. This is enforced via pendingConfIndex, which</span></span><br><span class="line">	<span class="comment">// is set to a value &gt;= the log index of the latest pending</span></span><br><span class="line">	<span class="comment">// configuration change (if any). Config changes are only allowed to</span></span><br><span class="line">	<span class="comment">// be proposed if the leader&#x27;s applied index is greater than this</span></span><br><span class="line">	<span class="comment">// value.</span></span><br><span class="line">	pendingConfIndex <span class="type">uint64</span></span><br><span class="line">	<span class="comment">// an estimate of the size of the uncommitted tail of the Raft log. Used to</span></span><br><span class="line">	<span class="comment">// prevent unbounded log growth. Only maintained by the leader. Reset on</span></span><br><span class="line">	<span class="comment">// term changes.</span></span><br><span class="line">	uncommittedSize <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">	readOnly *readOnly</span><br><span class="line"></span><br><span class="line">	<span class="comment">// number of ticks since it reached last electionTimeout when it is leader</span></span><br><span class="line">	<span class="comment">// or candidate.</span></span><br><span class="line">	<span class="comment">// number of ticks since it reached last electionTimeout or received a</span></span><br><span class="line">	<span class="comment">// valid message from current leader when it is a follower.</span></span><br><span class="line">	electionElapsed <span class="type">int</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// number of ticks since it reached last heartbeatTimeout.</span></span><br><span class="line">	<span class="comment">// only leader keeps heartbeatElapsed.</span></span><br><span class="line">	heartbeatElapsed <span class="type">int</span></span><br><span class="line"></span><br><span class="line">	checkQuorum <span class="type">bool</span></span><br><span class="line">	preVote     <span class="type">bool</span></span><br><span class="line"></span><br><span class="line">	heartbeatTimeout <span class="type">int</span></span><br><span class="line">	electionTimeout  <span class="type">int</span></span><br><span class="line">	<span class="comment">// randomizedElectionTimeout is a random number between</span></span><br><span class="line">	<span class="comment">// [electiontimeout, 2 * electiontimeout - 1]. It gets reset</span></span><br><span class="line">	<span class="comment">// when raft changes its state to follower or candidate.</span></span><br><span class="line">	randomizedElectionTimeout <span class="type">int</span></span><br><span class="line">	disableProposalForwarding <span class="type">bool</span></span><br><span class="line"></span><br><span class="line">	tick <span class="function"><span class="keyword">func</span><span class="params">()</span></span></span><br><span class="line">	step stepFunc</span><br><span class="line"></span><br><span class="line">	logger Logger</span><br><span class="line"></span><br><span class="line">	<span class="comment">// pendingReadIndexMessages is used to store messages of type MsgReadIndex</span></span><br><span class="line">	<span class="comment">// that can&#x27;t be answered as new leader didn&#x27;t committed any log in</span></span><br><span class="line">	<span class="comment">// current term. Those will be handled as fast as first log is committed in</span></span><br><span class="line">	<span class="comment">// current term.</span></span><br><span class="line">	pendingReadIndexMessages []pb.Message</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> StateType <span class="type">uint64</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	StateFollower StateType = <span class="literal">iota</span></span><br><span class="line">	StateCandidate</span><br><span class="line">	StateLeader</span><br><span class="line">	StatePreCandidate</span><br><span class="line">	numStates</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> SoftState <span class="keyword">struct</span> &#123;</span><br><span class="line">	Lead      <span class="type">uint64</span> <span class="comment">// must use atomic operations to access; keep 64-bit aligned.</span></span><br><span class="line">	RaftState StateType</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HardState <span class="keyword">struct</span> &#123;</span><br><span class="line">	Term   <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,1,opt,name=term&quot; json:&quot;term&quot;`</span></span><br><span class="line">	Vote   <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,2,opt,name=vote&quot; json:&quot;vote&quot;`</span></span><br><span class="line">	Commit <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,3,opt,name=commit&quot; json:&quot;commit&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> RawNode <span class="keyword">struct</span> &#123;</span><br><span class="line">	raft       *raft</span><br><span class="line">	prevSoftSt *SoftState</span><br><span class="line">	prevHardSt pb.HardState</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Tick()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Campaign() <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Propose(data []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> ProposeConfChange(cc pb.ConfChangeI) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> ApplyConfChange(cc pb.ConfChangeI) *pb.ConfState</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Ready() Ready</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> HasReady() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Advance(rd Ready)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Status() Status</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> BasicStatus() BasicStatus</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> WithProgress(visitor <span class="function"><span class="keyword">func</span><span class="params">(id <span class="type">uint64</span>, typ ProgressType, pr tracker.Progress)</span></span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> ReportUnreachable(id <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> ReportSnapshot(id <span class="type">uint64</span>, status SnapshotStatus)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> TransferLeader(transferee <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> ReadIndex(rctx []<span class="type">byte</span>)</span><br></pre></td></tr></table></figure>

<p><code>RawNode</code>结构中包含了指向<code>raft</code>结构的指针，所谓<code>raft状态机</code>，在代码中的体现就是<code>raft</code>结构及其定义的各种方法。</p>
<p>除了包含<code>raft</code>，<code>RawNode</code>中还包含了当前<code>raft状态机</code>的一些状态，包括：</p>
<ul>
<li>无需持久化保存的<code>SoftState</code>，记录了当前集群的<code>Leader</code>的ID，以及当前节点的角色；</li>
<li>需要持久化保存的<code>pb.HardState</code>。记录了节点的<code>Term</code>值，给哪个节点的投票，以及<code>commitIndex</code>；</li>
</ul>
<p><code>node.run</code>方法中收到各种消息后，一般就是调用相应的<code>RawNode</code>方法，而<code>RawNode</code>方法则是直接调用<code>raft</code>的方法。<code>RawNode</code>是非线程安全的<code>Node</code>，其中的大部分方法都与<code>Node</code>是重名的。</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-08选举和状态转换</title>
    <url>/2021/12/20/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/08%E9%80%89%E4%B8%BE%E5%92%8C%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h4><p>下面是Raft博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》中关于Leader选举的基本内容，节选自“3.4 Leader election ”和“3.6.2 Election restriction”。</p>
<p>当节点启动时，它们都是Follower。节点只要能从Leader或Candidate收到有效的RPC，它就会一直保持Follower状态。Leader周期性的向Follower发送心跳消息来维护自己的领导地位。如果一个Follower在一段时间（称为选举超时时间）内没有收到任何消息，那该Follower就会开始选举流程。</p>
<p>节点间通信时会交换当前Term值。如果一个节点当前Term比其他节点的小，该节点会将自己的Term更新为较大的那个值。如果一个 Candidate 或 Leader 发现自己的Term过期（较小）了，它会立即回到 Follower 状态。如果一个节点接收到一个包含过期（较小）Term的RPC消息，它会直接拒绝&#x2F;忽略这个RPC消息。</p>
<span id="more"></span>
<p>开始选举时，Follower增加自己的Term，并转为Candidate状态。然后投票给自己，并向集群中的其他节点并发的发送RequestVote请求。Candidate会一直保持这种状态，直到发送下面三种情况之一：</p>
<ul>
<li><p>a赢得选举：如果Candidate在同一任期内可以从集群中获得过半节点的投票，则该Candidate就可以赢得选举。在给定的任期内，每个节点最多只能投票给一个Candidate，先到先得（注：3.6.2节中增加了投票的额外限制）。需要获取过半节点的投票，这条规则使得特定任期内最多只会有一个Candidate赢得选举。一旦Candidate赢得选举，它就会转为Leader。然后向所有节点发送心跳消息，以确立起领导者地位，阻止新的选举。</p>
</li>
<li><p>b另一个节点确立了自己的领导者地位：在等待选票期间，Candidate可能会收到另一个声称自己是Leader的节点发来的<code>AppendEntries</code>请求。如果该RPC中附带的Term值大于等于该Candidate的本地Term，那Candidate就会承认该Leader是合法的，并回到Follower的状态。如果RPC中的Term小于该Candidate的本地Term，则Candidate拒绝该RPC，保持Candidate状态不变。</p>
</li>
<li><p>c选举超时时间内没有人赢得选举：如果同一时间内有多个Follower转为Candidate开始选举，那选票可能会被这些Candidate瓜分，从而没有Candidate能够获得过半节点的投票。这种情况发生时，每个Candidate都会再次选举超时，增加Term，然后开始新的选举。但是如果不采取额外措施，瓜分选票的情况会无限重复下去。</p>
<p>Raft采用随机的选举超时策略来确保瓜分选票的情况很少发生，即使发生了也能很快解决。选举超时时间在固定的时间范围（比如[100ms, 300ms]）内随机选择。这种随机策略会把各个节点的选举超时时刻打散，所以多数情况下只会有一个节点会超时；并且会在其他节点超时之前赢得选举并发送心跳消息。<font color=red>出现瓜分选票的情况是也采取同样的策略，每个Candidate在开始选举时会重置其随机的超时时间，等到超时后才进行下一次选举</font>；这就降低了新的选举中再次出现瓜分选票的可能性。</p>
</li>
</ul>
<p>在任何基于Leader的共识算法中，Leader最终必须包含所有已提交的日志条目。Raft采用简单的方法来保证选举时新Leader在当选时就有之前任期的所有已提交的条目，而无需将这些条目再传输给新Leader。<font color=red>这就意味着日志条目只会从Leader流向Follower，是单向的，而Leader永远不会覆盖日志中的条目。</font></p>
<p><font color=red>Raft在投票过程中增加限制：只有Candidate的日志包含了所有已提交条目，该Candidate才会当选。</font>在选举时Candidate需要与集群中过半节点通信，这就表示其中至少有一个节点包含所有已提交的条目。如果Candidate的日志至少与过半节点的日志一样新（下面定义了“新”的概念），那就能保证该Candidate包含了所有已提交的条目。在RequestVote RPC实现了该限制：RPC中包含了Candidate的日志信息，投票者判断如果自己的日志比Candidate新，则拒绝投票。</p>
<p><font color=red>Raft通过比较日志中最后一个条目的Index和Term来确定哪个日志更“新”。</font>如果两份日志的最后条目的Term不同，则具有更大Term的日志更新；如果Term一样，则Index更大的更新；</p>
<p><font color=red>个人理解</font>，基于这个“新”的定义，并不足以保证当选的Leader包含了所有已提交的条目，某些场景下还是会有问题，所以才有了3.6.2中的<font color=red>日志提交约束。</font>，即Raft不能仅通过计算副本数来提交之前任期创建的条目。针对当前任期创建的条目，通过计算副本数进行提交才是安全可行的，根据日志匹配属性，一旦当前任期中的条目以这种方法（计算副本数）提交了，那之前的条目也就间接的提交了。</p>
<h4 id="PreVote优化"><a href="#PreVote优化" class="headerlink" title="PreVote优化"></a>PreVote优化</h4><p>上面的流程基本上能工作，但是还有优化的余地，因此在9.6节”Preventing disruptions when a server rejoins the cluster”中增加了PreVote的机制。下面是该章节的部分内容：</p>
<p>基本的选举算法有一个缺点，一个曾经被分区隔离的集群节点，重新连接到集群时可能会造成集群中断。节点被隔离后，它收不到心跳消息，因此会增加Term以开始选举，此时它肯定无法成为Leader。但当该节点稍后恢复连接时，其较大的Term将传播到集群的其余节点（通过RequestVote请求或AppendEntries响应）。这将迫使正常的Leader转为Follower，然后必须进行新的选举以选出新的Leader。</p>
<p>Raft的基本选举算法可以扩展一个额外的阶段以防止这种干扰，即PreVote过程。<font color=red>在PreVote过程中，Candidate只有从群集中多数节点那里得知他们愿意给自己投票时（假设Candidate的日志足够最新，并且Voters在至少一个选举超时时间内没有收到有效Leader的心跳），才增加其Term。</font>该算法实际上是受到了ZooKeeper算法的启发。</p>
<p>PreVote过程解决了隔离节点在重新连接集群时中断集群的问题。当节点被分区时，它将不能增加其Term，因为它不能从集群的大多数接收选票。然后，当它重新加入集群时，它仍然无法增加它的Term，因为其他节点将一直收到来自Leader的心跳。一旦该节点收到来自Leader自身的心跳信号，它将返回到Follower状态（在同一Term中）。</p>
<hr>
<h3 id="代码流程"><a href="#代码流程" class="headerlink" title="代码流程"></a>代码流程</h3><h4 id="初始状态StateFollower"><a href="#初始状态StateFollower" class="headerlink" title="初始状态StateFollower"></a>初始状态StateFollower</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> becomeFollower(term <span class="type">uint64</span>, lead <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> reset(term <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> resetRandomizedElectionTimeout()</span><br></pre></td></tr></table></figure>

<p>节点启动时，都是以<code>Follower</code>状态启动，即调用<code>raft.becomeFollower</code>将状态置为<code>Follower</code>。该函数中主要是：</p>
<ul>
<li>将<code>raft.step</code>回调函数置为<code>stepFollower</code>；将<code>raft.tick</code>回调函数置为<code>raft.tickElection</code>；</li>
<li>将<code>raft.state</code>设置为<code>StateFollower</code>；</li>
<li>将<code>raft.lead</code>设置为参数<code>lead</code>；</li>
<li>调用<code>raft.reset(term)</code>重置本节点Term，并初始化其他状态。</li>
</ul>
<p>除了节点启动时会调用<code>becomeFollower</code>外，还有以下情况会调用该方法：</p>
<ul>
<li><code>raft.Step</code>中，收到的消息中Term大于节点本地Term时，一般就会调用本方法转为<code>Follower</code>，以下几种情况是例外：<ul>
<li>收到的消息是 <code>pb.MsgVote</code> 或 <code>pb.MsgPreVote</code> 时，这种情况下，如果下面的条件都满足，直接忽略这两类消息：<ul>
<li>消息中的<code>Context</code>不是<code>campaignTransfer</code>。<code>Context</code>为<code>campaignTransfer</code>表示这是领导权转移的目标节点发起的PreVote或Vote流程，不能忽略。具体参考《领导权转移流程》；</li>
<li><code>r.checkQuorum &amp;&amp; r.lead != None &amp;&amp; r.electionElapsed &lt; r.electionTimeout</code> 为<code>true</code>，这个条件的意思应该就是本节点距离上次收到<code>Leader</code>发来的信息的时间差，在选举超时时间的最小值<code>r.electionTimeout</code>内，因此本节点不会理会投票消息，直接丢弃。<font color=red>这是为了防止被分区隔离的<code>Follower</code>不断增加自己的Term来进行选举，导致正常的<code>Leader</code>被迫下线。具体参考《CheckQuorum和心跳消息》。</font></li>
</ul>
</li>
<li>收到<code>pb.MsgPreVote</code>消息时，消息中的Term是发起PreVote流程的节点Term+1，所以这种情况不会使当前节点转为<code>Follower</code>；</li>
<li>收到的是<code>pb.MsgPreVoteResp</code>消息，并且其中的<code>Reject</code>为<code>false</code>，这是对于<code>pb.MsgVote</code>消息成功回复的唯一场景，因为<code>pb.MsgVote</code>中的Term是本节点的Term+1，收到<code>pb.MsgPreVoteResp</code>成功回复时，其中的Term就是<code>pb.MsgVote</code>中的Term，所以大于本节点Term是正常的；</li>
</ul>
</li>
<li>如果节点当前是<code>Candidate</code>，收到了<code>pb.MsgApp, pb.MsgHeartbeat, pb.MsgSnap</code>消息，并且消息中的Term大于等于本地Term，说明这些消息都是有效<code>Leader</code>发来的消息，因此调用本方法转为<code>Follower</code>；</li>
<li>如果节点当前是<code>Candidate</code>，统计选票结果为<code>quorum.VoteLost</code>，调用本方法转为<code>Follower</code>；</li>
<li>作为<code>Leader</code>，在<code>stepLeader</code>中，收到了<code>pb.MsgCheckQuorum</code>消息，并且此时<code>r.prs.QuorumActive()</code>得到的结果为<code>false</code>，则调用本方法转为<code>Follower</code>。<font color=red>这是<code>Leader</code>的<code>CheckQuorum</code>检查失败，即<code>Leader</code>在一段时间内无法得到集群内Majority个节点的响应，说明该<code>Leader</code>很可能被分区隔离了，所以<code>Leader</code>主动降级为<code>Follower</code>。具体参考《CheckQuorum和心跳消息》</font>;</li>
</ul>
<p><code>raft.reset</code>方法主要在状态转换，即<code>becomeFollower</code>, <code>becomeCandidate</code>, <code>becomeLeader</code>中调用。该函数中除了会重置节点本地的Term之外，还会初始化并重置其他的一些状态：</p>
<ul>
<li>在<code>term</code>不等于当前<code>raft.Term</code>的情况下，重置<code>raft.Term</code>为参数<code>term</code>；并且将<code>raft.Vote</code>重置为<code>None</code>；</li>
<li>重置<code>raft.lead</code>为<code>None</code>；</li>
<li>将<code>raft.electionElapsed</code>，<code>raft.heartbeatElapsed</code>这两个时钟tick重置为<code>0</code>；并且调用<code>raft.resetRandomizedElectionTimeout()</code>，重新生成随机的超时选举时间<code>raft.randomizedElectionTimeout</code>，其值的范围是<code>[raft.electiontimeout, 2 * raft.electiontimeout - 1]</code>；</li>
<li>调用<code>raft.abortLeaderTransfer()</code>，将<code>raft.leadTransferee</code>重置为<code>None</code>；</li>
<li>调用<code>raft.prs.ResetVotes()</code>，重置记录投票结果的<code>ProgressTracker.Votes</code>；</li>
<li><font color=red>重建每个节点对应的<code>tracker.Progress</code>结构</font>：<ul>
<li>如果是本节点对应<code>tracker.Progress</code>结构，则将<code>Progress.Match</code>置为当前最后日志条目的Index，否则将<code>Progress.Match</code>置为<code>0</code>；</li>
<li>将<code>Progress.Next</code>置为当前最后日志条目的Index加<code>1</code>；</li>
<li>重建<code>Progress.Inflights</code>；</li>
<li>保留原来的<code>Progress.IsLearner</code>属性；</li>
<li>这里并没有显示的设置<code>Progress.StateProbe</code>属性，所以其值为<code>0</code>，即<code>StateProbe</code>；</li>
</ul>
</li>
<li>将<code>raft.pendingConfIndex</code>重置为0；将<code>raft.uncommittedSize</code>重置为0；新建<code>raft.readOnly</code>结构；</li>
</ul>
<hr>
<h4 id="选举超时"><a href="#选举超时" class="headerlink" title="选举超时"></a>选举超时</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Tick()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Tick()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> tickElection()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> promotable() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> pastElectionTimeout() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p><code>node.Tick</code>方法就是向<code>node.tickc</code>管道中发一个<code>struct&#123;&#125;&#123;&#125;</code>，即一个信号。一般在<code>raft</code>模块外部，收到一个由<code>time.NewTicker</code>创建的管道上的定时信号后就会调用<code>node.Tick</code>方法，即起到一个<font color=red>逻辑时钟滴答</font>的功能。</p>
<p>在<code>node.run</code>方法中，收到<code>node.Tick</code>中向<code>node.tickc</code>管道中发送的信号之后，就会调用<code>raft.tick</code>回调函数，针对<code>Follower</code>而言，该回调函数就是<code>raft.tickElection</code>方法。</p>
<p>在<code>raft.tickElection</code>方法中，首先对<code>raft.electionElapsed</code>进行自增，然后判断是否已经满足开始选举流程的条件，这里的条件有两个：</p>
<ul>
<li>由<code>raft.promotable</code>判断当前节点是否可以进行选举，满足以下条件才返回<code>true</code>：节点本身的<code>id</code>存在于<code>raft.prs.Progress</code>中，即节点是集群中的一员；节点不是<code>Learner</code>；节点当前没有尚未应用的快照；</li>
<li>由<code>raft.pastElectionTimeout</code>方法判断当前<code>raft.electionElapsed</code>是否已经到了选举超时时间<code>raft.randomizedElectionTimeout</code>。<code>raft.randomizedElectionTimeout</code>是在<code>raft.resetRandomizedElectionTimeout</code>中设置的，是<code>[raft.electiontimeout, 2 * raft.electiontimeout - 1]</code>范围内的随机值，其中<code>raft.electiontimeout</code>是一个配置决定的选举超时基准时间。而<code>raft.resetRandomizedElectionTimeout</code>方法是在<code>raft.reset</code>方法中调用，即当节点状态转换时会重新设置<code>raft.randomizedElectionTimeout</code>选举超时时间。</li>
</ul>
<p>满足选举的条件后，在<code>raft.tickElection</code>方法中，就会以一个<code>pb.MsgHup</code>类型的本地消息调用<code>raft.Step</code>方法。<code>pb.MsgHup</code>消息没有消息体，关键的就只是消息类型，主要用于在<code>raft.Step</code>方法中触发PreVote或Vote流程。</p>
<p>在<code>raft.Step</code>方法中，针对<code>pb.MsgHup</code>消息，根据<code>raft.preVote</code>是否为<code>true</code>，来决定发起PreVote或者Vote流程。这里<code>raft.preVote</code>是个配置选项，当前版本应该都支持PreVote流程。</p>
<hr>
<h4 id="PreVote流程"><a href="#PreVote流程" class="headerlink" title="PreVote流程"></a>PreVote流程</h4><h5 id="发送pb-MsgPreVote消息，状态转为StatePreCandidate"><a href="#发送pb-MsgPreVote消息，状态转为StatePreCandidate" class="headerlink" title="发送pb.MsgPreVote消息，状态转为StatePreCandidate"></a>发送pb.MsgPreVote消息，状态转为StatePreCandidate</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> hup(t CampaignType)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> campaign(t CampaignType)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> becomePreCandidate()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> poll(id <span class="type">uint64</span>, t pb.MessageType, v <span class="type">bool</span>) (granted <span class="type">int</span>, rejected <span class="type">int</span>, result quorum.VoteResult)</span><br></pre></td></tr></table></figure>

<p><code>raft.hub</code>的参数类型<code>CampaignType</code>，针对本方法而言，值可能为<code>campaignPreElection</code>或<code>campaignElection</code>。<code>campaignPreElection</code>表示PreVote流程，<code>campaignElection</code>表示Vote流程。<code>raft.hub</code>方法就是进行一系列检查后，调用<code>raft.campaign</code>方法开始PreVote或Vote流程。这里的检查项包括：</p>
<ul>
<li>当前<code>raft.state</code>不能是<code>StateLeader</code>，即当前节点不能是<code>Leader</code>；</li>
<li><code>raft.promotable</code>返回必须是<code>true</code>，即：节点本身的id存在于<code>raft.prs.Progress</code>中，即节点是集群中的一员；节点不是<code>Learner</code>；节点当前没有尚未应用的快照；</li>
<li>节点当前的日志中，Index在<code>[raft.raftLog.applied+1, raft.raftLog.committed+1]</code>的日志条目，即已提交未应用的日志条目中，不能有配置变更日志；</li>
<li>满足上面的条件后，调用<code>raft.campaign(t)</code>，开始PreVote流程；</li>
</ul>
<p><code>raft.campaign</code>方法，针对参数<code>t</code>为<code>campaignPreElection</code>的PreVote流程而言：</p>
<ul>
<li><p>首先调用<code>raft.becomePreCandidate</code>方法，该方法主要动作是：</p>
<ul>
<li>将<code>raft.step</code>回调函数置为<code>stepCandidate</code>；</li>
<li>调用<code>raft.prs.ResetVotes</code>方法，将<code>ProgressTracker.Votes</code>重置，即清空记录投票结果的map；</li>
<li>将<code>raft.tick</code>回调函数置为<code>raft.tickElection</code>；</li>
<li>将<code>raft.lead</code>置为<code>None</code>，将<code>raft.state</code>置为<code>StatePreCandidate</code>；</li>
<li><font color=red>注意</font>，这里没有重置<code>raft.Term</code>（即本节点的Term），也没有修改<code>raft.Vote</code>（即将选票投给自己）；</li>
</ul>
</li>
<li><p>接下来以本节点id，以及true为参数调用<code>raft.poll</code>方法，得到投给自己一票后的PreVote结果。<code>raft.poll</code>方法就是先将节点id以及投票结果v记录到<code>ProgressTracker.Votes</code>这个map中，然后调用<code>ProgressTracker.TallyVotes</code>方法，统计<code>ProgressTracker.Votes</code>中的投票结果并返回；</p>
</li>
<li><p>如果<code>raft.poll</code>返回结果为<code>quorum.VoteWon</code>，说明<font color=red>当前集群中只有一个节点</font>，因此直接进入下一步流程。针对PreVote流程而言，下一步就是调用<code>raft.campaign(campaignElection)</code>开始Vote流程；然后直接返回；</p>
</li>
<li><p>接下来，针对<code>raft.prs.Voters</code>中的每个节点（排除本节点），以id从小到大的顺序，发送<code>pb.MsgPreVote</code>消息。消息的结构如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	Term: term, <span class="comment">//节点Term+1</span></span><br><span class="line">	To: id, </span><br><span class="line">	Type: pb.MsgPreVote, </span><br><span class="line">	Index: r.raftLog.lastIndex(), <span class="comment">//最后日志条目的Index</span></span><br><span class="line">	LogTerm: r.raftLog.lastTerm(), <span class="comment">//最后日志条目的Term</span></span><br><span class="line">    Context: ctx <span class="comment">//nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><font color=red>注意</font>，PreVote流程中并没有修改节点的Term，所以<code>pb.MsgPreVote</code>消息中附带的Term是节点Term+1，否则其他节点不会投票给自己。</p>
</li>
</ul>
<hr>
<h5 id="接收处理pb-MsgPreVote消息"><a href="#接收处理pb-MsgPreVote消息" class="headerlink" title="接收处理pb.MsgPreVote消息"></a>接收处理pb.MsgPreVote消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>其他节点收到<code>pb.MsgPreVote</code>消息后，最终都在<code>raft.Step</code>方法中处理。</p>
<p><code>raft.Step</code>首先根据消息中的Term做不同的处理，<font color=red>一般的处理方式</font>是：</p>
<ul>
<li>如果消息中Term为<code>0</code>，则表示本地消息，不做特殊处理，在后面的流程中根据消息类型进行具体处理；</li>
<li>如果消息中Term大于本地Term，则一般表示本地的Term已经过期了，所以会直接调用<code>raft.becomeFollower</code>转为<code>Follower</code>状态，然后再根据消息类型做具体处理；</li>
<li>如果消息中Term小于本地Term，表示消息源节点是个过期节点，一般直接忽略消息，不在继续处理；</li>
</ul>
<p>针对<code>pb.MsgPreVote</code>消息，Step中的处理逻辑是：</p>
<ul>
<li><p>如果消息中的Term大于节点本地Term，并且下面的条件都满足，则直接返回，不再处理<code>pb.MsgPreVote</code>消息：</p>
<ul>
<li>消息中的<code>Context</code>不是<code>campaignTransfer</code>；<code>Context</code>为<code>campaignTransfer</code>表示这是领导权转移的目标节点发起的PreVote或Vote流程，不能忽略。具体参考《领导权转移流程》；</li>
<li><code>raft.checkQuorum &amp;&amp; raft.lead != None &amp;&amp; raft.electionElapsed &lt; raft.electionTimeout</code> 为<code>true</code>，这个条件的意思应该就是本节点距离上次收到<code>Leader</code>发来的信息的时间差，在选举超时时间的最小值<code>raft.electionTimeout</code>内，因此本节点不会理会投票消息，直接丢弃。<font color=red>这是为了防止被分区隔离的<code>Follower</code>不断增加自己的Term来进行选举，导致正常的<code>Leader</code>被迫下线。具体参考《CheckQuorum和心跳消息》。</font></li>
</ul>
</li>
<li><p>如果消息中的Term小于节点本地Term，则需要发送<code>Reject</code>为<code>true</code>的<code>pb.MsgPreVoteResp</code>响应消息，而不是像常规处理方式那样直接忽略。如果直接忽略，可能会造成集群的死锁（节点相互等待对方的投票）。具体的解释参考最后的<font color=red>“PreVote导致死锁”</font>的一节；</p>
</li>
<li><p>走到这里，表示消息中的Term大于等于节点本地Term，根据下面的条件判断是否可以投票：</p>
<ul>
<li><p><code>raft.raftLog.isUpToDate(m.Index, m.LogTerm)</code>返回<code>true</code>，即根据消息中的Index和LogTerm，发起选举的节点的日志要比本节点的日志更新。不满足该条件则会拒绝投票；</p>
</li>
<li><p><code>raft.Vote == m.From</code>为<code>true</code>（可以给同一节点重复投票），或者<code>raft.Vote == None &amp;&amp; raft.lead == None</code>（本节点没有<code>Leader</code>节点，也没有投过票），或者<code>m.Type == pb.MsgPreVote &amp;&amp; m.Term &gt; raft.Term</code>（收到的是<code>pb.MsgPreVote</code>消息，且消息中的Term大于本地Term）；</p>
</li>
<li><p>根据上面的条件决定发送的<code>pb.MsgPreVoteResp</code>响应消息内容。<code>pb.MsgPreVoteResp</code>响应消息格式如下：、</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message&#123;</span><br><span class="line">	To: m.From, </span><br><span class="line">	Term: m.Term / r.Term, <span class="comment">//见下面注释</span></span><br><span class="line">	Type: pb.MsgPreVoteResp,</span><br><span class="line">	Reject: <span class="literal">true</span> / <span class="literal">false</span> <span class="comment">//true表示投票，false则拒绝投票</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>如果可以投票，则消息中的Term使用的是<code>pb.MsgPreVote</code>消息中的Term，而不是节点本地Term，根据注释，这是因为：如果一个节点之前被隔离了，则其Term值可能是过时的（较小），如果响应消息中附带的是本地Term，则发起PreVote的节点可能会直接忽略本节点的响应消息；</li>
<li>如果拒绝投票，则消息中的Term使用的是本地Term；</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h5 id="接收处理pb-MsgPreVoteResp消息"><a href="#接收处理pb-MsgPreVoteResp消息" class="headerlink" title="接收处理pb.MsgPreVoteResp消息"></a>接收处理pb.MsgPreVoteResp消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepCandidate</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>发起PreVote流程的节点收到<code>pb.MsgPreVoteResp</code>消息后，最终都在<code>raft.Step</code>方法中处理。其中针对<code>pb.MsgPreVoteResp</code>消息的处理逻辑是：</p>
<ul>
<li><p>如果消息中的Term大于本地Term，并且消息中的<code>Reject</code>为<code>false</code>（这是<font color=red>PreVote成功的唯一情况</font>），此时消息中的Term为之前发送的<code>pb.MsgPreVote</code>消息中附带的Term，即本节点的Term+1，因此说明可以获取对方的选票，因此剩下的流程在<code>stepCandidate</code>中处理；</p>
</li>
<li><p>如果消息中的Term大于本地Term，但是消息中的<code>Reject</code>为<code>true</code>，此时消息中的Term就是对端节点的Term，因此本节点的Term小于对端节点的Term，所以直接调用<code>r.becomeFollower(m.Term, None)</code>，转为<code>Follower</code>状态，并更新自己的Term为更高的对端节点的Term值。</p>
<p>这里只要收到一个<code>Reject</code>为<code>true</code>，且<code>Term</code>较大的<code>pb.MsgPreVoteResp</code>消息就立即转为<code>Follower</code>状态，而不是等待其他节点的回复，这里也是没有问题的：如果其他节点可以PreVote成功，说明其他节点具备了相应的条件，如果其他节点也无法PreVote成功，则本节点在选举超时后以更大的Term进行PreVote，就有可能PreVote成功；</p>
</li>
<li><p>如果消息中的Term小于本地Term，则根据上面<code>pb.MsgPreVote</code>消息的流程，此时<code>Reject</code>一定为<code>true</code>，所以直接忽略，不再继续处理；</p>
</li>
<li><p>剩下的流程在<code>raft.stepCandidate</code>中处理，该方法中就是统计选票，即根据消息源节点id，以及<code>!Reject</code>为参数调用<code>raft.poll</code>方法，统计PreVote结果：</p>
<ul>
<li>如果结果为<code>quorum.VoteWon</code>，则可以进行正式投票了，调用<code>raft.campaign(campaignElection)</code>开始Vote流程；</li>
<li>如果结果为<code>quorum.VoteLost</code>，则调用<code>raft.becomeFollower(r.Term, None)</code>转为<code>Follower</code>状态；</li>
<li>其他情况说明当前选票还不到半数，所以需要等待其他<code>pb.MsgPreVoteResp</code>消息，一直等到可以通过<code>raft.poll</code>方法获取确切的投票结果，或者等到选举超时时间后，再次发起PreVote流程；</li>
</ul>
</li>
</ul>
<hr>
<h4 id="Vote流程"><a href="#Vote流程" class="headerlink" title="Vote流程"></a>Vote流程</h4><p>Vote流程与PreVote流程，除了一些细节外基本一致。为了方便查阅，下面的描述跟PreVote有冗余的地方。</p>
<h5 id="发送pb-MsgVote消息，状态转为StateCandidate"><a href="#发送pb-MsgVote消息，状态转为StateCandidate" class="headerlink" title="发送pb.MsgVote消息，状态转为StateCandidate"></a>发送pb.MsgVote消息，状态转为StateCandidate</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> campaign(t CampaignType)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> becomeCandidate()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> reset(term <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> poll(id <span class="type">uint64</span>, t pb.MessageType, v <span class="type">bool</span>) (granted <span class="type">int</span>, rejected <span class="type">int</span>, result quorum.VoteResult)</span><br></pre></td></tr></table></figure>

<p>在<code>stepCandidate</code>回调函数中，PreVote收集到过半的选票后，调用<code>raft.campaign(campaignElection)</code>开始Vote流程。</p>
<p><code>raft.campaign</code>方法，针对参数<code>t</code>为<code>campaignElection</code>的Vote流程而言：</p>
<ul>
<li><p>首先调用<code>raft.becomeCandidate</code>方法，该方法主要动作是：</p>
<ul>
<li>将<code>raft.step</code>回调函数置为<code>stepCandidate</code>；</li>
<li>调用<code>raft.reset(r.Term + 1)</code>，增加本节点的Term；同时初始化一些状态，包括将集群中其他节点的<code>tracker.Progress</code>进行重置：将<code>Match</code>置为0，而<code>Next</code>置为<code>r.raftLog.lastIndex() + 1</code>；调用<code>r.prs.ResetVotes</code>方法，将<code>ProgressTracker.Votes</code>重置，即清空记录投票结果的<code>map</code>；</li>
<li>将<code>raft.tick</code>回调函数置为<code>raft.tickElection</code>；</li>
<li>将<code>raft.state</code>置为<code>StateCandidate</code>；</li>
<li>将<code>raft.Vote</code>置为自己的id，表示投票给自己；</li>
<li><font color=red>注意</font>，与<code>raft.becomePreCandidate</code>不同，这里重置了<code>raft.Term</code>（即本节点的Term），也也修改了<code>raft.Vote</code>（即将选票投给自己）；</li>
</ul>
</li>
<li><p>接下来以本节点id，以及<code>true</code>为参数调用<code>raft.poll</code>方法，得到投给自己一票后的PreVote结果。<code>raft.poll</code>方法就是先将节点id以及投票结果<code>v</code>记录到<code>ProgressTracker.Votes</code>这个<code>map</code>中，然后调用<code>ProgressTracker.TallyVotes</code>方法，统计<code>ProgressTracker.Votes</code>中的投票结果并返回；</p>
</li>
<li><p>如果<code>raft.poll</code>返回结果为<code>quorum.VoteWon</code>，说明当前集群中只有一个节点，因此直接进入下一步流程。针对Vote流程而言，下一步就是调用<code>raft.becomeLeader()</code>直接转为<code>Leader</code>状态；然后直接返回；</p>
</li>
<li><p>接下来，针对<code>raft.prs.Voters</code>中的每个节点（排除本节点），以<code>id</code>从小到大的顺序，发送<code>pb.MsgVote</code>消息。这里的结构如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	Term: term, <span class="comment">//节点Term</span></span><br><span class="line">	To: id, </span><br><span class="line">	Type: pb.MsgVote, </span><br><span class="line">	Index: r.raftLog.lastIndex(), <span class="comment">//最后日志条目的Index</span></span><br><span class="line">	LogTerm: r.raftLog.lastTerm(), <span class="comment">//最后日志条目的Term</span></span><br><span class="line">    Context: ctx <span class="comment">//nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h5 id="接收处理pb-MsgVote消息"><a href="#接收处理pb-MsgVote消息" class="headerlink" title="接收处理pb.MsgVote消息"></a>接收处理pb.MsgVote消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>其他节点收到<code>pb.MsgVote</code>消息后，最终都在<code>raft.Step</code>方法中处理。其中针对<code>pb.MsgVote</code>消息的处理逻辑是：</p>
<ul>
<li><p>如果消息中的Term大于节点本地Term，并且下面的条件都满足，则直接返回，不再处理<code>pb.MsgVote</code>消息：</p>
<ul>
<li>消息中的<code>Context</code>不是<code>campaignTransfer</code>；<code>Context</code>为<code>campaignTransfer</code>表示这是领导权转移的目标节点发起的PreVote或Vote流程，不能忽略。具体参考《领导权转移流程》；</li>
<li><code>raft.checkQuorum &amp;&amp; raft.lead != None &amp;&amp; raft.electionElapsed &lt; raft.electionTimeout</code> 为<code>true</code>，这个条件的意思是本节点距离上次收到<code>Leader</code>发来的信息的时间差，在选举超时时间的最小值<code>raft.electionTimeout</code>内，因此本节点不会理会投票消息，直接丢弃。<font color=red>这是为了防止被分区隔离的<code>Follower</code>不断增加自己的Term来进行选举，导致正常的<code>Leader</code>被迫下线。具体参考《CheckQuorum和心跳消息》。</font></li>
</ul>
</li>
<li><p>如果消息中的Term大于节点本地Term，并且上面的条件不满足，则调用<code>raft.becomeFollower(m.Term, None)</code>转为<code>Follower</code>状态，并且设置<code>raft.lead</code>为<code>None</code>（下面要用到该条件）；</p>
</li>
<li><p>如果消息中的Term小于本节点Term，则不予处理；</p>
</li>
<li><p>走到这里，此时节点本地Term肯定等于消息中的Term。根据下面的条件判断是否可以投票：</p>
<ul>
<li><p><code>raft.raftLog.isUpToDate(m.Index, m.LogTerm)</code>返回<code>true</code>，即根据消息中的<code>Index</code>和<code>LogTerm</code>，发起选举的节点的日志要比本节点的日志更新。不满足该条件则会拒绝投票；</p>
</li>
<li><p><code>raft.Vote == m.From</code>为<code>true</code>（可以给同一节点重复投票），或者<code>raft.Vote == None &amp;&amp; raft.lead == None</code>（本节点没有<code>Leader</code>节点，也没有投过票），或者<code>m.Type == pb.MsgPreVote &amp;&amp; m.Term &gt; raft.Term</code>（收到的是<code>pb.MsgPreVote</code>消息，且消息中的Term大于本地Term）；</p>
</li>
<li><p>根据上面的条件决定发送的<code>pb.MsgVoteResp</code>响应消息内容。<code>pb.MsgVoteResp</code>响应消息格式如下：、</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message&#123;</span><br><span class="line">	To: m.From, </span><br><span class="line">	Term: m.Term / r.Term, <span class="comment">//见下面注释</span></span><br><span class="line">	Type: pb.MsgPreVoteResp,</span><br><span class="line">	Reject: <span class="literal">true</span> / <span class="literal">false</span> <span class="comment">//true表示投票，false则拒绝投票</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>针对Vote流程而言，走到这一步了，此时<code>m.Term</code>肯定等于本地<code>r.Term</code>；所以虽然代码中针对<code>Reject</code>为<code>true</code>或<code>false</code>的情况，分别使用了<code>m.Term</code>和<code>r.Term</code>，但是此时它们肯定是一样的；</li>
<li>在可以投票的情况下，将<code>raft.electionElapsed</code>置为0，同时记录<code>raft.Vote</code>；</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h5 id="接收处理pb-MsgVoteResp消息"><a href="#接收处理pb-MsgVoteResp消息" class="headerlink" title="接收处理pb.MsgVoteResp消息"></a>接收处理pb.MsgVoteResp消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepCandidate</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>发起Vote流程的节点收到<code>pb.MsgVoteResp</code>消息后，最终都在<code>raft.Step</code>方法中处理。其中针对<code>pb.MsgVoteResp</code>消息的处理逻辑是：</p>
<ul>
<li><p>如果消息中的Term大于本地Term，则调用<code>raft.becomeFollower(m.Term, None)</code>转为<code>Follower</code>状态；</p>
</li>
<li><p>如果消息中的Term小于本地Term，则不予处理；</p>
</li>
<li><p>剩下的流程在<code>raft.stepCandidate</code>中处理，该方法中就是统计选票，即根据消息源节点<code>id</code>，以及<code>!Reject</code>为参数调用<code>raft.poll</code>方法，统计<code>PreVote</code>结果：</p>
<ul>
<li>如果结果为<code>quorum.VoteWon</code>，则调用 <code>raft.becomeLeader() 和 raft.bcastAppend()</code>，状态转为<code>Leader</code>，并发送空的日志条目；</li>
<li>如果结果为<code>quorum.VoteLost</code>，则调用<code>raft.becomeFollower(raft.Term, None)</code>转为<code>Follower</code>状态；</li>
<li>其他情况说明当前选票还不到半数，所以需要等待其他<code>pb.MsgPreVoteResp</code>消息，一直等到可以通过<code>raft.poll</code>方法获取确切的投票结果，或者等到选举超时时间后，再次发起PreVote流程；</li>
</ul>
</li>
</ul>
<hr>
<h4 id="状态转为StateLeader"><a href="#状态转为StateLeader" class="headerlink" title="状态转为StateLeader"></a>状态转为StateLeader</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> becomeLeader()</span><br></pre></td></tr></table></figure>

<p>调用<code>raft.becomeLeader</code>将状态置为<code>Leader</code>，其逻辑是：</p>
<ul>
<li>将<code>step</code>回调函数置为<code>stepLeader</code>;</li>
<li>调用<code>raft.reset(r.Term)</code>，重置一些状态；</li>
<li>将<code>tick</code>回调函数置为<code>raft.tickHeartbeat</code>；</li>
<li>设置<code>raft.lead</code>为自己的<code>id</code>；</li>
<li>将<code>raft.state</code>置为<code>StateLeader</code>；</li>
<li>调用<code>progress.BecomeReplicate</code>方法，将节点自己对应的<code>Progress</code>中，状态置为<code>StateReplicate</code>，将<code>Next</code>置为<code>Match+1</code>；</li>
<li>将<code>raft.pendingConfIndex</code>置为最后一个日志条目的Index，这样做的效果就是直到<code>Leader</code>当前的所有日志都应用到状态机之前，该<code>Leader</code>不会接受新的配置变更提案；这样做的目的就是为了防止多个配置变更并行执行，具体参考《成员变更流程》；</li>
<li>生成一个包含空数据的Entry（即no-op日志条目），然后将其追加到本地日志中。这里对应的是小论文中的<code>5.4.2</code>一节和大论文的<code>3.6.2</code>一节，即增加的<font color=red>日志提交约束</font>：<code>Leader</code>不能直接提交之前保存的日志，而是在当选之后首先追加一条no-op日志，这条no-op日志一旦被提交，就可以间接的导致之前保存的日志被提交。增加这条限制的原因是为了防止已提交的日志被覆盖，具体的场景可以参考论文或《日志追加》中的描述。</li>
<li>最后，调用<code>raft.reduceUncommittedSize([]pb.Entry&#123;emptyEnt&#125;)</code>，<font color=red>具体参考《Ready的处理及其他》；</font></li>
</ul>
<hr>
<h3 id="PreVote导致死锁"><a href="#PreVote导致死锁" class="headerlink" title="PreVote导致死锁"></a>PreVote导致死锁</h3><p>根据<a href="https://github.com/etcd-io/etcd/issues/8501%E4%B8%AD%E7%9A%84%E6%8F%8F%E8%BF%B0%EF%BC%8C%E8%80%83%E8%99%91%E4%B8%8B%E9%9D%A2%E7%9A%84%E5%9C%BA%E6%99%AF%EF%BC%9A">https://github.com/etcd-io/etcd/issues/8501中的描述，考虑下面的场景：</a></p>
<p>Raft库是从某个版本才开始支持PreVote的，因此在将不支持PreVote的Raft库升级到支持PreVote的Raft库时出现了问题。升级采用的是rolling restart方式，即集群内节点依次重启的方式，因此这个过程中，集群中某些节点是支持PreVote的，而另一些则是不支持PreVote。</p>
<p>考虑3个节点的集群，节点为<code>n1,n2,n3</code>，此时<code>n1</code>和<code>n2</code>支持PreVote，而<code>n3</code>尚不支持，<code>n1</code>为<code>Leader</code>，<code>n2</code>和<code>n3</code>为<code>Follower</code>。<code>n1,n2,n3</code>的Term此时都是2，然后经过下面的步骤：</p>
<ul>
<li><code>n3</code>被分区隔离了；</li>
<li><code>n1</code>向<code>n1, n2</code>和<code>n3</code>进行AppendEntry，此时<code>n1</code>和<code>n2</code>可以Append成功，所以<code>n1</code>和<code>n2</code>最后的日志索引要比<code>n3</code>大；</li>
<li><code>n3</code>选举超时后，因其不支持PreVote，因此直接开始Vote流程，增加自身的Term后开始选举。经过一段时间后，<code>n3</code>的Term值要远大于<code>n1</code>和<code>n2</code>的Term值；</li>
<li><code>n3</code>节点被升级，重启后可以支持PreVote了；</li>
<li><code>n3</code>网络恢复，与<code>n1</code>和<code>n2</code>重新连接；</li>
<li><code>n2</code>成为新的<code>Leader</code>，接下来<code>n2</code>被分区隔离，与<code>n1,n3</code>失去联系；</li>
<li><code>n1</code>和<code>n3</code>开始选举，都开始PreVote流程。此时<code>n3</code>的Term比<code>n1</code>大，而<code>n1</code>的最后日志条目Index比<code>n3</code>大；</li>
<li>这种情况下，<code>n1</code>不会给<code>n3</code>投票，因为<code>n1</code>的日志比<code>n3</code>要新，而<code>n3</code>也不会给<code>n1</code>投票，因为<code>n1</code>的Term较小。这就造成了<code>n1</code>和<code>n3</code>互相等待对方的投票，形成了死锁的局面。</li>
<li>为了解决该问题，在PreVote流程中，如果收到的<code>pb.MsgPreVote</code>消息中的Term小于节点本地Term，则需要发送<code>Reject</code>为<code>true</code>的<code>pb.MsgPreVoteResp</code>响应消息，而不是直接忽略，响应消息中的Term就是本地较大的Term。</li>
<li>因此，上面的场景中，<code>n3</code>收到<code>n1</code>的<code>pb.MsgPreVote</code>消息后，发送<code>pb.MsgPreVoteResp</code>响应消息，其中的<code>Reject</code>为<code>true</code>，且Term为本地较大的Term。<code>n1</code>收到该响应消息后，在<code>raft.Step</code>方法中，因消息中的Term大于本地Term，因此调用<code>raft.becomeFollower(m.Term, None)</code>，即以消息中的Term更新本地Term，转为<code>Follower</code>；当其选举再次超时后，因<code>n1</code>当前的Term已经与<code>n3</code>一样大，且其日志更新，所以<code>n1</code>最终可以赢得选举，成为新的<code>Leader</code>；</li>
</ul>
<hr>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/366661148">https://zhuanlan.zhihu.com/p/366661148</a></li>
<li><a href="https://www.jianshu.com/p/1496228df9a9">https://www.jianshu.com/p/1496228df9a9</a></li>
<li><a href="https://github.com/etcd-io/etcd/issues/8501">https://github.com/etcd-io/etcd/issues/8501</a></li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-11成员变更1-confchange包</title>
    <url>/2022/02/15/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/11%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B41-confchange%E5%8C%85/</url>
    <content><![CDATA[<p><code>confchang</code>包中包含两个文件：<code>confchange.go</code>和<code>restore.go</code>。<code>confchange.go</code>主要实现了配置变更的具体操作；<code>restore.go</code>主要实现了节点崩溃时或者收到<code>snapshot</code>时恢复配置状态相关的一系列动作；</p>
<p>本文介绍的是配置变更的基础模块，都是<code>confchange.go</code>中的实现逻辑。配置变更的流程细节请参考《配置变更2-流程》一同食用。</p>
<span id="more"></span>
<p><code>confchange.go</code>实现了节点配置变更的具体操作，即收到<code>confchange</code>消息后，执行配置变更动作，如添加节点、删除节点等。它实现了单节点变更（<code>simple</code>）和联合共识（<code>joint consensus</code>）两种配置变更方法，并且在变更时，会对一些<code>Invariant</code>（不变式）做验证，以便校验配置变更的合法性。</p>
<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p><code>confchange.go</code>中的逻辑都是在<code>Changer</code>结构的方法中实现的。<code>Changer</code>结构的定义如下，它实际上就是对<code>tracker.ProgressTracker</code>的封装。<font color=red>所谓集群配置变更，就是对<code>tracker.ProgressTracker</code>的修改</font>：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> Changer <span class="keyword">struct</span> &#123;</span><br><span class="line">	Tracker   tracker.ProgressTracker</span><br><span class="line">	LastIndex <span class="type">uint64</span> <span class="comment">//最新的日志索引</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="incoming和outgoing"><a href="#incoming和outgoing" class="headerlink" title="incoming和outgoing"></a>incoming和outgoing</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">incoming</span><span class="params">(voters quorum.JointConfig)</span></span> quorum.MajorityConfig      &#123; <span class="keyword">return</span> voters[<span class="number">0</span>] &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">outgoing</span><span class="params">(voters quorum.JointConfig)</span></span> quorum.MajorityConfig      &#123; <span class="keyword">return</span> voters[<span class="number">1</span>] &#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">outgoingPtr</span><span class="params">(voters *quorum.JointConfig)</span></span> *quorum.MajorityConfig &#123; <span class="keyword">return</span> &amp;voters[<span class="number">1</span>] &#125;</span><br></pre></td></tr></table></figure>

<p>在联合共识算法中，<code>Tracker</code>中包含了配置变更时的<code>Cnew,old</code>配置。代码中，以<code>incoming</code>（即<code>voters[0]</code>）和<code>outgoing</code>（即<code>voters[1]</code>）表示。在配置变更之前，只有<code>incoming</code>，发生配置变更时将<code>incoming</code>拷贝一个副本，该副本就是<code>outgoing</code>，然后将变更应用到<code>incoming</code>中，因此，<code>incoming</code>就是所谓的<code>Cnew</code>，而<code>outgoing</code>是<code>Cold</code>。</p>
<p><font color=red>注意</font>，<code>incoming</code>和<code>outgoing</code>都是指的<code>Voter</code>；</p>
<p>根据当前<code>outgoing</code>的长度，即可判断当前是否处于联合共识配置变更过程中：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">joint</span><span class="params">(cfg tracker.Config)</span></span> <span class="type">bool</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">len</span>(outgoing(cfg.Voters)) &gt; <span class="number">0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在进行<code>Simple</code>配置变更时，要保证<code>Cold</code>和<code>Cnew</code>只有一个节点的差异，这是通过<code>symdiff</code>函数来实现：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">symdiff</span><span class="params">(l, r <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="keyword">struct</span>&#123;&#125;)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">	<span class="keyword">var</span> n <span class="type">int</span></span><br><span class="line">	pairs := [][<span class="number">2</span>]quorum.MajorityConfig&#123;</span><br><span class="line">		&#123;l, r&#125;, <span class="comment">// count elems in l but not in r</span></span><br><span class="line">		&#123;r, l&#125;, <span class="comment">// count elems in r but not in l</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> _, p := <span class="keyword">range</span> pairs &#123;</span><br><span class="line">		<span class="keyword">for</span> id := <span class="keyword">range</span> p[<span class="number">0</span>] &#123;</span><br><span class="line">			<span class="keyword">if</span> _, ok := p[<span class="number">1</span>][id]; !ok &#123;</span><br><span class="line">				n++</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> n</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>按照该函数的逻辑，如果<code>incoming &amp; outgoing</code>是<code>(1)&amp;(1 2)</code>，则他们的差值是<code>1</code>，即<code>outgoing</code>删除一个节点变成<code>incoming</code>，而<code>(1)&amp;(2)</code>这样的配置变更，差值是<code>2</code>，即<code>outgoing</code>要先删除一个节点<code>2</code>，然后添加一个节点<code>1</code>，才能变成<code>incoming</code>。</p>
<hr>
<h3 id="不变式检查"><a href="#不变式检查" class="headerlink" title="不变式检查"></a>不变式检查</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkInvariants</span><span class="params">(cfg tracker.Config, prs tracker.ProgressMap)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>配置变更时，要保证不变式的成立。检查不变式是在<code>checkInvariants</code>中实现的。有下面一些不变式：</p>
<ul>
<li><code>tracker.Config</code>中的<code>Voters</code>，<code>Learners</code>和<code>LearnersNext</code>，其中包含的每个节点，都能在<code>tracker.ProgressMap</code>找到对应的<code>Progress</code>；</li>
<li><code>tracker.Config.LearnersNext</code>中的节点，表示<code>Voter</code>中将要被降级为<code>Learner</code>的节点，因此其中的节点必须包含在<code>outgoing</code>中，而且对应的<code>Progress.IsLearner</code>必须为<code>false</code>；</li>
<li><code>Learners ∩ Voters == ∅</code>，即<code>tracker.Config.Learners</code>中的节点，不能出现在<code>outgoing</code>和<code>incoming</code>中；而且对应的<code>Progress.IsLearner</code>必须为<code>true</code>；</li>
<li>如果当前没有执行配置变更（或者已经完成了），则<code>outgoing</code>必须为<code>nil</code>，<code>tracker.Config.LearnersNext</code>必须为<code>nil</code>，而且<code>tracker.Config.AutoLeave</code>必须是<code>false</code>；</li>
</ul>
<hr>
<h3 id="新节点初始化Progress"><a href="#新节点初始化Progress" class="headerlink" title="新节点初始化Progress"></a>新节点初始化Progress</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> initProgress(cfg *tracker.Config, prs tracker.ProgressMap, id <span class="type">uint64</span>, isLearner <span class="type">bool</span>)</span><br></pre></td></tr></table></figure>

<p>对于新加入的节点（<code>Voter</code>或者<code>Learner</code>），配置变更操作实际上就是创建该节点对应的<code>Progress</code>，并将节点加入到<code>Voters</code>或者<code>Learners</code>中。这些都是通过调用<code>Changer.initProgress</code>方法实现的。</p>
<p>该函数的逻辑是：</p>
<ul>
<li><p>参数<code>isLearner</code>表示新加入的节点是<code>Learner</code>还是<code>Voter</code>，该参数为<code>true</code>，则将<code>节点ID</code>记录到<code>cfg.Learners</code>中；否则记录到<code>incoming</code>中；</p>
</li>
<li><p>然后创建新的<code>Progress</code>，插入到<code>prs</code>这个<code>map</code>中。新创建的<code>Progress</code>的初始属性如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">prs[id] = &amp;tracker.Progress&#123;</span><br><span class="line">    <span class="comment">// 将Next置为最新的日志索引LastIndex，注意下面的注释 </span></span><br><span class="line">    <span class="comment">// TODO(tbg): seems awfully optimistic(太过乐观了). Using the first index would be</span></span><br><span class="line">    <span class="comment">// better(使用初始索引可能更好些). The general expectation(期望) here is that the follower </span></span><br><span class="line">    <span class="comment">// has no log at all (and will thus likely need a snapshot), though the app may</span></span><br><span class="line">    <span class="comment">// have applied a snapshot out of band before adding the replica (thus</span></span><br><span class="line">    <span class="comment">// making the first index the better choice).</span></span><br><span class="line">    Next:      c.LastIndex,</span><br><span class="line">    Match:     <span class="number">0</span>,</span><br><span class="line">    Inflights: tracker.NewInflights(c.Tracker.MaxInflight),</span><br><span class="line">    IsLearner: isLearner,</span><br><span class="line">    <span class="comment">// When a node is first added, we should mark it as recently active.</span></span><br><span class="line">    <span class="comment">// Otherwise, CheckQuorum may cause us to step down if it is invoked</span></span><br><span class="line">    <span class="comment">// before the added node has had a chance to communicate with us.</span></span><br><span class="line">    RecentActive: <span class="literal">true</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> remove(cfg *tracker.Config, prs tracker.ProgressMap, id <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>要删除的节点，既有可能是<code>Voter</code>，也有可能是<code>Learner</code>：</p>
<ul>
<li>如果要删除的<code>节点ID</code>在<code>prs</code>中找不到对应的<code>Progress</code>，说明该节点不是系统中的节点，则直接返回；</li>
<li>将<code>节点ID</code>从<code>incoming</code>中删除；</li>
<li>将<code>节点ID</code>从<code>cfg.Learners</code>和<code>cfg.LearnersNext</code>中删除；如果<code>节点ID</code>不在<code>outgoing</code>中（<code>Learner</code>的情况），则直接将节点对应的<code>Progress</code>从<code>prs</code>中删除；</li>
</ul>
<hr>
<h3 id="新增Voter"><a href="#新增Voter" class="headerlink" title="新增Voter"></a>新增Voter</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> makeVoter(cfg *tracker.Config, prs tracker.ProgressMap, id <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>新增<code>Voter</code>节点，既有可能是添加一个全新的<code>Voter</code>节点，也有可能是将<code>Learner</code>升级为<code>Voter</code>节点：</p>
<ul>
<li>如果<code>prs</code>中不包含<code>节点ID</code>，说明添加的是一个全新的<code>Voter</code>节点，直接调用<code>initProgress</code>即可；</li>
<li>如果是<code>Learner</code>升级成<code>Voter</code>节点，则将<code>节点ID</code>从<code>cfg.Learners</code>中，以及从<code>cfg.LearnersNext</code>中删除（这种情况，原因可能是，<code>joint consensus</code>配置变更可以包含多个变更步骤，之前的某一步将<code>Voter</code>降级为<code>Learner</code>，后面的某一步又将<code>Learner</code>升级为<code>voter</code>），并将<code>ID</code>添加到<code>incoming</code>中；</li>
</ul>
<hr>
<h3 id="新增Learner"><a href="#新增Learner" class="headerlink" title="新增Learner"></a>新增Learner</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> makeLearner(cfg *tracker.Config, prs tracker.ProgressMap, id <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>新增<code>Learner</code>节点，既有可能是添加一个全新的<code>Learner</code>节点，也有可能是将<code>Voter</code>降级为<code>Learner</code>节点：</p>
<ul>
<li>如果<code>prs</code>中不包含<code>节点ID</code>，说明添加的是一个全新的<code>Learner</code>节点，则直接调用<code>initProgress</code>即可；</li>
<li>如果是<code>Voter</code>节点降级为<code>Learner</code>节点，先调用<code>remove</code>将节点从<code>incoming</code>中删除，然后如果新加<code>节点ID</code>包含在<code>outgoing</code>中，则将其添加到<code>cfg.LearnersNext</code>中；否则（这种情况，原因可能是，<code>joint consensus</code>配置变更可以包含多个变更步骤，之前的某一步新增了全新的<code>Voter</code>，后面的某一步又将<code>Voter</code>降级为<code>Learner</code>），直接将<code>节点ID</code>添加到<code>cfg.Learners</code>中；</li>
</ul>
<hr>
<h3 id="配置变更相关"><a href="#配置变更相关" class="headerlink" title="配置变更相关"></a>配置变更相关</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> checkAndCopy() (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkAndReturn</span><span class="params">(cfg tracker.Config, prs tracker.ProgressMap)</span></span> (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p><code>checkAndCopy</code>方法复制并返回<code>c.Tracker.Config</code>和<code>c.Tracker.Progress</code>的副本，后续所作的配置变更都是在副本上执行的，然后调用<code>checkAndReturn</code>函数进行不变式的检查。</p>
<p>需要注意的是：这里<code>c.Tracker.Config</code>使用的深拷贝，而<code>c.Tracker.Progress</code>使用的是浅拷贝（根据注释，配置变更时只会修改<code>Progress</code>中的<code>Learner</code>，所以使用浅拷贝）。这里使用副本，猜测可能是为了保证配置变更违反不变式时，可以方便的回退。</p>
<p><code>checkAndReturn</code>函数就是调用<code>checkInvariants</code>检查<code>cfg</code>和<code>prs</code>是否满足不变式，不满足则返回错误；</p>
<hr>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> apply(cfg *tracker.Config, prs tracker.ProgressMap, ccs ...pb.ConfChangeSingle) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p><code>apply</code>函数就是根据<code>ccs</code>中包含的一系列配置变更消息，执行相应的动作：</p>
<ul>
<li>消息类型为<code>pb.ConfChangeAddNode</code>，则调用<code>makeVoter</code>；</li>
<li>消息类型为<code>pb.ConfChangeAddLearnerNode</code>，则调用<code>makeLearner</code>；</li>
<li>消息类型为<code>pb.ConfChangeRemoveNode</code>，则调用<code>remove</code>；</li>
<li>其他消息，报错退出；</li>
<li>如果变更后，<code>incoming</code>为空，则抛出一个”removed all voters”错误；</li>
</ul>
<hr>
<p>剩下的内容，参考《成员变更2-流程》</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-09日志追加</title>
    <url>/2022/01/02/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/09%E6%97%A5%E5%BF%97%E8%BF%BD%E5%8A%A0/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>下面是Raft博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》中关于日志追加的内容，节选自“3.5 Log replication”和“3.6.2 Committing entries from previous terms”。</p>
<p>选出Leader后，它就可以处理Client的请求。Client的请求中包含了要在复制状态机（the replicated state machine）中执行的命令。Leader将该命令作为新的条目追加到其日志中，然后向所有Follower发送AppendEntries RPC使他们复制该条目。当该条目安全的复制到大多数Follower上之后，Leader将该条目应用到状态机，并且将执行结果回复给Client。如果Follower崩溃或者运行的太慢，或者网络中丢包了，Leader会不断的重发AppendEntries（即使Leader已经回复Client了），直到所有Follower最终都保存了该条目为止。</p>
<span id="more"></span>
<p>日志条目中包含一条状态机命令，以及Leader创建该条目时的Term值。该Term用于检查节点间日志是否一致，并且用于保证表3.2中的一些安全属性。每个日志条目还有一个Index，标识其在日志中的位置。</p>
<p>一旦创建日志条目的Leader在过半节点上复制了该条目，就会提交该条目。这也会提交Leader日志中之前的所有条目，包括前任Leader创建的条目。<font color=red>Leader跟踪记录已提交日志的最大Index，即commitIndex，并且把该Index包含在AppendEntries（包括心跳消息）中，这样集群中的其他节点最终也会知道该Index。</font>实际上，所谓标记某条目已提交，就是通过递增commitIndex实现的。</p>
<p>我们设计了Raft的日志机制，以便集群中不同节点上的日志能够保持一致。这种机制简化了系统行为，使其更好预测，更重要的是，这是确保安全性的重要组件。<font color=red>Raft保证下面的属性，这些属性共同构成图3.2中的日志匹配属性</font>：</p>
<ul>
<li>不同节点的日志中，如果两个条目的Index和Term都相同，那它们保存了相同的命令；</li>
<li>不同节点的日志中，如果两个条目的Index和Term都相同，那在它们之前的日志条目也是完全相同的；</li>
</ul>
<p><font color=red>第一个属性是这样保证的：Leader在特定的Index和特定Term下只会创建一个条目，而且从来不会改变日志中的条目的位置；第二个属性是靠处理AppendEntries RPC时执行的一致性检查来保证的。</font>当Leader发送AppendEntries  RPC时，RPC中包含了preLogIndex和preLogTerm，它们表示Leader本地日志中，位于RPC中包含的新条目之前一个条目的Index和Term。如果Follower在其本地日志中找不到相同的Index和Term，就会拒绝该RPC。所以，只要AppendEntries成功返回，Leader就知道Follower的日志中，在RPC中的新条目，以及之前的条目，都跟自己是一样的。</p>
<p>正常运行时，Leader和Follower的日志保持一致，所以AppendEntries的一致性检查不会失败。但是，一旦Leader崩溃就可能导致日志不一致（旧Leader可能还没有完全复制其日志中的所有条目）。这种不一致会随着一系列的Leader和Follower的崩溃而加剧。</p>
<p><font color=red>Raft中，处理这种不一致的方式，是Leader强迫Follower复制自己的日志。这表示Follower日志中的冲突条目会被Leader中的日志条目覆盖。</font></p>
<p><font color=red>为了使Follower的日志与自己的保证一致，Leader必须找到两个节点日志中一致的Index最大的条目，删除Follower日志中该条目之后的所有日志，然后将自己日志中该条目之后的所有条目发给Follower。这些动作发生在AppendEntries的一致性检查过程中。</font>Leader为每个Follower维护一个nextIndex，表示Leader会发送给Follower的下一条日志条目的Index。Leader掌权时，会把所有nextIndex初始化为其本地日志中最后一个条目的Index加1。如果某个Follower的日志与Leader不一致，那AppendEntries的一致性检查就会失败，Follower对该AppendEntries回复“拒绝”后，Leader减少该Follower的nextIndex的值，重发AppendEntries RPC。最终nextIndex会减少到Leader和Follower的日志匹配的点。在这之后，AppendEntries就会成功，从而删除Follower中冲突的日志条目，并把Leader的日志追加到Follower中。<font color=red>一旦AppendEntries成功后，Follower的日志就与Leader保持一致了，并在当前任期内一直保持一致。</font></p>
<p>为了节省带宽，在发现与Follower的日志匹配点之前，Leader可以发送没有条目的AppendEntries RPC。然后一旦nextIndex紧跟在matchIndex之后时，说明找到了匹配点，Leader才开发发送实际的条目。</p>
<p>Leader知道如果一个当前任期创建条目在内复制到了过半节点上，那就可以提交该条目。<font color=red>如果Leader在提交条目之前崩溃了，那后续的Leader会尝试继续完成该条目的复制。然而，如果某个之前任期创建的条目已经复制到过半节点上了，Leader也不能立即将其提交。</font></p>
<p>考虑下面的场景：</p>
<ul>
<li>(a)S1是Leader，把Index2_Term2的条目复制到了部分节点上；</li>
<li>(b)S1崩溃了，S3以Term 3赢得了S3、S4以及自己的选票，成为了Leader，并且创建了Index2_Term3的条目；</li>
<li>(c)S5崩溃了，S1重启了，以Term 4成为新Leader，继续复制日志，并把Index2_Term2的条目复制到了过半节点上。</li>
<li>现在分两种情况：<ul>
<li>(d1)如果S1没有复制新条目，则Index2_Term2的条目就是S1、S2、S3的最后一个条目，此时S1崩溃了，S5的日志比较新，又重新当选了，继续复制Index2_Term3的条目，它就覆盖了S1、S2和S3上的Index2_Term2条目；</li>
<li>(d2)如果S1在崩溃之前以自己的Term复制并提交了新条目，即Index3_Term4，则S5不会当选，并且Index3_Term4之前的Index2_Term2也顺便被提交了。</li>
</ul>
</li>
</ul>
<p><font color=red>如果在(d1)场景下S1直接提交了Index2_Term2条目，则S5当选后用Index3_Term4覆盖了已提交的条目，这违反了图3.2中的状态机安全性，造成了不同节点的状态机应用了不同的日志条目。为了避免出现这种情况，Raft不能仅通过计算副本数来提交之前任期创建的条目。针对当前任期创建的条目，通过计算副本数进行提交才是安全可行的，根据日志匹配属性，一旦当前任期中的条目以这种方法（计算副本数）提交了，那之前的条目也就间接的提交了。</font></p>
<hr>
<h3 id="日志追加使用的RPC消息"><a href="#日志追加使用的RPC消息" class="headerlink" title="日志追加使用的RPC消息"></a>日志追加使用的RPC消息</h3><p>外部模块向<code>raft模块</code>提交提案，如果收到提案的是<code>Leader</code>，则其将提案转化为日志条目，然后将日志条目追加到自己以及所有<code>Follower</code>的日志中；如果收到提案的是<code>Follower</code>，则它需要将提案消息转发给<code>Leader</code>；</p>
<p><code>pb.MsgProp</code>消息就表示提案消息，其格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	Type: pb.MsgProp, </span><br><span class="line">	Entries: []pb.Entry&#123;&#123;Data: data&#125;&#125;,</span><br><span class="line">	From: id,</span><br><span class="line">	To: leaderId,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>这种类型的消息没有设置<code>Term</code>，或者说它的<code>Term</code>值被置为<code>0</code>，表示本地数据。即使是在<code>Follower</code>收到提案，需要将<code>pb.MsgProp</code>消息转发给<code>Leader</code>时，在<code>raft.send</code>函数中，针对<code>pb.MsgProp</code>消息，也不会设置<code>Term</code>。</li>
<li>如果是<code>Leader</code>直接收到提案，则其中的<code>From</code>字段是在<code>node.run</code>中收到<code>pb.MsgProp</code>消息时设置为节点自己的<code>id</code>；如果是<code>Follower</code>收到提案转发<code>pb.MsgProp</code>消息给<code>Leader</code>时，在<code>raft.send</code>函数中，会设置<code>From</code>为自己的<code>id</code>；</li>
<li>如果是<code>Leader</code>直接收到提案，则其中的<code>To</code>字段不会设置；如果是<code>Follower</code>收到提案需要转发<code>pb.MsgProp</code>消息给<code>Leader</code>时，在<code>stepFollower</code>函数中，设置<code>To</code>字段为其保存的<code>Leader</code>的<code>id</code>；</li>
</ul>
<p><code>Leader</code>收到提案后，需要将其转换成日志条目，并通过<code>AppendEntries</code>消息将日志条目追加到<code>Follower</code>中。这里所谓的<code>AppendEntries</code>消息，就是<code>pb.MsgApp</code>消息。<code>pb.MsgApp</code>的格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	Type: pb.MsgApp, </span><br><span class="line">	Entries: ents, <span class="comment">//日志条目</span></span><br><span class="line">	From: id, <span class="comment">//Leader自己的id</span></span><br><span class="line">	To: id, <span class="comment">//Follower的id</span></span><br><span class="line">	Index: Progress.Next<span class="number">-1</span>, <span class="comment">//Follower对应的Progress中的Next-1，对应论文中的prevLogIndex</span></span><br><span class="line">	LogTerm: term, <span class="comment">//Index对应的Term，对应论文中的prevLogTerm</span></span><br><span class="line">	Commit: raftLog.committed <span class="comment">//Leader本地日志的committed Index</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>Follower</code>收到<code>pb.MsgApp</code>消息，追加完日志后会回复<code>pb.MsgAppResp</code>响应消息。如果追加日志成功，则回复的<code>pb.MsgAppResp</code>消息格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	To: m.From, <span class="comment">//发送AppendEntry消息的Leader的id</span></span><br><span class="line">	Type: pb.MsgAppResp, </span><br><span class="line">	Index: mlastIndex, <span class="comment">//Follower已经复制的日志条目的最大Index</span></span><br><span class="line">	From: id, <span class="comment">//Follower自己的id</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果追加日志失败，则回复的<code>pb.MsgAppResp</code>消息格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message&#123;</span><br><span class="line">	To:         m.From,  <span class="comment">//发送AppendEntry消息的Leader的id</span></span><br><span class="line">	Type:       pb.MsgAppResp,</span><br><span class="line">	Index:      m.Index, <span class="comment">//pb.MsgApp消息中的Index，即prevLogIndex</span></span><br><span class="line">	Reject:     <span class="literal">true</span>, <span class="comment">//表示追加日志失败</span></span><br><span class="line">	RejectHint: hintIndex, <span class="comment">//通过raftLog.findConflictByTerm(min(m.Index, raftLog.lastIndex()), m.LogTerm) 计算得到的日志Index，Leader收到后根据该值决定下次发送的日志条目</span></span><br><span class="line">	LogTerm:    hintTerm, <span class="comment">//RejectHint对应的日志Term</span></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<hr>
<h3 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h3><h4 id="发起并处理提案"><a href="#发起并处理提案" class="headerlink" title="发起并处理提案"></a>发起并处理提案</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Propose(ctx context.Context, data []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> stepWait(ctx context.Context, m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> stepWithWaitOption(ctx context.Context, m pb.Message, wait <span class="type">bool</span>) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>外部模块通过<code>node.Propose</code>方法发起提案。该方法根据<code>data</code>，组装<code>pb.MsgProp</code>消息，调用<code>n.stepWait</code>方法发出去。这里组装的<code>pb.MsgProp</code>消息中只包含了<code>Type</code>和<code>Entries</code>字段。</p>
<p><code>stepWait</code>方法，以<code>wait</code>为<code>true</code>，通过<code>stepWithWaitOption</code>实现，所以这种消息是需要等待处理结果的。</p>
<p><code>stepWithWaitOption</code>方法中，针对<code>pb.MsgProp</code>消息，首先将<code>pb.MsgProp</code>消息和一个接收应答的通道<code>pm.result</code>，封装成一个<code>msgWithResult</code>结构，将其发给<code>node.propc</code>通道，然后等待<code>pm.result</code>上的回复；</p>
<hr>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br></pre></td></tr></table></figure>

<p>在<code>node.run</code>方法中，接收到<code>node.propc</code>通道上的消息后，从中解析出<code>pb.MsgProp</code>消息以及发送应答的通道<code>pm.result</code>。设置<code>pb.MsgProp</code>中的<code>From</code>字段为自己的<code>id</code>，然后调用<code>raft.Step</code>方法处理<code>pb.MsgPro</code>p消息，然后将返回的<code>err</code>发送到<code>pm.result</code>中，最后关闭<code>pm.result</code>。</p>
<hr>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepFollower</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>在<code>raft.Step</code>方法中，针对<code>pb.MsgProp</code>消息，都是通过最后<code>raft.step</code>回调函数处理的。</p>
<p>如果是<code>Follower</code>，则<code>raft.step</code>就是<code>stepFollower</code>，该函数中针对<code>pb.MsgProp</code>消息的处理逻辑是：</p>
<ul>
<li>检查当前是否存在<code>Leader</code>，如果没有，则直接返回<code>ErrProposalDropped</code>错误；</li>
<li>如果配置项<code>raft.disableProposalForwarding</code>为<code>true</code>，表示不可以转发提案给<code>Leader</code>，所以返回<code>ErrProposalDropped</code>错误；</li>
<li>最后，设置消息中的<code>To</code>字段，然后调用<code>raft.send</code>函数发给<code>Leader</code>；</li>
</ul>
<p>如果是<code>Leader</code>，则<code>raft.step</code>就是<code>stepLeader</code>，该函数中针对<code>pb.MsgProp</code>消息的处理逻辑是：</p>
<ul>
<li>如果消息中的<code>Entries</code>为空，则调用<code>raft.logger.Panicf</code>，记录错误信息；</li>
<li>如果<code>raft.prs.Progress[r.id]</code>为<code>nil</code>，说明当前节点不是集群中的节点（比如该节点之前是<code>Leader</code>，现在被移除了），返回<code>ErrProposalDropped</code>错误；</li>
<li>如果<code>raft.leadTransferee</code>不是<code>None</code>，说明正在进行领导权转移，因此直接丢弃，返回<code>ErrProposalDropped</code>错误；</li>
<li>接下来轮训<code>m.Entries</code>中的每条消息，针对消息中的配置变更消息进行检查和处理，比如不能允许多个配置变更并行执行，联合共识过程中保证收到正确的配置变更消息，具体参考《成员变更》；</li>
<li>然后调用<code>raft.appendEntry</code>，将<code>m.Entries</code>追加到自己的日志中，如果该函数返回<code>false</code>，则返回<code>ErrProposalDropped</code>错误。该方法的具体流程参考下面；</li>
<li>最后，调用<code>raft.bcastAppend</code>方法，向<code>Follower</code>追加日志。返回<code>nil</code>；</li>
</ul>
<hr>
<h4 id="追加日志"><a href="#追加日志" class="headerlink" title="追加日志"></a>追加日志</h4><h5 id="Leader追加本地日志"><a href="#Leader追加本地日志" class="headerlink" title="Leader追加本地日志"></a>Leader追加本地日志</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> appendEntry(es ...pb.Entry) (accepted <span class="type">bool</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> maybeCommit() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p>该方法向本地日志中追加新的日志条目。逻辑是：</p>
<ul>
<li>首先调用<code>raft.raftLog.lastIndex</code>方法，获取本地日志（<code>raft.unstable</code>或<code>raft.storage</code>）中当前最后一个条目的<code>Index</code>，然后依次为<code>es</code>中的每个条目赋予新的<code>Term</code>和<code>Index</code>，其中<code>Term</code>就是当前<code>Leader</code>自己的<code>Term</code>，而<code>Index</code>，是<code>lastIndex+1+i</code>；</li>
<li>调用<code>raft.increaseUncommittedSize(es)</code>，检查当前未提交的日志条目的总大小是否超过了限制，如果是的话则返回<code>false</code>；关于大小限制相关的逻辑，参考《Ready的处理及其他》</li>
<li>然后调用<code>r.raftLog.append(es...)</code>，将<code>es</code>追加到本地日志（实际上就是<code>raft.unstable</code>）中，该方法返回最新的<code>lastIndex：li</code>；</li>
<li>调用<code>raft.prs.Progress[r.id].MaybeUpdate(li)</code>，更新<code>Progress</code>中自己的<code>Progress.Match</code>信息；</li>
<li>最后调用<code>raft.maybeCommit</code>方法，尝试更新本地<code>commitIndex</code>，即<code>raft.raftLog.committed</code>。该方法就是先调用<code>raft.prs.Committed</code>，得到已经成功复制到集群中过半节点的最大日志<code>Index</code>（即轮训<code>raft.prs</code>中每个节点的<code>Progress.Match</code>，找到过半节点的最大<code>Progress.Match</code>），然后调用<code>raft.raftlog.maybeCommit</code>方法，调整本地的<code>commitIndex</code>；</li>
</ul>
<hr>
<h5 id="Leader向Follower发送pb-MsgApp消息"><a href="#Leader向Follower发送pb-MsgApp消息" class="headerlink" title="Leader向Follower发送pb.MsgApp消息"></a>Leader向Follower发送<code>pb.MsgApp</code>消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> maybeSendAppend(to <span class="type">uint64</span>, sendIfEmpty <span class="type">bool</span>) <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> sendAppend(to <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> bcastAppend()</span><br></pre></td></tr></table></figure>

<p><code>raft.maybeSendAppend</code>方法由<code>Leader</code>调用，尝试向<code>Follower</code>发送日志条目或快照。日志条目对应的是<code>pb.MsgApp</code>消息，快照对应的是<code>pb.MsgSnap</code>消息（参考《日志压缩》）。</p>
<ul>
<li>参数<code>to</code>表示目标<code>Follower</code>的<code>id</code>；<code>sendIfEmpty</code>表示是否允许发送空的<code>pb.MsgApp</code>消息，这种空消息在新<code>Leader</code>上任后传递<code>commitIndex</code>时很有用；</li>
<li>该方法返回<code>true</code>表示会向<code>Follower</code>发送<code>pb.MsgSnap</code>或<code>pb.MsgApp</code>消息，<code>false</code>表示不会向<code>Follower</code>发送任何消息；</li>
</ul>
<p><code>raft.maybeSendAppend</code>方法的逻辑如下：</p>
<ul>
<li>首先从<code>r.prs.Progress[to]</code>中得到<code>Follower</code>对应的<code>Progress：pr</code>，接下来都是根据该<code>Progress</code>中的信息决定发送的消息内容；</li>
<li><code>pr.next</code>表示接下来要追加到<code>Follower</code>的日志起始索引。因此首先调用<code>r.raftLog.term(pr.Next - 1)</code>，获取前一个日志条目的<code>Term</code>，即<code>preLogTerm</code>，然后调用<code>r.raftLog.entries(pr.Next, r.maxMsgSize)</code>，获取要发送的日志条目；</li>
<li>如果获取到的日志条目为空，并且参数<code>sendIfEmpty</code>为<code>false</code>，则直接返回<code>false</code>，表示本次没有日志条目可以发送给<code>Follower</code>；</li>
<li>如果<code>r.raftLog.term(pr.Next - 1)</code>或<code>r.raftLog.entries(pr.Next, r.maxMsgSize)</code>返回的<code>err</code>不为<code>nil</code>，表示<code>pr.Next</code>已经没有对应的日志条目了，需要发送快照了，具体参考《日志压缩》；</li>
<li>如果获取<code>term</code>或者日志条目时没有错误，则将获取到的日志条目组装到<code>pb.MsgApp</code>消息中：<ul>
<li>将消息中的<code>Index</code>（即论文中的<code>prevLogIndex</code>）置为<code>pr.Next-1</code>，<code>LogTerm</code>（即论文中的<code>prevLogTerm</code>）置为<code>term</code>；</li>
<li>消息中的<code>Commit</code>，就是本地<code>commitIndex</code>，即<code>raft.raftLog.committed</code>，实际上这也是整个集群的<code>commitIndex</code>。<code>Leader</code>向<code>Follower</code>发的消息中，大多都带有<code>commitIndex</code>，以便<code>Follower</code>能够及时更新<code>commitIndex</code>；</li>
</ul>
</li>
<li>最后调用<code>send</code>发送消息。在<code>send</code>之前，如果日志条目不为空，则根据<code>pr</code>的状态更新<code>pr</code>的属性：<ul>
<li>如果<code>Progress</code>状态是<code>tracker.StateReplicate</code>，则调用<code>pr.OptimisticUpdate</code>和<code>pr.Inflights.Add</code>，将<code>pr.Next</code>设置最后一条日志条目的索引，并将该索引添加到<code>pr.Inflights</code>中；</li>
<li>如果<code>Progress</code>状态是<code>tracker.StateProbe</code>，则置<code>pr.ProbeSent</code> 为<code>true</code>；</li>
<li>其他状态，直接<code>panic</code>；</li>
</ul>
</li>
</ul>
<p><code>raft.sendAppend</code>方法就是以<code>sendIfEmpty</code>为<code>true</code>调用<code>maybeSendAppend</code>，尝试向<code>Follower</code>发送<code>pb.MsgApp</code>或<code>pb.MsgSnap</code>消息。</p>
<p><code>raft.bcastAppend</code>方法就是调用<code>ProgressTracker.Visit</code>方法，针对集群中除了自己以外的其他节点，调用<code>sendAppend</code>发送<code>AppendEntry</code>消息。</p>
<hr>
<h5 id="Follower处理pb-MsgApp消息，回复pb-MsgAppResp消息"><a href="#Follower处理pb-MsgApp消息，回复pb-MsgAppResp消息" class="headerlink" title="Follower处理pb.MsgApp消息，回复pb.MsgAppResp消息"></a>Follower处理<code>pb.MsgApp</code>消息，回复<code>pb.MsgAppResp</code>消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> handleAppendEntries(m pb.Message)</span><br></pre></td></tr></table></figure>

<p><code>Follower</code>收到<code>pb.MsgApp</code>消息后，调用<code>raft.handleAppendEntries</code>方法进行处理，该方法就是根据不同的情况发送<code>pb.MsgAppResp</code>响应消息：</p>
<ul>
<li>如果消息中的<code>Index</code>（即<code>prevLogIndex</code>）小于<code>Follower</code>本地的<code>commitIndex</code>，则直接回复<code>pb.MsgAppResp</code>成功消息，设置其中的<code>Index</code>为<code>commitIndex</code>；</li>
<li>接下来调用<code>raft.raftLog.maybeAppend</code>进行追加日志，并根据消息中的<code>Commit</code>字段尝试更新本地<code>commitIndex</code>。该方法如果返回<code>true</code>，则返回的<code>Index</code>就是<code>m.Entries</code>中最后一条日志的<code>Index</code>，所以将其作为<code>pb.MsgAppResp</code>消息中的<code>Index</code>字段，回复给<code>Leader</code>；</li>
<li>如果<code>raft.raftLog.maybeAppend</code>返回<code>false</code>，说明追加失败，需要回复<code>Reject</code>为<code>true</code>的<code>pb.MsgAppResp</code>消息给<code>Leader</code>。该消息中需要附带<code>RejectHint</code>和对应的<code>LogTerm</code>，以便<code>Leader</code>决定下次重试的日志条目。具体是：<ul>
<li>调用<code>raft.raftLog.findConflictByTerm(hintIndex, m.LogTerm)</code>，这里的<code>hintIndex</code>就是<code>min(m.Index, r.raftLog.lastIndex())</code>，这就表示要在<code>Follower</code>当前的日志中，找到<code>Term</code>小于等于<code>m.LogTerm</code>，<code>Index</code>小于等于<code>min(m.Index, lastIndex)</code>的最大<code>Index</code>，作为<code>MsgAppResp</code>消息中的<code>RejectHint</code>字段，并根据该字段得到对应的<code>Term</code>作为消息中的<code>LogTerm</code>，回复给<code>Leader</code>；</li>
<li>关于<code>findConflictByTerm</code>更详细的解释参考《raft包–log》中的解释；</li>
</ul>
</li>
</ul>
<hr>
<h5 id="Leader处理pb-MsgAppResp响应消息"><a href="#Leader处理pb-MsgAppResp响应消息" class="headerlink" title="Leader处理pb.MsgAppResp响应消息"></a>Leader处理<code>pb.MsgAppResp</code>响应消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>该方法是<code>Leader</code>在收到各种消息后的处理逻辑。当收到的是<code>pb.MsgAppResp</code>消息时，其处理逻辑是：</p>
<ul>
<li>如果回复中的<code>Reject</code>为<code>true</code>，说明是<code>Follower</code>对于<code>pb.MsgApp</code>的失败回复：<ul>
<li>在<code>m.LogTerm</code>大于<code>0</code>的情况下（<code>Follower</code>的日志为空时就会出现<code>LogTerm</code>为<code>0</code>的情况），调用<code>r.raftLog.findConflictByTerm(m.RejectHint, m.LogTerm)</code>，得到<code>nextProbeIdx</code>；如果<code>m.LogTerm</code>为<code>0</code>，则<code>nextProbeIdx</code>就是<code>m.RejectHint</code>；</li>
<li>然后调用<code>pr.MaybeDecrTo(m.Index, nextProbeIdx)</code>，重新设置对应<code>Follower</code>的<code>pr.Next</code>，即下次要发送的日志条目起始索引；如果该函数返回<code>false</code>，表示<code>m.Index</code>是过期的回复消息，不再处理；否则调用<code>r.sendAppend</code>尝试再次发送<code>pb.MsgApp</code>消息；如果<code>pr</code>当前是<code>StateReplicate</code>状态，则直接调用<code>pr.BecomeProbe</code>转为<code>StateProbe</code>状态；</li>
</ul>
</li>
<li>如果回复的<code>Reject</code>为<code>false</code>，则<code>pb.MsgApp</code>成功了：<ul>
<li>此时<code>m.Index</code>实际上就是<code>Follower</code>已经复制的日志条目的最大<code>Index</code>，所以这里先调用<code>pr.MaybeUpdate(m.Index)</code>更新<code>pr.Match</code>。该函数返回<code>false</code>表示过期消息，不再继续处理。返回成功的情况下才继续下面的处理：</li>
<li>如果<code>Progress</code>当前状态是<code>tracker.StateProbe</code>，则调用<code>pr.BecomeReplicate</code>将状态转移到<code>StateReplicate</code>，并把<code>pr.Next</code>置为<code>pr.Match+1</code>；</li>
<li>如果<code>Progress</code>当前状态是<code>tracker.StateSnapshot</code>，并且新<code>pr.Match</code>大于等于<code>pr.PendingSnapshot</code>，则先调用<code>pr.BecomeProbe</code>转入<code>StateProbe</code>状态，然后调用<code>pr.BecomeReplicate</code>转入<code>StateReplicate</code>状态。新<code>pr.Match</code>就是回复中的<code>Index</code>，而<code>pr.PendingSnapshot</code>表示发送给<code>Follower</code>的<code>snapshot</code>的最后条目的<code>Index</code>，<code>pr.Match</code>大于等于<code>pr.PendingSnapshot</code>，就表示<code>Follower</code>应用了收到的<code>snapshot</code>；</li>
<li>如果<code>Progress</code>当前状态已经是<code>StateReplicate</code>，则调用<code>Inflights.FreeLE</code>方法，调整<code>Inflights</code>内部状态，表示收到了<code>AppendEntry</code>对于<code>m.Index</code>的成功回复；</li>
<li>接下来调用<code>raft.maybeCommit</code>方法，尝试更新本地<code>commitIndex</code>，即<code>raft.raftLog.committed</code>。该方法就是先调用<code>raft.prs.Committed</code>，得到已经成功复制到集群中过半节点的最大日志<code>Index</code>（即轮训<code>raft.prs</code>中每个节点的<code>Progress.Match</code>，找到过半节点的最大<code>Progress.Match</code>），然后调用<code>raft.raftlog.maybeCommit</code>方法，调整本地的<code>commitIndex</code>；</li>
<li>如果<code>r.raftlog.maybeCommit</code>方法返回<code>true</code>，说明<code>commitIndex</code>得到了更新，因此需要及时通知给所有<code>Follower</code>更新<code>commitIndex</code>，所以，先调用<code>releasePendingReadIndexMessage</code>方法（<font color=red>参考《处理读请求》</font>），然后调用<code>raft.bcastAppend</code>，向集群所有<code>Follower</code>再次发送<code>pb.MsgApp</code>消息，主要是为了让其他<code>Follower</code>更新<code>commitIndex</code>；</li>
<li>如果<code>r.raftlog.maybeCommit</code>方法返回<code>false</code>，并且之前该<code>Follower</code>对应的<code>Progress</code>是<code>Paused</code>的话，则调用<code>raft.sendAppend</code>针对该<code>Follower</code>消息发送<code>Paused</code>期间阻塞的<code>pb.MsgApp</code>消息，这里也是为了更新该<code>Follower</code>的<code>commitIndex</code>；</li>
<li>接下来在循环中调用<code>raft.maybeSendAppend(m.From, false)</code>，直到该函数返回<code>false</code>。根据注释，之前的<code>raft.sendAppend</code>主要是为了更新<code>commitIndex</code>，而这里主要是为了尽可能多的发送包含日志条目的<code>pb.MsgApp</code>消息，因为收到了<code>pb.MsgApp</code>成功的回复后，该<code>Follower</code>对应的<code>Progress</code>状态就变成了<code>StateReplicate</code>；</li>
<li>最后处理领导权转移流程，即如果该<code>Follower</code>就是<code>raft.leadTransferee</code>，即被转移的新<code>Leader</code>的话，而且该<code>Follower</code>的日志已经和当前<code>Leader</code>一样新，则调用<code>raft.sendTimeoutNow(m.From)</code>，发送选举定时器超时消息，使其开始选举；</li>
</ul>
</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-10日志压缩</title>
    <url>/2022/01/27/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/10%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>下面是Raft博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》中关于日志压缩的内容，主要节选自“5.1 Snapshotting memory-based state machines”。</p>
<p>在正常运行期间，随着接收的客户端请求的增多，Raft的日志大小也会增长，它会占用更多的空间，需要更多的时间来replay。如果没有压缩日志的方法，最终将导致可用性问题：<font color=red>服务器要么空间不足，要么启动时间过长。</font>因此，任何实际系统都需要某种形式的日志压缩方法。</p>
<span id="more"></span>
<p>日志压缩的一般思路是，随着时间的推移，日志中的许多信息就会变得过时，从而可以丢弃。一旦日志条目被提交并应用到状态机，就不再需要用于到达当前状态的中间状态和操作，从而可以将它们压缩掉。</p>
<p>与Raft的核心算法和成员变更算法不同，<font color=red>不同的系统在日志压缩方面会有不同的需求</font>。在各种方法中，<font color=red>日志压缩的大部分责任都落在状态机上，状态机负责将状态写入磁盘并压缩状态。</font>对于基于内存的状态机而言，快照（Snapshotting ）是概念上是最简单的方法。在该方法中，整个当前系统状态会写入稳定存储上的快照文件中，然后丢弃该点之前的整个日志。</p>
<p>日志压缩的各种方法有几个共同的核心概念：</p>
<ul>
<li>首先，<font color=red>每个服务器独立压缩已提交的日志，而不是将压缩功能都集中在Leader身上。</font>这避免了Leader将快照数据传输给本身已有日志数据的Follower。这也有助于模块化：日志压缩的大部分复杂性都落在状态机中，而与Raft本身没有太多交互。</li>
<li>其次，状态机和Raft之间的基本交互涉及将已提交的日志从Raft传输给状态机。在应用日志条目之后，状态机迟早会以一种可以恢复到当前系统状态的方式将这些条目反映到磁盘上。一旦这样做了，它会告诉Raft丢弃日志中相应的条目。<font color=red>在Raft丢弃日志条目之前，Raft自身必须保存一些描述这些日志条目的信息。具体而言，Raft必须保留其丢弃的最后一个条目的Index和Term；</font>这俩信息可用于将位于状态机的压缩的状态之后的日志部分保持在正确的OFFSET上，并保证AppendEntries的一致性检查可以继续工作（该RPC需要包含其中的第一个条目之前的条目的Index和Term信息）。<font color=red>Raft还需要保留丢弃日志中的成员配置信息，以支持集群成员配置变更。</font></li>
<li>第三，一旦Raft丢弃了日志的部分条目，状态机将承担两个新的职责：如果服务器重新启动，状态机需要从磁盘加载与丢弃的日志条目对应的状态，然后才能应用Raft日志中的任何条目；此外，状态机需要生成一致的状态镜像文件，以便将其发送给慢Follower（其日志远远落后于领导者的日志）。</li>
</ul>
<p>快照方法适用于状态机的数据结构保存在内存中的场景。对于数据集为千兆字节或数十千兆字节的状态机来说，这是一个合理的选择。</p>
<p>每个服务器都独立地进行快照，快照只覆盖日志中已提交且应用的条目。快照方法的大部分工作就是将状态机的当前状态进行序列化，而这是特定于状态机来实现的。</p>
<p><font color=red>一旦状态机写完快照，就可以将日志截断。Raft首先保存重启所需的状态：快照中最后一个条目的Index和Term，以及快照中的最新成员配置，然后将Index之前的日志丢弃，之前保存的快照也可以丢弃，它们也没用了。</font></p>
<p>Leader可能偶尔需要将其状态发送给慢Follower和新加入集群的服务器。在快照方法中，这个状态就是最新的快照文件，Leader使用RPC发送该快照。<font color=red>当Follower收到此RPC中包含的快照文件时，它必须决定如何处理其现有的日志项。</font></p>
<ul>
<li>通常，快照文件中会包含Follower日志中没有的新信息，在这种情况下，Follower丢弃其整个日志，将其直接用快照取代，日志中可能有与快照冲突的未提交条目，也直接被快照覆盖。</li>
<li>相反，如果Follower接收到的快照，仅包含其日志条目的头部部分条目（由于重传或某些错误），则快照所能覆盖的那些日志条目将被删除，但快照之后的条目仍然有效，必须保留。</li>
</ul>
<p>创建快照可能需要很长时间，包括状态序列化和将其写入磁盘。因此，<font color=red>序列化和写入快照必须与正常操作同时进行，以避免出现可用性问题。</font>可以使用操作系统支持的写时拷贝技术（在编程环境允许的情况下）。例如，在Linux上，内存状态机可以使用fork来复制服务进程的整个地址空间。然后，在父进程继续为请求提供服务的同时，子进程可以将状态写入文件并退出。LogCabin目前使用的是fork，但是<font color=red>fork在涉及多线程，以及C++析构函数时需要慎重设计，要使其正确工作存在一些困难。但是，使用fork只需要少量代码，而且完全不需要修改状态机的数据结构，因此我们认为使用fork是正确的方法。</font></p>
<p>服务器必须决定何时快照。如果服务器快照太频繁，则会浪费磁盘带宽和其他资源；如果快照频率太低，则可能会耗尽其存储容量，并会增加重启时重放（replay）日志所需的时间。使用上一个快照的大小而不是下一个快照的大小作为参照，可能会更加合理。一旦日志的大小超过前一个快照的大小乘以可配置的扩展因子，服务器就可以进行快照。</p>
<hr>
<h3 id="节点自行创建快照"><a href="#节点自行创建快照" class="headerlink" title="节点自行创建快照"></a>节点自行创建快照</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> CreateSnapshot(i <span class="type">uint64</span>, cs *pb.ConfState, data []<span class="type">byte</span>) (pb.Snapshot, <span class="type">error</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> Compact(compactIndex <span class="type">uint64</span>) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p>创建快照的流程如下，主要涉及MemoryStorage中的方法，以下内容与《raft包–storage》有重复：</p>
<ul>
<li>外部模块在满足一定的条件后，会调用<code>MemoryStorage.CreateSnapshot</code>方法主动创建<code>MemoryStorage.snapshot</code>；<ul>
<li>该方法的参数<code>i</code>表示新<code>snapshot</code>的<code>Metadata.Index</code>；参数<code>data</code>表示新<code>snapshot</code>的二进制数据（也就是说新<code>snapshot</code>的数据来源于外部模块，实际上就是当前系统状态的快照）；参数<code>cs</code>表示当前的集群成员配置；</li>
<li>该方法会检查<code>i</code>是否在 <code>(MemoryStorage.snapshot.Metadata.Index, MemoryStorage.lastIndex() ]</code>范围内，然后才设置新的<code>snapshot</code>。即保证新快照文件必须能覆盖旧快照文件。</li>
<li>该方法并没有对<code>MemoryStorage.ents</code>做任何改动。</li>
</ul>
</li>
<li>然后外部模块将<code>snapshot</code>保存到<code>NVS</code>中；</li>
<li>然后外部模块会调用<code>MemoryStorage.Compact</code>方法来摒弃一些<code>MemoryStorage.ents</code>中的日志条目，即<code>compactlndex</code>之前的日志。<ul>
<li>需要注意的是，调用该方法的参数<code>compactlndex</code>并不一定等于调用<code>CreateSnapshot</code>时传递的<code>Metadata.Index</code>，其值有可能比当前的新<code>snapshot.Metadata.Index</code>要小，即保留一些已经压缩到<code>snapshot</code>中的日志条目。根据注释，这里是为了照顾一些比较慢的Follower，以便不发送非必要的<code>snapshot</code>，节省带宽。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Leader向Follower发送快照"><a href="#Leader向Follower发送快照" class="headerlink" title="Leader向Follower发送快照"></a>Leader向Follower发送快照</h3><h4 id="消息格式"><a href="#消息格式" class="headerlink" title="消息格式"></a>消息格式</h4><p><code>Leader</code>通过<code>pb.MsgSnap</code>消息向<code>Follower</code>发送快照，该消息的格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	Type: pb.MsgSnap, </span><br><span class="line">	Snapshot: snapshot, <span class="comment">//raftLog.snapshot()得到的快照</span></span><br><span class="line">	From: id, <span class="comment">//Leader自己的id</span></span><br><span class="line">	To: id, <span class="comment">//Follower的id</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>Follower</code>收到<code>pb.MsgSnap</code>消息后，不管是否应用了该快照，都会回复<code>pb.MsgAppResp</code>消息，该消息的格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message &#123;</span><br><span class="line">	To: m.From, <span class="comment">//发送AppendEntry消息的Leader的id</span></span><br><span class="line">	Type: pb.MsgAppResp, </span><br><span class="line">	Index: lastIndex or commitIndex, <span class="comment">//应用了快照则是raftLog.lastIndex()，否则是raftLog.committed</span></span><br><span class="line">	From: id, <span class="comment">//Follower自己的id</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Leader发送pb-MsgSnap消息"><a href="#Leader发送pb-MsgSnap消息" class="headerlink" title="Leader发送pb.MsgSnap消息"></a>Leader发送<code>pb.MsgSnap</code>消息</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> maybeSendAppend(to <span class="type">uint64</span>, sendIfEmpty <span class="type">bool</span>) <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pr *Progress)</span></span> BecomeSnapshot(snapshoti <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> snapshot() (pb.Snapshot, <span class="type">error</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> Snapshot() (pb.Snapshot, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p><code>Leader</code>在调用<code>raft.maybeSendAppend</code>尝试向某个<code>Follower</code>发送日志条目时，会先调用<code>raftLog.term(pr.Next - 1)</code>尝试得到<code>preLogTerm</code>，然后调用<code>raftLog.entries(pr.Next, r.maxMsgSize)</code>尝试得到要发送的日志条目。如果这两个调用返回了非空的错误，就认为向该<code>Follower</code>发送的条目已经不存在于日志中，因此需要向该<code>Follower</code>发送<code>pb.MsgSnap</code>消息。</p>
<p>发送<code>pb.MsgSnap</code>消息时，需要先调用<code>raft.snapshot</code>方法获取快照。该方法首先看当前是否有<code>raftlog.unstable.snapshot</code>（即该<code>Leader</code>还是<code>Follower</code>时从前任<code>Leader</code>处收到的，并且尚未保存到<code>NVS</code>中的快照），没有的话在调用<code>MemoryStorage.Snapshot</code>得到<code>NVS</code>中的快照（这种的可能性应该更大一些）；</p>
<p>剩下的就是组装<code>pb.MsgSnap</code>消息并发送了，发消息之前会调用<code>Progress.BecomeSnapshot</code>将状态置为<code>StateSnapshot</code>，并把<code>Progress.PendingSnapshot</code>置为快照中的<code>Metadata.Index</code>，后续收到<code>Follower</code>的回复时会用到该字段。</p>
<hr>
<h4 id="Follower处理pb-MsgSnap消息"><a href="#Follower处理pb-MsgSnap消息" class="headerlink" title="Follower处理pb.MsgSnap消息"></a>Follower处理<code>pb.MsgSnap</code>消息</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> handleSnapshot(m pb.Message)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> restore(s pb.Snapshot) <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *raftLog)</span></span> restore(s pb.Snapshot)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> restore(s pb.Snapshot)</span><br></pre></td></tr></table></figure>

<p><code>Follower</code>收到<code>pb.MsgSnap</code>消息后，调用<code>handleSnapshot</code>方法处理，该方法就是调用<code>r.restore(m.Snapshot)</code>尝试应用收到的快照，该函数返回<code>true</code>表示应用了快照，否则返回<code>false</code>。不管<code>restore</code>返回<code>true</code>或<code>false</code>，都会回复<code>reject</code>为<code>false</code>的<code>pb.MsgAppResp</code>消息，区别在于：</p>
<ul>
<li><code>restore</code>返回<code>true</code>时，<code>pb.MsgAppResp</code>消息中的<code>Index</code>被置为最新的<code>r.raftLog.lastIndex()</code>，即快照中最后日志条目的<code>Index</code>；</li>
<li><code>restore</code>返回<code>false</code>时，<code>pb.MsgAppResp</code>消息中的<code>Index</code>被置为<code>r.raftLog.committed</code>，即<code>Follower</code>当前的<code>commitIndex</code>。</li>
</ul>
<p><code>restrore</code>尝试应用快照恢复日志和成员配置。如果该方法返回<code>false</code>，说明没有应用快照，一般而言这表示快照是过期的，或者是发生了某种错误。该方法的逻辑如下：</p>
<ul>
<li><font color=red>如果快照中的最后日志条目的<code>Index</code>小于等于当前的<code>r.raftLog.committed</code>，说明快照是过期快照</font>，直接返回<code>false</code>；</li>
<li>如果当前节点是<code>Leader</code>，则以<code>r.Term+1</code>调用<code>r.becomeFollower</code>，转为<code>Follower</code>，然后返回<code>false</code>。按照注释，这是一种<code>defense-in-depth</code>，正常来讲<code>Leader</code>不可能走到这一步，调用<code>restrore</code>时节点肯定是<code>Follower</code>状态。但是万一走到了，这里为了避免对集群造成破坏，直接转成<code>Follower</code>后返回；</li>
<li>轮训快照中<code>Metadata.ConfState</code>中的<code>Voters, Learners</code>和<code>VotersOutgoing</code>这些id数组，这些<code>id</code>数组中包含了集群当前所有的节点<code>id</code>，如果在其中找不到当前<code>Follower</code>的<code>id</code>，则返回<code>false</code>；这还是一种<code>defense-in-depth</code>；</li>
<li>调用<code>r.raftLog.matchTerm(s.Metadata.Index, s.Metadata.Term)</code>，该方法返回<code>true</code>，<font color=red>表示<code>Follower</code>当前的日志条目已完全包含了快照中的日志，无需应用快照</font>，所以更新<code>raftLog.committed</code>为<code>s.Metadata.Index</code>后，返回<code>false</code>；</li>
<li>调用<code>r.raftLog.restore</code>应用快照。该方法会将<code>raftLog.committed</code>设置为<code>s.Metadata.Index</code>，并且将<code>raftLog.unstable.snapshot</code>设置为<code>s</code>，并将<code>raftLog.unstable.entries</code>置为<code>nil</code>。<font color=red>即把<code>snapshot</code>保存到<code>unstable</code>中，并清空其中的日志</font>；</li>
<li>然后创建新的<code>r.prs</code>，即<code>ProgressTracker</code>，调用<code>confchange.Restore</code>，根据快照中的<code>Metadata.ConfState</code>设置当前集群的成员配置。<font color=red>具体可参考《配置变更》</font></li>
<li>最后，调用<code>pr.MaybeUpdate(pr.Next-1)</code>，更新<code>Follower</code>自己的<code>Progress.Match</code>；最后返回<code>true</code>；</li>
</ul>
<hr>
<h4 id="Leader处理pb-MsgAppResp响应消息"><a href="#Leader处理pb-MsgAppResp响应消息" class="headerlink" title="Leader处理pb.MsgAppResp响应消息"></a>Leader处理<code>pb.MsgAppResp</code>响应消息</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func stepLeader(r *raft, m pb.Message) error</span><br></pre></td></tr></table></figure>

<p><code>Leader</code>收到<code>Follower</code>对于<code>pb.MsgSnap</code>消息回复的<code>reject</code>为<code>false</code>的<code>pb.MsgAppResp</code>响应消息后，在<code>stepLeader</code>中的处理逻辑是：</p>
<ul>
<li>先调用<code>pr.MaybeUpdate(m.Index)</code>更新<code>pr.Match</code>。该函数返回<code>false</code>表示过期消息，不再继续处理。返回成功的情况下才继续下面的处理：</li>
<li><code>Follower</code>对应的<code>Progress</code>当前状态肯定是<code>tracker.StateSnapshot</code>，如果新<code>pr.Match</code>大于等于<code>pr.PendingSnapshot</code>，则先调用<code>pr.BecomeProbe</code>转入<code>StateProbe</code>状态，然后调用<code>pr.BecomeReplicate</code>转入<code>StateReplicate</code>状态。</li>
<li>新<code>pr.Match</code>就是回复消息中的<code>m.Index</code>，而<code>pr.PendingSnapshot</code>表示发送给<code>Follower</code>的<code>snapshot</code>的最后条目的<code>Index</code>，如果<code>pr.Match</code>大于<code>pr.PendingSnapshot</code>，说明<code>Follower</code>收到的快照无法完全覆盖其本地日志条目，从而没有应用快照，所以<code>Follower</code>返回的是其<code>commitIndex</code>；如果<code>pr.Match</code>等于<code>pr.PendingSnapshot</code>，说明<code>Follower</code>应用了快照。不管哪种情况，都可以将<code>Follower</code>对应的<code>Progress</code>转为<code>StateReplicate</code>状态，接下来就可以发送日志条目了；</li>
</ul>
<hr>
<h4 id="Follower将快照保存到NVS"><a href="#Follower将快照保存到NVS" class="headerlink" title="Follower将快照保存到NVS"></a><code>Follower</code>将快照保存到<code>NVS</code></h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> readyWithoutAccept() Ready</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newReady</span><span class="params">(r *raft, prevSoftSt *SoftState, prevHardSt pb.HardState)</span></span> Ready</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">IsEmptySnap</span><span class="params">(sp pb.Snapshot)</span></span> <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ms *MemoryStorage)</span></span> ApplySnapshot(snap pb.Snapshot) <span class="type">error</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Advance(rd Ready)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> advance(rd Ready)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *unstable)</span></span> stableSnapTo(i <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>当<code>raft模块</code>需要将<code>Ready</code>结构发送给外部模块处理时，首先会调用<code>RawNode.readyWithoutAccept -&gt; newReady</code>获取最新的<code>Ready</code>结构。<code>newReady</code>函数会在<code>raftLog.unstable.snapshot</code>不为<code>nil</code>的情况下将其保存到<code>Ready.Snapshot</code>中。</p>
<p>外部模块收到<code>Ready</code>结构后，调用<code>IsEmptySnap</code>判断其中的<code>Ready.Snapshot</code>不为空的情况下，会首先将<code>Snapshot</code>保存到<code>NVS</code>中，然后调用<code>MemoryStorage.ApplySnapshot</code>方法，替换掉现有的<code>MemoryStorage.snapshot</code>，并将其中所有日志条目<code>MemoryStorage.ents</code>清空，只保留<code>ents[0]</code>的信息，设置<code>ents[0]</code>的<code>Index</code>和<code>Term</code>分别为最新的<code>snapshot.Metadata.Index</code>和<code>snapshot.Metadata.Term</code>。该方法在替换<code>MemoryStorage.snapshot</code>之前，需<font color=red>要保证新的<code>snapshot.Metadata.Index</code>必须大于旧的<code>snapshot.Metadata.Index</code></font>，否则就表示新<code>snapshot</code>不能完全覆盖旧<code>snapshot</code>，这种情况下直接报错；</p>
<p>外部模块处理完<code>Ready</code>结构后，调用<code>raft.advance</code>方法。如果<code>Ready.Snapshot</code>不为空，则说明其中的<code>snapshot</code>已经保存到<code>NVS</code>中了，则以其为参数调用<code>raftlog.stableSnapTo</code>方法，最终调用到<code>unstable.stableSnapTo</code>方法 ，将<code>unstable.snapshot</code>置为<code>nil</code>。</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-12CheckQuorum和心跳消息处理</title>
    <url>/2022/03/15/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/12CheckQuorum%E5%92%8C%E5%BF%83%E8%B7%B3%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><code>Leader</code>通过发送心跳消息到所有的<code>Follower</code>，来维护自己的权威。如果因网络分区导致某个<code>Follower</code>长时间收不到<code>Leader</code>的消息，该<code>Follower</code>会自增本地Term，发起选举流程。这种带有更大Term的选举消息会导致当前正常的<code>Leader</code>被迫转为<code>Follower</code>，从而导致集群暂时的不可用。RAFT的实现中，引入了<code>CheckQuorum</code>机制，即如果<code>Follower</code>在选举超时时间的最小值内收到过<code>Leader</code>的心跳消息，则<code>Follower</code>就认为该<code>Leader</code>是活跃的，从而会拒绝其他节点发来的<code>PreVote</code>或<code>Vote</code>消息，避免了这种无意义的选举。</p>
<span id="more"></span>
<p>同样的，如果<code>Leader</code>在选举超时时间内，向所有<code>Follower</code>发送的心跳消息，没有收到集群中Majority节点的回复，这说明该<code>Leader</code>已经被分区隔离了，所以该<code>Leader</code>需要降级为<code>Follower</code>，避免对正常集群<code>Leader</code>的干扰；如果能收到Majority节点的响应，说明该<code>Leader</code>的权威是得到认可的，从而可以方便的处理读请求。</p>
<p>RAFT大论文中有关上述机制的描述主要来自于：<code>3.4: Leader election</code>、<code>4.2.3 Disruptive servers</code>以及<code>6.2 Routing requests to the leader</code>：</p>
<blockquote>
<p>Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader.</p>
</blockquote>
<blockquote>
<p>Without additional mechanism, servers not in Cnew can disrupt the cluster. Once the cluster leader has created the Cnew entry, a server that is not in Cnew will no longer receive heartbeats, so it will time out and start new elections. Furthermore, it will not receive the Cnew entry or learn of that entry’s commitment, so it will not know that it has been removed from the cluster. The server will send RequestVote RPCs with new term numbers, and this will cause the current leader to revert to follower state. A new leader from Cnew will eventually be elected, but the disruptive server will time out again and the process will repeat, resulting in poor availability. If multiple servers have been removed from the cluster, the situation could degrade further.</p>
<p>Raft’s solution uses heartbeats to determine when a valid leader exists. In Raft, a leader is considered active if it is able to maintain heartbeats to its followers (otherwise, another server will start an election). Thus, servers should not be able to disrupt a leader whose cluster is receiving heartbeats. We modify the RequestVote RPC to achieve this: if a server receives a RequestVote request within the minimum election timeout of hearing from a current leader, it does not update its term or grant its vote. It can either drop the request, reply with a vote denial, or delay the request; the result is essentially the same. This does not affect normal elections, where each server waits at least a minimum election timeout before starting an election. However, it helps avoid disruptions from servers not in Cnew: while a leader is able to get heartbeats to its cluster, it will not be deposed by larger term numbers.</p>
</blockquote>
<blockquote>
<p>A server might be in the leader state, but if it isn’t the current leader, it could be needlessly delaying client requests. For example, suppose a leader is partitioned from the rest of the cluster, but it can still communicate with a particular client. Without additional mechanism, it could delay a request from that client forever, being unable to replicate a log entry to any other servers. Meanwhile, there might be another leader of a newer term that is able to communicate with a majority of the cluster and would be able to commit the client’s request. Thus, a leader in Raft steps down if an election timeout elapses without a successful round of heartbeats to a majority of its cluster; this allows clients to retry their requests with another server.</p>
</blockquote>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="心跳消息的发送和处理"><a href="#心跳消息的发送和处理" class="headerlink" title="心跳消息的发送和处理"></a>心跳消息的发送和处理</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Leader发送pb.MsgHeartbeat消息</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> tickHeartbeat()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> bcastHeartbeat()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> bcastHeartbeatWithCtx(ctx []<span class="type">byte</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> sendHeartbeat(to <span class="type">uint64</span>, ctx []<span class="type">byte</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//Follower收到pb.MsgHeartbeat消息，回复pb.MsgHeartbeatResp消息</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepFollower</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> handleHeartbeat(m pb.Message)</span><br><span class="line"></span><br><span class="line"><span class="comment">//Leader处理pb.MsgHeartbeatResp回复消息</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p><code>Leader</code>发送<code>pb.MsgHeartbeat</code>消息</p>
<ul>
<li>节点变成<code>Leader</code>后，在<code>raft.becomeLeader</code>方法中，将<code>raft.tick</code>回调函数设置为<code>tickHeartbeat</code>，因此每一个定时器tick就会调用一次<code>tickHeartbeat</code>回调函数。该函数中，增加<code>raft.heartbeatElapsed</code>的值，当该值大于等于<code>raft.heartbeatTimeout</code>时，就以<code>pb.MsgBeat</code>调用<code>raft.Step</code>方法，针对<code>pb.MsgBeat</code>消息，在<code>raft.Step</code>中是调用<code>stepLeader</code>处理的；</li>
<li><code>stepLeader</code>函数中，收到<code>pb.MsgBeat</code>消息，调用<code>raft.bcastHeartbeat</code>方法，向集群中所有Follower（包括Learner）发送<code>pb.MsgHeartbeat</code>消息。<code>raft.bcastHeartbeat</code>方法中，根据当前是否有读请求（<font color=red>读请求发送心跳时需要附带ETCD发来的请求数据，具体参考《处理读请求》</font>），使用不同的参数调用<code>raft.bcastHeartbeatWithCtx</code>方法;</li>
<li><code>raft.bcastHeartbeatWithCtx</code>方法针对<code>ProgressTracker.Progress</code>中所有的节点，调用<code>raft.sendHeartbeat</code>，发送<code>pb.MsgHeartbeat</code>消息；</li>
</ul>
<p><code>Follower</code>收到<code>pb.MsgHeartbeat</code>消息：</p>
<ul>
<li><code>Follower</code>收到<code>pb.MsgHeartbeat</code>消息后，在<code>stepFollower</code>回调函数中进行处理；</li>
<li>该函数中就是调用<code>raft.handleHeartbeat</code>，更新自己本地的<code>CommitIndex</code>，并回复<code>pb.MsgHeartbeatResp</code>消息；</li>
</ul>
<p><code>Leader</code>收到<code>pb.MsgHeartbeatResp</code>消息：</p>
<ul>
<li><code>Leader</code>收到<code>pb.MsgHeartbeatResp</code>消息后，在<code>stepLeader</code>回调函数中进行处理；</li>
<li>该函数中：<ul>
<li>将该<code>Follower</code>对应的<code>Progress.RecentActive</code>置为<code>true</code>；将<code>Progress.ProbeSent</code>置为<code>false</code>；</li>
<li>调用<code>raft.sendAppend</code>尝试向<code>Follower</code>发送<code>pb.MsgApp</code>消息；</li>
<li>剩下的逻辑，针对的是因收到读请求而发送的心跳消息，收到心跳响应后的处理；<font color=red>具体参考《处理读请求》</font></li>
</ul>
</li>
</ul>
<hr>
<h4 id="CheckQuorum流程"><a href="#CheckQuorum流程" class="headerlink" title="CheckQuorum流程"></a>CheckQuorum流程</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> tickHeartbeat()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *ProgressTracker)</span></span> QuorumActive() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p><code>Leader</code>每隔一段时间（确切的说就是每隔<code>electionTimeout</code>个tick），就会检查这段时间内自己是否能收到Majority个<code>Follower</code>的正常响应。如果能，就说明自己还是这个集群的<code>Leader</code>；否则的话，说明自己要么被分区隔离了，要么就是自己在新配置中被删除或者降级了，这种情况下，该<code>Leader</code>要主动变成<code>Follower</code>。</p>
<p>节点变成<code>Leader</code>状态后，在<code>raft.becomeLeader</code>方法中将<code>raft.tick</code>回调函数设置为<code>tickHeartbeat</code>，因此每一个定时器tick就会调用一次<code>tickHeartbeat</code>回调函数。该函数中：</p>
<ul>
<li>增加<code>raft.electionElapsed</code>的值（对于<code>Follower</code>而言，<code>raft.electionElapsed</code>是决定是否开始进行选举的时间计数器；而对于<code>Leader</code>而言，它是决定是否开始<code>CheckQuorum</code>的计时器），当该值大于等于<code>raft.electionTimeout</code>后，并且<code>raft.checkQuorum</code>配置为<code>true</code>时，以<code>pb.MsgCheckQuorum</code>消息调用<code>raft.Step</code>方法。在<code>raft.Step</code>方法中，针对<code>pb.MsgCheckQuorum</code>消息，是在<code>stepLeader</code>回调函数中处理的。该函数中对<code>pb.MsgCheckQuorum</code>消息的处理逻辑是：<ul>
<li>首先将本节点对应的<code>Progress.RecentActive</code>置为<code>true</code>；</li>
<li>然后调用<code>tracker.ProgressTracker.QuorumActive</code>检查是否有过半的<code>Follower</code>（只有<code>Voter</code>，不包括<code>Learner</code>）都是active的，即该<code>Follower</code>对应的<code>Progress.RecentActive</code>是true。当<code>Leader</code>能收到<code>Follower</code>发来的<code>pb.MsgAppResp</code>或<code>pb.MsgHeartbeatResp</code>响应消息时，就会把该<code>Follower</code>对应的<code>Progress.RecentActive</code>置为<code>true</code>。</li>
<li>如果<code>tracker.ProgressTracker.QuorumActive</code>返回<code>false</code>，则调用<code>becomeFollower</code>转为<code>Follower</code>；</li>
<li>接下来，将所有节点的<code>Progress.RecentActive</code>置为<code>false</code>，为下次的<code>CheckQuorum</code>做准备；</li>
</ul>
</li>
<li>在<code>tickHeartbeat</code>中，接下来，如果当前<code>Leader</code>在转移领导权，则说明这个过程已经持续时间已经超过了<code>electionTimeout</code>，所以调用<code>abortLeaderTranser</code>终止这个过程。</li>
<li>接下来，在<code>tickHeartbeat</code>中，如果当前节点的状态已经不是<code>Leader</code>了，直接返回。这个逻辑说明之前<code>CheckQuorum</code>中，该<code>Leader</code>检查失败，自己已经转为<code>Follower</code>了；</li>
<li>接下来就是发心跳消息；</li>
</ul>
<hr>
<h4 id="raft-checkQuorum配置选项"><a href="#raft-checkQuorum配置选项" class="headerlink" title="raft.checkQuorum配置选项"></a>raft.checkQuorum配置选项</h4><p>在<code>raft.checkQuorum</code>配置为<code>true</code>的情况下，<code>Follower</code>如果收到了其他节点发来的Vote或者PreVote消息，即使消息中的Term大于本地Term，但是只要当前其选举超时<code>tick：electionElapsed</code>小于<code>electionTimeout</code>，说明本节点与<code>Leader</code>的通信还在Lease之内，所以不会给其他节点投票。该功能是在20160527提交的<code>337ef64e：raft: implemented leader lease when quorum check is on</code>支持的。</p>
<p>在<code>raft.checkQuorum</code>为<code>true</code>的情况下，在<code>Step</code>函数中，如果收到了Term值较小的<code>pb.MsgHeartbeat</code> 或 <code>pb.MsgApp</code>消息（这肯定是<code>Leader</code>发来的），即条件<code>(r.checkQuorum ) &amp;&amp; (m.Type == pb.MsgHeartbeat || m.Type == pb.MsgApp)</code>为<code>true</code>，则当前的处理是，则要回复一个带有更高本地Term的<code>pb.MsgAppResp</code>消息。这么做的原因是：</p>
<ul>
<li>收到<code>Leader</code>发来的带有更小Term的消息，这有可能网络中延迟的消息，但是也有可能是本节点因网络隔离而自增了本地的Term（<code>checkQuorum</code>的实现比<code>preVote</code>要早），隔离阶段本节点会发起选举流程，但是肯定得不到选票，从而不断增加自己的Term。现在网络恢复了，本节点如果还发起选举流程，因为<code>checkQuorum</code>为<code>true</code>，所以其他节点收到本节点的Vote消息后直接忽略；如果本节点收到<code>Leader</code>发来的消息后直接忽略（实现<code>checkQuorum</code>之前，收到较小Term消息时就是直接忽略），这就导致本节点无法重新回到集群中。</li>
<li>所以，这种情况下，即<code>checkQuorum</code>为<code>true</code>，又收到了<code>Leader</code>发来的<code>pb.MsgHeartbeat</code> 或 <code>pb.MsgApp</code>消息，则直接回复一个更高Term的<code>pb.MsgAppResp</code>消息，这当然会导致<code>Leader</code>降级为<code>Follower</code>，但是这也是本节点回到集群中的唯一方法。在引入了PreVote流程之后，隔离的节点发起选举就不会自增本地Term了，所以这种导致<code>Leader</code>降级的场景在新版本中不会出现了；</li>
<li>这么做也不会使配置变更中被删除的节点触发<code>Leader</code>的重新选举，被删除的节点根本不会收到<code>Leader</code>发来的消息，所以也就不会发送更高Term的<code>pb.MsgAppResp</code>消息导致<code>Leader</code>重新选举。</li>
<li>上面的逻辑，是在20160527提交的<code>337ef64e：raft: implemented leader lease when quorum check is on</code>支持的。而在20170909提交的<code>8597361f：raft: fix Pre-Vote migration</code>中，增加了<code>preVote</code>的条件，即变成了<code>(r.checkQuorum || r.preVote) &amp;&amp; (m.Type == pb.MsgHeartbeat || m.Type == pb.MsgApp)</code>，这与解决<code>PreVote</code>导致死锁问题一同合入的代码（具体参考《选举和状态转换》），场景还是发生在<code>preVote</code>版本迁移的过程中：<ul>
<li>如果某节点之前不支持<code>preVote</code>，所以该节点被隔离时会不断的自增自己的Term；</li>
<li>该节点重启并升级为支持<code>PreVote</code>后，它的Term很大，但它的日志却很旧，所以其他节点收到它的<code>preVoteMsg</code>时也不会给他投票（这就类似于<code>checkQuorum</code>中也不会给他投票），所以如果不特殊处理，该节点可能也回不到集群中（如果<code>Leader</code>一切正常的情况下），所以这里也是让该节点收到<code>Leader</code>的消息后，回复一个带有Term的<code>pb.MsgAppResp</code>消息，从而让本节点重新回到集群。</li>
</ul>
</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-13处理读请求</title>
    <url>/2022/04/03/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/13%E5%A4%84%E7%90%86%E8%AF%BB%E8%AF%B7%E6%B1%82/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>对读请求的处理，可以像处理写请求那样，通过日志机制，在Majority节点上达成共识后返回给CLIENT结果。但是读请求并不修改状态，很自然的就能想到只读请求是否可以绕过RAFT的日志机制，这样性能上就能有所提升。</p>
<p>如果没有额外的预防措施，<font color=red>绕过日志可能会导致读请求得到过时的结果。</font>比如，收到只读请求的Leader可能已经被分区隔离了，如果被隔离的Leader没有与集群中的其他节点沟通的情况下直接回复只读请求，CLIENT就可能得到一个过时的结果，这就不符合<font color=red>线性一致性（Linearizability ）</font>的约束了。</p>
<span id="more"></span>
<p>RAFT博士论文中提出了两种处理读请求的方法，这两种方法的宗旨都是：<font color=red>保证读请求不会返回过期数据的基础上</font>，尽可能的提升性能。</p>
<h4 id="ReadIndex方法"><a href="#ReadIndex方法" class="headerlink" title="ReadIndex方法"></a>ReadIndex方法</h4><p>收到读请求后，Leader的动作如下：</p>
<ol>
<li>Leader需要知道当前最大的committed Index。Leader刚刚当选时，虽然它具有集群中最新的日志，但是它不知道当前最大的committed Index是多少。所以，刚当选的Leader需要以其当前Term提交一个空日志条目，等到该条目提交后，就可以保证Leader的committed Index是当前集群所有节点中最大的，该Leader才能处理读请求。</li>
<li>Leader将其当前的committed Index保存为节点内部的变量ReadIndex，该ReadIndex会作为读请求返回的系统状态的版本的<font color=red>下限</font>。即读请求返回的系统状态<font color=red>至少</font>能反映出应用ReadIndex日志修改后系统状态。</li>
<li>Leader需要确保自己没有被分区隔离。所以它会发起新一轮的心跳消息，等到收到集群中Majority节点的响应消息后，Leader就知道，<font color=red>在它发出心跳消息的那一刻，就不可能存在一个具有更大Term的新Leader（反证：如果发出心跳时，集群中存在一个更大Term的Leader，那肯定不会收到Majority节点的心跳响应），</font>所以就能保证，ReadIndex确实是发出心跳消息之前（或者可以说，是收到只读请求时），集群中所有节点最大的committed Index。<ul>
<li>问：某个Follower先收到了其他Candidate的Vote消息并回复成功了，然后收到了该Leader的心跳消息，怎么办？答：先收到Vote消息并回复成功，说明该Follower的Term增加了，收到心跳消息后就直接忽略了；</li>
</ul>
</li>
<li>Leader等到状态机运行到ReadIndex后，就能保证线性一致性了。状态机应用到 ReadIndex之后的状态都能使这个读请求满足线性一致，不管过了多久，也不管 Leader 有没有飘走。因为线性一致性的要求就是读请求返回最近一次写操作的结果，如果返回更加新的结果当然也是没问题的。</li>
<li>此时，Leader向状态机发起读请求，并将结果返回给CLIENT。</li>
</ol>
<p>ReadIndex 跳过了日志机制，节省了磁盘开销，它能大幅提升读的吞吐，减小延时（但不显著）。</p>
<hr>
<h4 id="LeaseRead方法"><a href="#LeaseRead方法" class="headerlink" title="LeaseRead方法"></a>LeaseRead方法</h4><p>上面的方法在异步模型中保证了线性一致性，所谓异步模型，就是时钟，CPU，消息的传递都可以以任意的速度进行。这种级别的安全性就需要通信来保证：每一批读请求都需要发送一轮到过半节点的心跳消息，这就增加了处理请求的延迟。LeaseRead方法依赖时钟精度，而非消息，来保证线性一致性。这种方法采用的是租约机制。</p>
<p>Leader在发送常规的心跳消息时记一个时间点start，<font color=red>一旦心跳得到集群中Majority节点的回应，Leader就可以将租约延长到 start + election_timeout &#x2F; clock_drift_bound(时钟漂移范围)。因为Follower在租约超期之前，肯定不会因选举超时而发起新的选举。</font>Leader在租约内可以无需通信，直接处理读请求。</p>
<p>针对这种机制的解释如下：</p>
<ul>
<li>先抛开节点间的clock drift不谈，start是发送心跳消息的起始时间，Follower收到心跳消息的时间点记为<code>t_recvHeart</code>，那显然<code>start &lt; t_recvHeart</code>，收到Majority节点的心跳响应，就说明Follower至少在<code>t_recvHeart + election_timeout</code>内不会选举超时，也不会投票给其他节点，因为<code>start + election_timeout</code>肯定小于<code>t_recvHeart + election_timeout</code>，所以Leader将租约定为<code>start + election_timeout</code>肯定是安全的。</li>
<li>考虑clock drift的话，由于不同节点的<code>CPU</code>时钟可能有不同程度的漂移（Computer clocks are subject to clock drift , they count time at different rates），这会导致在一个很小的时间窗口内，即使Leader认为其持有租约，但集群已经选举出了新的Leader。因此，一些系统在实现Lease Read时缩小了Leader持有租约的时间，选择了一个略小于 <code>election_timeout</code> 的时间，以减小时钟漂移带来的影响。</li>
</ul>
<p>租约机制需<font color=red>要保证节点之间的时钟差异有一个界限</font>（各个服务器之间 clock 走的频率不一样，有些太快，有些太慢，但这个差异总有一个界限。如果没有界限，这套 lease 机制就可能出问题），发现和维护这种时钟漂移界限可能会有额外的工作，但是如果这个假设被打破的话，则Leader就可能会回复CLIENT一个过期数据。</p>
<hr>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p><code>etcd/raft</code>中实现了ReadIndex方法和LeaseRead方法，不过实际上对于<code>etcd/raft</code>而言，他要做的仅仅就是在收到读请求时，把那一时刻集群中最大的<code>committIndex</code>返回给使用<code>Raft</code>的应用程序，即<code>etcd</code>，这就保证读操作符合线性一致性。</p>
<p>线性一致性，就是要求读操作至少要返回，之前已经完成的写操作所能反映出的最新的系统状态。而<code>raft</code>找到并返回收到读请求那一刻集群中最大的<code>commitIndex</code>，就是为了找到已经完成（准确的说是从外部看已经完成的写操作）最近一次的写操作的日志条目，等到状态机应用了这条日志之后，再去读状态机当前的状态就是符合线性一致性的。</p>
<p>所以，<code>etcd/raft</code>收到读请求，经过内部处理，通过<code>Ready</code>结构返回给<code>etcd</code>那一时刻最大的<code>commitIndex</code>后，<code>etcd</code>要等到状态机运转到<code>commitIndex</code>之后，再去读状态机的状态，返回给外部Client，完成读操作。</p>
<hr>
<h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h4><h5 id="ReadOnlyOption"><a href="#ReadOnlyOption" class="headerlink" title="ReadOnlyOption"></a>ReadOnlyOption</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ReadOnlyOption <span class="type">int</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	<span class="comment">// ReadOnlySafe guarantees the linearizability of the read only request by</span></span><br><span class="line">	<span class="comment">// communicating with the quorum. It is the default and suggested option.</span></span><br><span class="line">	ReadOnlySafe ReadOnlyOption = <span class="literal">iota</span></span><br><span class="line">	<span class="comment">// ReadOnlyLeaseBased ensures linearizability of the read only request by</span></span><br><span class="line">	<span class="comment">// relying on the leader lease. It can be affected by clock drift.</span></span><br><span class="line">	<span class="comment">// If the clock drift is unbounded, leader might keep the lease longer than it</span></span><br><span class="line">	<span class="comment">// should (clock can move backward/pause without any bound). ReadIndex is not safe</span></span><br><span class="line">	<span class="comment">// in that case.</span></span><br><span class="line">	ReadOnlyLeaseBased</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><code>raft.go</code>中的<code>ReadOnlyOption</code>表示处理读请求采用的方法。目前读请求有两种方法：</p>
<ul>
<li>一种是<code>ReadOnlySafe</code>，对应论文中的ReadIndex方法。可以保证线性一致性，是默认的读请求处理方法。</li>
<li>一种是<code>ReadOnlyLeaseBased</code>，对应于论文中的LeaseRead方法。使用Leader的租约来保证线性一致性，但是这依赖于节点间的clock drift是否有一个界限，没有的话则不能保证一致性。使用这种方法时，必须保证支持<code>CheckQuorum</code>选项。</li>
</ul>
<hr>
<h5 id="readOnly结构"><a href="#readOnly结构" class="headerlink" title="readOnly结构"></a>readOnly结构</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> readIndexStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">	req   pb.Message</span><br><span class="line">	index <span class="type">uint64</span></span><br><span class="line">	acks <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="type">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> readOnly <span class="keyword">struct</span> &#123;</span><br><span class="line">	option           ReadOnlyOption</span><br><span class="line">	pendingReadIndex <span class="keyword">map</span>[<span class="type">string</span>]*readIndexStatus</span><br><span class="line">	readIndexQueue   []<span class="type">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newReadOnly</span><span class="params">(option ReadOnlyOption)</span></span> *readOnly</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> addRequest(index <span class="type">uint64</span>, m pb.Message)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> recvAck(id <span class="type">uint64</span>, context []<span class="type">byte</span>) <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> advance(m pb.Message) []*readIndexStatus</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> lastPendingRequestCtx() <span class="type">string</span></span><br></pre></td></tr></table></figure>

<p><code>read_only.go</code>中定义的<code>readOnly</code>结构，主要是<code>ReadOnlySafe</code>方法处理读请求时使用的结构。它主要用于在发送心跳消息前记录读请求相关信息，收到心跳响应之后检查是否收到了Majority节点的响应。</p>
<p><code>readIndexStatus</code>结构是记录读请求以及用于收到心跳回复后的Majority检查。其中的成员有：</p>
<ul>
<li><code>req</code>：<code>etcd</code>发来的<font color=red>读请求数据（实际上就是一个标识每个读请求的<code>requestId</code>）</font>；</li>
<li><code>index</code>：发心跳之时，<code>Leader</code>本身的<code>commitIndex</code>；</li>
<li><code>acks</code>：记录收到了哪些<code>Follower</code>的心跳响应；</li>
</ul>
<p><code>readOnly</code>结构的关键属性和方法如下：</p>
<ul>
<li><code>option</code>：表示处理读请求的方式；</li>
<li><code>pendingReadIndex</code>：在发送心跳消息的时候，以<code>etcd</code>发来的读请求数据为key，以<code>readIndexStatus</code>结构为value记录到<code>pendingReadIndex</code>中；收到<code>Follower</code>的心跳响应之后，在<code>readIndexStatus.acks</code>中记录，并检查是否收到了Majority节点的响应，以便进行下一步动作；</li>
<li><code>readIndexQueue</code>：发送心跳消息时，将<code>etcd</code>发来的读请求数据放到该队列中。这就保证了处理读请求的有序性；收到Majority节点的心跳响应之后，将相应的（包括队列中排在对应请求之前的读请求）读请求数据出队，返回给<code>etcd</code>相应的数据；</li>
<li><code>addRequest</code>方法：参数<code>index</code>就表示<code>Leader</code>的<code>commitIndex</code>；<code>m</code>就是<code>etcd</code>的读请求数据封装后的消息。该方法以<code>etcd</code>读请求数据为key，新建一个<code>readIndexStatus</code>结构记录到<code>readOnly.pendingReadIndex</code>中，并将读请求数据放到<code>readOnly.readIndexQueue</code>中进行排队；</li>
<li><code>recvAck</code>方法：收到<code>Follower</code>发来的心跳响应之后调用该方法。参数<code>id</code>表示<code>Follower</code>的<code>id</code>；<code>context</code>就表示发给该<code>Follower</code>心跳消息是附带的<code>etcd</code>请求数据。该方法就是以<code>context</code>为key，在<code>readOnly.pendingReadIndex</code>中寻找对应的<code>readIndexStatus</code>结构，并将其中的<code>acks[id]</code>置为<code>true</code>；</li>
<li><code>advance</code>方法：当明确已经收到针对某个<code>etcd</code>读请求的Majority个心跳响应之后，调用该方法。参数<code>m</code>就是对<code>etcd</code>读请求数据的封装。该方法就是从头轮训<code>readOnly.readIndexQueue</code>队列，针对其中的<code>etcd</code>读请求数据，在<code>readOnly.pendingReadIndex</code>中寻找对应的<code>readIndexStatus</code>结构，将其添加到<code>rss</code>结果队列中，直到找到对应的<code>etcd</code>读请求数据之后，才结束循环，并返回<code>rss</code>。<ul>
<li>该函数的处理逻辑是：假如依次收到了<code>A, B, C</code>三个读请求，这三个读请求就会发送<code>3</code>次心跳消息，并将三次读请求按序排队。假如收到了<code>C</code>对应的Majority个心跳回复（<code>A</code>和<code>B</code>的还不够Majority），那<code>advance</code>方法会把<code>A, B, C</code>三个读请求都返回，表示这三个读请求都可以处理了。这是因为能收到<code>C</code>的Majority个响应，肯定能保证发送<code>A</code>和<code>B</code>时，没有其他<code>Leader</code>当选（反证法即可证明）。</li>
</ul>
</li>
<li><code>lastPendingRequestCtx</code>方法：找到<code>readOnly.readIndexQueue</code>队列中最后一个元素，即最新的<code>etcd</code>读请求数据。</li>
</ul>
<hr>
<h5 id="ReadState结构"><a href="#ReadState结构" class="headerlink" title="ReadState结构"></a>ReadState结构</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// It&#x27;s caller&#x27;s responsibility to call ReadIndex first before getting this state from ready, it&#x27;s </span></span><br><span class="line"><span class="comment">// also caller&#x27;s duty to differentiate if this state is what it requests through RequestCtx, </span></span><br><span class="line"><span class="comment">// eg. given a unique id as RequestCtx</span></span><br><span class="line"><span class="keyword">type</span> ReadState <span class="keyword">struct</span> &#123;</span><br><span class="line">	Index      <span class="type">uint64</span></span><br><span class="line">	RequestCtx []<span class="type">byte</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>read_only.go</code>中定义的<code>ReadState</code>结构，是<code>etcd/raft</code>针对读请求，要返回给<code>etcd</code>的结构，会被封装在<code>Ready</code>结构中。</p>
<p>该结构就包含两个信息：</p>
<ul>
<li><code>Index</code>：收到读请求时最大的<code>commitIndex</code>；</li>
<li><code>RequestCtx</code>：<code>etcd</code>的读请求数据。这样<code>etcd</code>就能判断<code>ReadState</code>结构对应的是哪个读请求。</li>
</ul>
<hr>
<h4 id="ReadIndex方法的实现"><a href="#ReadIndex方法的实现" class="headerlink" title="ReadIndex方法的实现"></a>ReadIndex方法的实现</h4><h5 id="node模块接收读请求"><a href="#node模块接收读请求" class="headerlink" title="node模块接收读请求"></a>node模块接收读请求</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ReadIndex(ctx context.Context, rctx []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> step(ctx context.Context, m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> stepWithWaitOption(ctx context.Context, m pb.Message, wait <span class="type">bool</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br></pre></td></tr></table></figure>

<p><code>etcd</code>收到<code>Client</code>发来的读请求之后，首先就是调用<code>node.ReadIndex</code>方法，这是<code>etcd/raft</code>处理读请求的入口。</p>
<p><code>node.ReadIndex</code>方法就是把读请求<code>rctx</code>封装成<code>pb.MsgReadIndex</code>消息，然后经过<code>node.step、node.stepWithWaitOption</code>的调用发送到<code>node.recvc</code>管道。</p>
<p><code>node.run</code>方法在<code>node.recvc</code>管道上收到该消息后，调用<code>raft.Step</code>方法进行处理。</p>
<h5 id="Leader发送心跳消息"><a href="#Leader发送心跳消息" class="headerlink" title="Leader发送心跳消息"></a>Leader发送心跳消息</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepFollower</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> committedEntryInCurrentTerm() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sendMsgReadIndexResponse</span><span class="params">(r *raft, m pb.Message)</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> addRequest(index <span class="type">uint64</span>, m pb.Message)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> bcastHeartbeatWithCtx(ctx []<span class="type">byte</span>)</span><br></pre></td></tr></table></figure>

<p><code>raft.Step</code>方法中，针对<code>pb.MsgReadIndex</code>消息的处理都是通过调用<code>stepLeader/stepFollower</code>函数完成的：</p>
<ul>
<li>如果是<code>Follower</code>收到了读请求，则在<code>stepFollower</code>函数需要将<code>pb.MsgReadIndex</code>消息转发给<code>Leader</code>；</li>
<li>而<code>Leader</code>收到读请求，则在<code>stepLeader</code>中处理<code>pb.MsgReadIndex</code>消息；</li>
</ul>
<p>在<code>stepLeader</code>函数中，针对<code>pb.MsgReadIndex</code>消息的处理逻辑是：</p>
<ul>
<li>如果集群当前的配置中只有<code>Leader</code>一个节点，则直接调用<code>raft.responseToReadIndexReq</code>方法后返回。该方法是<code>raft</code>内部处理读请求的最后一步，在这里就是将当前<code>Leader</code>的<code>commitIndex</code>封装成<code>ReadState</code>结构，添加到<code>raft.readStates</code>中。后续在<code>node.run</code>这个goroutine中，会把<code>raft.readStates</code>封装到<code>Ready</code>结构中，返回给<code>Client</code>；</li>
<li>调用<code>raft.committedEntryInCurrentTerm</code>，判断当前的<code>Leader</code>是否已经以自己的Term提交过日志了。该方法的检测方法很简单，就是判断当前<code>Leader</code>的<code>commitIndex</code>的Term是不是自己的Term。如果该方法返回<code>false</code>，那现在就不能处理读请求，所以需要将当前的<code>pb.MsgReadIndex</code>消息先缓存到<code>raft.pendingReadIndexMessages</code>列表中。而<code>raft.pendingReadIndexMessages</code>列表中的消息处理时机是：<ul>
<li>在<code>Leader</code>刚当选时调用的<code>raft.becomeLeader</code>方法中，会在本地日志中追加空日志条目，然后接下来在<code>raft.bcastAppend</code>中通过<code>pb.MsgApp</code>将该日志广播给<code>Follower</code>；</li>
<li>等收到<code>Follower</code>对于<code>pb.MsgApp</code>的成功响应消息，即<code>pb.MsgAppResp</code>消息时，在<code>stepLeader</code>函数中，先调用<code>raft.maybeCommit</code>方法，查看是否有Majority节点成功Append了这条空日志条目；如果确实如此的话，就调用<code>releasePendingReadIndexMessages</code>函数，将<code>raft.pendingReadIndexMessages</code>列表中的<code>pb.MsgReadIndex</code>消息发出去；</li>
</ul>
</li>
<li>到了这里，就说明当前Leader本身的<code>commitIndex</code>就是收到读请求时最大的<code>commitIndex</code>。接下来要通过心跳消息来确定当前<code>Leader</code>依然是集群中的<code>Leader</code>。这一步通过调用<code>sendMsgReadIndexResponse</code>函数实现，该函数就是发送心跳消息。针对<code>ReadOnlySafe</code>选项而言，该函数的逻辑是：<ul>
<li>调用<code>readOnly.addRequest</code>方法，保存当前的<code>commitIndex</code>以及<code>etcd</code>发来的请求数据。这里以<code>etcd</code>读请求数据作为key，以新建的<code>readIndexStatus</code>结构作为value，添加到<code>readOnly.pendingReadIndex</code>这个map中，然后把<code>etcd</code>请求数据记录到<code>readOnly.readIndexQueue</code>这个队列中；</li>
<li>然后调用<code>readOnly.recvAck</code>方法，表示自己收到了心跳消息。<code>readOnly.recvAck</code>方法就是收到心跳响应消息之后，需要在<code>readOnly.pendingReadIndex</code>这个map中记录的<code>readIndexStatus</code>结构中，将响应消息的来源<code>id</code>记录到<code>acks</code>中，以便用于统计有多少Follower收到并成功恢复了心跳消息；</li>
<li>最后，调用<code>raft.bcastHeartbeatWithCtx</code>方法，向所有<code>Follower</code>广播心跳消息，注意，这里发送的心跳消息与普通心跳不同，这里附带了<code>etcd</code>的请求数据，以便收到响应后找到对应的<code>readIndexStatus</code>结构。</li>
</ul>
</li>
</ul>
<h5 id="Follower响应心跳消息，Leader完成读请求"><a href="#Follower响应心跳消息，Leader完成读请求" class="headerlink" title="Follower响应心跳消息，Leader完成读请求"></a>Follower响应心跳消息，Leader完成读请求</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> handleHeartbeat(m pb.Message)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> recvAck(id <span class="type">uint64</span>, context []<span class="type">byte</span>) <span class="keyword">map</span>[<span class="type">uint64</span>]<span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ro *readOnly)</span></span> advance(m pb.Message) []*readIndexStatus</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> responseToReadIndexReq(req pb.Message, readIndex <span class="type">uint64</span>) pb.Message</span><br></pre></td></tr></table></figure>

<p><code>Follower</code>收到心跳消息后，调用<code>handleHeartbeat</code>发送响应消息<code>pb.MsgHeartbeatResp</code>，并且其中附带了心跳消息中的读请求数据。</p>
<p><code>Leader</code>收到心跳响应之后，走完常规的处理流程之后，针对读请求有如下代码：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> r.readOnly.option != ReadOnlySafe || <span class="built_in">len</span>(m.Context) == <span class="number">0</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> r.prs.Voters.VoteResult(r.readOnly.recvAck(m.From, m.Context)) != quorum.VoteWon &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rss := r.readOnly.advance(m)</span><br><span class="line"><span class="keyword">for</span> _, rs := <span class="keyword">range</span> rss &#123;</span><br><span class="line">	<span class="keyword">if</span> resp := r.responseToReadIndexReq(rs.req, rs.index); resp.To != None &#123;</span><br><span class="line">		r.send(resp)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这部分的逻辑就是：</p>
<ul>
<li>如果当前<code>readOnly.option</code>不是<code>ReadOnlySafe</code>，即不是<code>ReadIndex</code>方式处理读请求，或者消息中的<code>Context</code>为空，即这是普通的心跳响应消息，而非针对读请求的心跳响应，则无需继续处理，直接返回；</li>
<li>然后调用<code>readOnly.recvAck</code>，根据消息中的<code>Context</code>，在<code>readOnly.pendingReadIndex</code>中找到对应的<code>readIndexStatus</code>结构，将响应消息的来源<code>id</code>记录到其中的<code>acks</code>。</li>
<li>然后统计有多少<code>Follower</code>收到并成功恢复了心跳消息。如果有Majority个<code>Follower</code>都响应的心跳消息，那就证明当前<code>Leader</code>还是<code>Leader</code>。所以可以继续往下处理；<font color=red>疑问：超时了怎么办？</font></li>
<li>调用<code>readOnly.advance</code>方法，在<code>ro.readIndexQueue</code>中，把当前心跳响应对应的读请求，以及所有在它之前的读请求的<code>ReadIndex</code>结构都返回。同时清理<code>ro.pendingReadIndex</code>和<code>ro.readIndexQueue</code>。</li>
<li>最后，调用<code>raft.responseToReadIndexReq</code>方法：<ul>
<li>如果读请求是<code>Leader</code>自己收到的，则将<code>ReadIndex</code>结构中的<code>commitIndex</code>以及<code>etcd</code>请求数据，封装成<code>ReadStates</code>结构，追加到<code>raft.readStates</code>队列中，等待将其封装到<code>Ready</code>结构并返回给<code>etcd</code>；</li>
<li>这里如果这个读请求是<code>Follower</code>转发给<code>Leader</code>的，则<code>Leader</code>要把<code>commitIndex</code>和<code>etcd</code>请求数据，封装成<code>pb.MsgReadIndexResp</code>消息，回复给<code>Follower</code>。<code>Follower</code>收到后，在把他们封装成<code>ReadState</code>结构，并添加到<code>raft.readStates</code>队列中；</li>
</ul>
</li>
</ul>
<p>至此，<code>etcd/raft</code>内部就完成了读请求的处理。</p>
<hr>
<h4 id="LeaseRead方法的实现"><a href="#LeaseRead方法的实现" class="headerlink" title="LeaseRead方法的实现"></a>LeaseRead方法的实现</h4><p><code>etcd/raft</code>中实现的<code>LeaseRead</code>方法非常简单，他没有额外的机制来检查clock drift是否有bound，而是默认就直接使用<code>Leader</code>心跳维护的<code>Lease</code>，即通过<code>raft.tickHeartbeat</code>方法中，每隔<code>heartbeatTimeout</code>个tick发送一次心跳消息来维护Lease。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ReadIndex(ctx context.Context, rctx []<span class="type">byte</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> step(ctx context.Context, m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> stepWithWaitOption(ctx context.Context, m pb.Message, wait <span class="type">bool</span>) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepFollower</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> committedEntryInCurrentTerm() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sendMsgReadIndexResponse</span><span class="params">(r *raft, m pb.Message)</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> responseToReadIndexReq(req pb.Message, readIndex <span class="type">uint64</span>) pb.Message</span><br></pre></td></tr></table></figure>

<p>LeaseRead的具体实现，直到<code>sendMsgReadIndexResponse</code>这里，跟ReadIndex方法都是一样的。而在<code>sendMsgReadIndexResponse</code>函数中，针对<code>ReadOnlyLeaseBased</code>选项，直接调用<code>raft.responseToReadIndexReq</code>方法，将当前的<code>commitIndex</code>，连同<code>etcd</code>的读请求数据，封装成<code>ReadStates</code>结构，追加到<code>raft.readStates</code>队列中，等待将其封装到<code>Ready</code>结构并返回给<code>etcd</code>；</p>
<hr>
<h4 id="etcd中raft之外的逻辑"><a href="#etcd中raft之外的逻辑" class="headerlink" title="etcd中raft之外的逻辑"></a>etcd中raft之外的逻辑</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *EtcdServer)</span></span> LinearizableReadNotify(ctx context.Context) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *EtcdServer)</span></span> linearizableReadNotify(ctx context.Context) <span class="type">error</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *EtcdServer)</span></span> linearizableReadLoop()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *EtcdServer)</span></span> requestCurrentIndex(leaderChangedNotifier &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;, requestId <span class="type">uint64</span>) (<span class="type">uint64</span>, <span class="type">error</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raftNode)</span></span> start(rh *raftReadyHandler)</span><br></pre></td></tr></table></figure>

<p><code>EtcdServer.LinearizableReadNotify</code>函数是<code>etcd</code>中读请求的入口，实际上该函数并不处理具体的读请求，这里它的作用类似于内存屏障。</p>
<p>外部<code>Client</code>发起读请求到<code>etcd</code>后，首先调用<code>EtcdServer.LinearizableReadNotify</code>函数，该函数主要就是保证读请求的线性一致性。该函数的作用就是：</p>
<ul>
<li>确认<code>Leader</code>当前的最大<code>commitIndex</code>；确认当前<code>Leader</code>仍然是集群中的<code>Leader</code>；</li>
<li>然后等待状态机执行日志到<code>commitIndex</code>之后的日志，就是收到读请求之时，当前集群中最大的<code>commitIndex</code>，说白了就是收到读请求时，最后更新状态机状态的操作索引；</li>
<li>然后等待状态机应用日志的<code>appliedIndex</code>超过了<code>commitIndex</code>，这就能保证，该接口返回的时候，状态机的状态肯定已经是比较新的了（至少状态肯定对应的是收到读请求操作之后的操作了），这就满足了线性一致性的要求；</li>
<li>之所以说它像内存屏障，就是因为该接口返回后，可以保证状态机已经完成了写操作，这个时候在处理读请求，就符合线性一致性的要求了；</li>
</ul>
<p>具体的处理读请求的流程是：</p>
<ol>
<li><code>EtcdServer.linearizableReadNotify</code>中向<code>EtcdServer.readwaitc</code>发送一个信号，然后等待<code>EtcdServer.readNotifier</code>上的通知；</li>
<li>在<code>EtcdServer.linearizableReadLoop</code>这个goroutine中，收到<code>EtcdServer.readwaitc</code>管道上的信号之后，调用<code>EtcdServer.requestCurrentIndex</code>方法。该方法<font color=red>以一个递增的<code>requestId</code>，作为读请求数据</font>调用<code>EtcdServer.sendReadIndex</code>，其内部会调用到<code>raft.ReadIndex</code>，开始<code>etcd/raft</code>内部的处理流程；然后在<code>EtcdServer.requestCurrentIndex</code>方法中，等待<code>EtcdServer.r.readStateC</code>上到来的<code>ReadState</code>；</li>
<li>在<code>raftNode.start</code>中的goroutine中，收到<code>etcd/raft</code>模块返回<code>Ready</code>结构后，向<code>EtcdServer.r.readStateC</code>管道发送<code>Ready</code>中的<code>ReadState</code>；</li>
<li>在<code>EtcdServerrequestCurrentIndex</code>中，收到<code>EtcdServer.r.readStateC</code>上到来的<code>ReadState</code>后，返回其中的<code>commitIndex</code>，这就是收到读请求时，集群中最大的<code>commitIndex</code>；</li>
<li>在<code>EtcdServer.linearizableReadLoop</code>这个goroutine中，得到读请求对应的<code>commitIndex</code>后，阻塞等待状态机的<code>appliedIndex</code>，即应用条目的Index超过<code>commitIndex</code>；最后向<code>EtcdServer.readNotifier</code>发通知；</li>
<li>在<code>EtcdServerlinearizableReadNotify</code>中，收到<code>EtcdServer.readNotifier</code>上的通知之后，就可以保证此时状态机中的状态已经是收到读请求之前最新的了，所以该接口的调用者接下来就可以直接访问状态机获取要读取的状态了。</li>
</ol>
<hr>
<h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/31050303">https://zhuanlan.zhihu.com/p/31050303</a></li>
<li><a href="https://z.itpub.net/article/detail/29B4D408D967AE015AF40C2C47F7E5AE">https://z.itpub.net/article/detail/29B4D408D967AE015AF40C2C47F7E5AE</a></li>
<li><a href="https://github.com/etcd-io/etcd/issues/741">https://github.com/etcd-io/etcd/issues/741</a></li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-14领导权转移</title>
    <url>/2022/04/10/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/14%E9%A2%86%E5%AF%BC%E6%9D%83%E8%BD%AC%E7%A7%BB/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>有时候需要将<code>Leader</code>的领导权转移到其他节点，比如<code>Leader</code>必须下线进行维护，或者是从集群中删除；或者某些情况下可能有更适合当<code>Leader</code>的节点。</p>
<span id="more"></span>
<p>Raft中实现领导权转移，当前<code>Leader</code>将日志发给目标节点，然后<font color=red>目标节点不等到选举超时就开始选举流程。</font>这样当前<code>Leader</code>保证目标节点拥有其任期开始到现在的所有日志，然后通过常规的选举流程，Majority节点的投票就保证了安全性。下面是详细过程：</p>
<ol>
<li>当前<code>Leader</code>停止接收<code>Client</code>发来的请求；</li>
<li>当前<code>Leader</code>同步日志给目标节点，保证其日志跟自己的一样；</li>
<li>当前<code>Leader</code>给目标节点发一个<code>TimeoutNow</code>消息，收到这个消息与选举计时器超时有相同的效果，目标节点开始选举，增加其Term并成为<code>Candidate</code>；新<code>Leader</code>的消息中带有增加后的Term，这就会让当前<code>Leader</code>自动下线，这样就完成了领导权的转移。</li>
</ol>
<p><font color=red>Raft已经保证了，即使节点时钟以任意速度运行也可以保证安全性，当目标节点收到<code>TimeoutNow</code>请求时，相当于目标节点的时钟发生了跳变，所以这是安全的。</font></p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> None <span class="type">uint64</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> raft <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">// leadTransferee is id of the leader transfer target when its value is not zero.</span></span><br><span class="line">	<span class="comment">// Follow the procedure defined in raft thesis 3.10.</span></span><br><span class="line">	leadTransferee <span class="type">uint64</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> abortLeaderTransfer()</span><br></pre></td></tr></table></figure>

<p><code>raft.leadTransferee</code>表示领导权转移的目标节点ID；检查该值是否为<code>None</code>，可以判断当前是否正在进行领导权的转移。当该值不为<code>None</code>时，在<code>stepLeader</code>中收到<code>pb.MsgProp</code>消息，即<code>Client</code>发来的提案消息，则直接不予处理。</p>
<p><code>raft.abortLeaderTransfer</code>方法仅仅将<code>raft.leadTransferee</code>置为<code>None</code>，表示终止领导权转移过程。</p>
<hr>
<h4 id="发送pb-MsgTransferLeader消息"><a href="#发送pb-MsgTransferLeader消息" class="headerlink" title="发送pb.MsgTransferLeader消息"></a>发送<code>pb.MsgTransferLeader</code>消息</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> TransferLeadership(ctx context.Context, lead, transferee <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p><code>Client</code>调用<code>node.TransferLeadership</code>，表示开始领导权转移。其中参数<code>lead</code>表示当前<code>Leader</code>，而<code>transferee</code>表示转移的目标节点ID。</p>
<p>该方法就是向<code>node.recvc</code>管道发送一个<code>pb.MsgTransferLeader</code>消息，消息中的<code>From</code>字段设置为<code>transferee</code>，而<code>To</code>设置为<code>lead</code>；</p>
<p>在<code>node.run</code>中从<code>node.recvc</code>管道收到该消息后，调用<code>raft.Step</code>处理。而该函数最终是调用<code>stepLeader</code>或<code>stepFollower</code>处理。在<code>stepFollower</code>中收到该消息，表示<code>Follower</code>节点收到了消息，它直接转发给<code>Leader</code>。</p>
<hr>
<h4 id="当前Leader处理pb-MsgTransferLeader消息"><a href="#当前Leader处理pb-MsgTransferLeader消息" class="headerlink" title="当前Leader处理pb.MsgTransferLeader消息"></a>当前<code>Leader</code>处理<code>pb.MsgTransferLeader</code>消息</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> sendTimeoutNow(to <span class="type">uint64</span>)</span><br></pre></td></tr></table></figure>

<p>当前<code>Leader</code>收到<code>pb.MsgTransferLeader</code>消息后，在<code>stepLeader</code>中进行处理。处理逻辑是：</p>
<ul>
<li>如果当前正在进行领导权转移，则判断如果当前转移的目标节点与新收到的消息中目标节点一样的话，则直接返回；否则调用<code>raft.abortLeaderTransfer</code>，终止当前的领导权转移流程；</li>
<li>重置<code>raft.electionElapsed</code>为<code>0</code>，这样在<code>raft.tickHeartbeat</code>中判断如果经过<code>electionTimeout</code>个tick之后，本节点还是<code>Leader</code>并且领导权转移流程还没完，则直接调用<code>raft.abortLeaderTransfer</code>终止领导权转移流程；</li>
<li>设置<code>raft.leadTransferee</code>为消息中的<code>From</code>字段，开始进行领导权的转移，具体而言就是：<ul>
<li>如果目标节点的日志跟当前<code>Leader</code>一样新，则直接调用<code>raft.sendTimeoutNow</code>，向目标节点发送<code>pb.MsgTimeoutNow</code>消息；</li>
<li>否则，调用<code>raft.sendAppend</code>，向目标节点同步日志。后续在<code>stepLeader</code>中收到目标节点发来的<code>pb.MsgAppResp</code>成功回复后，再次判断该节点的日志是否追上<code>Leader</code>了，如果是的话，调用<code>raft.sendTimeoutNow</code>向目标节点发送<code>pb.MsgTimeoutNow</code>消息；</li>
</ul>
</li>
</ul>
<hr>
<h4 id="目标节点处理pb-MsgTimeoutNow消息"><a href="#目标节点处理pb-MsgTimeoutNow消息" class="headerlink" title="目标节点处理pb.MsgTimeoutNow消息"></a>目标节点处理<code>pb.MsgTimeoutNow</code>消息</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> sendTimeoutNow(to <span class="type">uint64</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> hup(t CampaignType)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> campaign(t CampaignType)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> abortLeaderTransfer()</span><br></pre></td></tr></table></figure>

<p>目标<code>Follower</code>收到<code>pb.MsgTimeoutNow</code>消息后，在<code>stepFollower</code>中处理，就是调用<code>raft.hub(campaignTransfer)</code>，直接开始新一轮选举，这里直接进行的Vote流程，即<code>Follower</code>增加自己的Term变成<code>Candidate</code>后开始拉票；这里绕过了preVote流程；</p>
<p>其他<code>Follower</code>节点收到这种类型的拉票消息，即使当前尚在Lease之内，也会投票；</p>
<p>如果是当前<code>Leader</code>收到了这种拉票消息，因消息中的<code>Term</code>比较大，所以这里直接调用<code>raft.becomeFollower</code>转为<code>Follower</code>，其中会调用<code>raft.abortLeaderTransfer</code>表示结束领导权转移了，至此就表示领导权转移流程结束了。</p>
<p>另外，如果在领导权转移过程中，配置发生了变更，并且目标<code>Follower</code>被删了，则也要终止领导权转移流程；</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-11成员变更2-流程</title>
    <url>/2022/02/23/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/11%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B42-%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>以下内容，主要来自于Raft博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》中的“4 Cluster membership changes”。</p>
<p>配置变更过程中，<font color=red>最重要的保证，就是协议的安全性，即集群中只能有一个 <code>Leader</code>，也就是要保证变更期间不能出现脑裂。</font>因集群中所有节点不可能同时自动切换到新配置，所以在变更期间，需要防止集群分裂为两个不相交的Majority集合。比如下图中的场景，集群中一部分节点使用<code>Cold</code>，另一部分节点使用<code>Cnew</code>，一旦发生网络分区，很有可能出现<code>Cold</code>集合和<code>Cnew</code>集合各自选出一个<code>Leader</code>来的情况。</p>
<span id="more"></span>
<img src="/img/14成员变更/v2-26c7a68e9aae047a910c2071bcff4831_720w.jpg" alt="img" style="zoom: 50%;" />

<p>RAFT大论文和小论文中，对于配置变更提出了两种方法，一种是单节点变更，一种是联合共识。不管哪种方法，核心思想都是为了防止脑裂情况的出现：</p>
<ul>
<li>单节点变更：一次只能变更（添加、删除等）一个节点，这样就能保证<code>Cold</code>和<code>Cnew</code>的Majority肯定有交集，防止了集群被分裂成两个独立的Majority节点集合，从而避免了脑裂。</li>
<li>联合共识：使用两阶段变更，第一阶段是<code>Cold</code>转为<code>Cnew,old</code>，第二阶段是<code>Cnew,old</code>转为<code>Cnew</code>。持有<code>Cnew,old</code>配置的节点，任何决策，比如日志追加和<code>Leader</code>选举等，必须是<code>Cnew</code>和<code>Cold</code>中Majority都达成共识。这样就保证了<code>Cnew,old</code>是<code>Cold</code>的超集，也是<code>Cnew</code>的超集，不会出现脑裂的情况。</li>
</ul>
<p>RAFT作者更加推崇单节点变更算法，在<code>etcd/raft</code>的实现中，最初只实现了单节点变更算法，在2019年的commit中，实现了联合共识算法，但是具体的实现细节又与论文中的描述略有不同。</p>
<p>大论文中对单节点变更算法的实现中会遇到的问题有很详细的阐述。实际上很多问题都是共通的，这些问题在联合共识算法中也会遇到。下面是对大论文中单节点变更算法的总结：</p>
<ul>
<li><p><code>Leader</code>收到添加&#x2F;删除节点的配置变更请求时，将<code>Cnew</code>作为一条日志项，使用复制日志的方式将其通知给其他节点。<code>Cnew</code>日志只会复制到<code>Cnew</code>中的节点，而且使用<code>Cnew</code>中的Majority来决定<code>Cnew</code>日志是否提交。</p>
</li>
<li><p><font color=red>其他节点在收到该日志项并将其添加到本地日志之后，就认为其生效了.</font></p>
</li>
<li><p>如果多个配置变更同时进行，即使是单节点变更，也可能会造成上图中的脑裂场景。比如：</p>
<ul>
<li>初始集群配置，即<code>Cold</code>是<code>(1,2,3)</code>，<code>3</code>是<code>Leader</code>;</li>
<li>现在要添加<code>4</code>，配置为<code>(1,2,3,4)</code>的<code>Cnew1</code>的日志只在<code>3</code>上生成了，尚未追加到其他节点；</li>
<li>此时又开始添加<code>5</code>，然后配置为<code>(1,2,3,4,5)</code>的<code>Cnew2</code>日志追加到了<code>3,4,5</code>节点上。</li>
<li>所以这一时刻，<code>(1,2)</code>使用的是<code>Cold</code>，而<code>(3,4,5)</code>使用的是<code>Cnew2</code>，这就很容易造成脑裂的场景。</li>
<li>联合共识同样有这样的问题，所以必须保证当前配置变更完成后再进行下一次配置变更。</li>
</ul>
</li>
<li><p><code>Cnew</code>日志被提交（commit，而非apply）后，就认为配置变更完成了，此时<code>Leader</code>就可以明确<code>Cnew</code>中的Majority节点已经使用了<code>Cnew</code>配置。从而可以开始新的配置变更了。</p>
</li>
<li><p>节点直接使用日志中最新的配置，而不是等到配置提交后才使用。这使得<code>Leader</code>在当前配置未提交时，不会开始新的配置变更，避免了配置变更并行的场景。只有当<code>Cold</code>中的Majority节点都已经感知到<code>Cnew</code>后，才开始进行新的配置变更才是安全的。不过这种机制可能会使得配置日志被删除（当<code>Leader</code>发生变更时），所以节点必须能回退到日志中上一个配置。如果节点等到<code>Cnew</code>日志提交后才使用<code>Cnew</code>的配置，那<code>Leader</code>是无法感知是否有Majority的节点已经使用<code>Cnew</code>配置了。</p>
<p><font color=red>注</font>：因为节点使用日志中的最新配置，而非提交后的配置，只有当配置日志提交后，<code>Leader</code>才知道集群中Majority节点已经感知到新配置了，所以才能开始新的配置变更；而在<code>etcd/raft</code>的实现中，节点一直使用旧配置，直到配置日志apply后才使用新配置，因节点apply配置的时间不统一，<code>Leader</code>是无法感知是否有Majority个节点已经使用<code>Cnew</code>配置了。所以必须在选举时增加限制，即有已提交未应用的配置日志时，不允许参与选举；<font color=red>具体参考下面的“思考”一节</font></p>
</li>
<li><p>使用调用者（RPC发起者）的配置来实现共识，而非被调用者（RPC接收者）的配置。</p>
</li>
<li><p>新节点加入集群时，它本身肯定无任何日志，如果它直接成为集群的一员，那他的日志需要很长一段时间才能追上<code>Leader</code>，这段时间内，集群很容易处于不可用的状态。为了避免该问题，Raft在进行配置变更之前引入了新的日志追加阶段，这个阶段内，新节点会作为非投票成员，<code>Leader</code>会向新节点同步日志，而在投票或者日志复制时，不会把它当做形成Majority的条件。当新节点的日志追上集群后，配置变更才可以继续进行。</p>
</li>
<li><p>如果<code>Cnew</code>中要删除当前的<code>Leader</code>，则<code>Leader</code>要等到<code>Cnew</code>提交之后才能下线。只有到了这个时间点，新配置才可以在没有被移除的<code>Leader</code>参与的情况下继续运行集群：即<code>Cnew</code>中可以选举出新的<code>Leader</code>。</p>
</li>
<li><p>当<code>Leader</code>创建好<code>Cnew</code>日志后，<code>Cnew</code>之外的节点就再也收不到心跳包了，所以该节点最终会超时，并且开始新的选举，从而破坏当期的集群。一旦<code>Leader</code>创建了<code>Cnew</code>日志后，被移除的节点就已经具有破坏性了。Raft的解决方法是使用心跳来检测有效<code>Leader</code>的存在（租约）。在Raft中，只要<code>Leader</code>能维持与<code>Follower</code>的心跳消息，该<code>Leader</code>就被认为是active的。因此，移除节点就无法破坏当前的<code>Leader</code>了。</p>
</li>
</ul>
<hr>
<h3 id="单节点变更的问题"><a href="#单节点变更的问题" class="headerlink" title="单节点变更的问题"></a>单节点变更的问题</h3><h4 id="覆盖已提交的日志"><a href="#覆盖已提交的日志" class="headerlink" title="覆盖已提交的日志"></a>覆盖已提交的日志</h4><p>对于单节点变更算法，实际上还有一个BUG，Raft作者在<a href="%5Bhttps://groups.google.com/g/raft-dev/c/t4xj6dJTP6E%5D(https://link.zhihu.com/?target=https://groups.google.com/g/raft-dev/c/t4xj6dJTP6E%22%20%5Ct%20%22_blank)">Raft-dev</a>详细的说明了这个问题。考虑下面的场景：</p>
<ul>
<li>t₀：节点<code>a b c d</code>的成员配置为<code>C₀</code>；</li>
<li>t₁：节点<code>a b c d</code>在Term 0选出<code>a</code>为<code>Leader</code>，<code>b</code>和<code>c</code>为<code>Follower</code>；</li>
<li>t₂：节点<code>a</code>同步成员变更日志<code>Cᵤ</code>，只同步到<code>a</code>和<code>u</code>，未成功提交，但是对于<code>a</code>和<code>u</code>而言，集群的配置已经变成了<code>abcdu</code>；</li>
<li>t₃：节点<code>a</code>宕机；</li>
<li>t₄：节点<code>d</code>在Term 1被选为<code>Leader</code>，<code>b</code>和<code>c</code>为<code>Follower</code>；</li>
<li>t₅：节点<code>d</code>同步成员变更日志<code>Cᵥ</code>，同步到<code>cdv</code>，成功提交；所以，对于<code>cdv</code>而言，集群的配置变成了<code>abcdv</code>；</li>
<li>t₆：节点<code>d</code>同步普通日志E，同步到<code>cdv</code>，成功提交；</li>
<li>t₇：节点<code>d</code>宕机；</li>
<li>t₈：节点<code>a</code>在Term 2重新选为<code>Leader</code>，<code>u</code>和<code>b</code>为<code>Follower</code>；<code>a</code>重启后，其内部记录的集群配置是<code>abcdu</code>；而<code>b</code>始终没有收到任何配置变更消息，所以其内部记录的集群配饰还是<code>abcd</code>，<code>a</code>发起选举后，<code>u</code>和<code>b</code>投票给了<code>a</code>，对于<code>a</code>而言，他认为自己拿到了<code>abcdu</code>中的<code>3</code>票，所以成为了新<code>Leader</code>；但是实际上当前已经提交的集群配置是<code>abcdv</code>；</li>
<li>t₉：节点<code>a</code>同步本地的日志<code>Cᵤ</code>给所有人，造成已提交的<code>Cᵥ</code>和<code>E</code>丢失。</li>
</ul>
<p>产生该问题的原因，有以下两点：</p>
<ul>
<li>节点在将配置变更条目添加到本地日志之后，就认为其生效了；</li>
<li>上一任<code>Leader</code>的成员变更日志<code>Cnew1</code>还没有同步到多数派就宕机了，新<code>Leader</code>没有收到<code>Cnew1</code>，并且一上任就进行成员变更<code>Cnew2</code>，使用新的成员配置提交日志；</li>
<li>上一任<code>Leader</code>重新上任之后，还是按照<code>Cnew1</code>继续成员变更，可能形成另外一个多数派集合，产生脑裂，将已提交的日志覆盖，造成数据丢失。</li>
</ul>
<p>Raft作者在发现这个问题之后，也给出了修复方法。修复方法很简单, 跟Raft的日志Commit条件类似：新任<code>Leader</code>必须在当前Term提交一条no-op日志之后，才允许同步成员变更日志。也即<code>Leader</code>在当前Term还未提交日志之前，不允许同步成员变更日志。</p>
<p><font color=red>对应上面这个例子，就是<code>d</code>当选<code>Leader</code>后必须先提交一条no-op日志，该no-op只有复制到<code>C₀</code>中Majority之后才能提交。然后<code>L₁</code>才能开始同步<code>Cᵥ</code>和<code>E</code>，这样，当<code>a</code>竞选<code>L₂</code>时，因日志不够新，所以当不了新<code>Leader</code>。</font>具体场景如下：</p>
<ul>
<li>t₀：节点<code>a b c d</code>的成员配置为<code>C₀</code>；</li>
<li>t₁：节点<code>a b c d</code>在Term 0选出<code>a</code>为<code>Leader</code>，<code>b</code>和<code>c</code>为<code>Follower</code>；</li>
<li>t₂：节点<code>a</code>同步成员变更日志<code>Cᵤ</code>，只同步到<code>a</code>和<code>u</code>，未成功提交，但是对于<code>a</code>和<code>u</code>而言，集群的配置已经变成了<code>abcdu</code>；</li>
<li>t₃：节点<code>a</code>宕机；</li>
<li>t₄：节点<code>d</code>在Term 1被选为<code>Leader</code>，<code>b</code>和<code>c</code>为<code>Follower</code>；</li>
<li>t₅：<code>d</code>提交一个no-op日志，此时<code>d</code>认为的集群配置是<code>abcd</code>，所以该no-op日志肯定到达了Majority节点，即<code>bcd</code>；</li>
<li>t₆：节点<code>d</code>同步成员变更日志<code>Cᵥ</code>，同步到<code>cdv</code>，成功提交；所以，对于<code>cdv</code>而言，集群的配置变成了<code>abcdv</code>；</li>
<li>t₇：节点<code>d</code>同步普通日志E，同步到<code>cdv</code>，成功提交；</li>
<li>t₈：节点<code>d</code>宕机；</li>
<li>t₉：<code>a</code>重启后，其内部记录的集群配置是<code>abcdu</code>；<code>a</code>发起选举，因<code>a</code>的日志肯定要比<code>bcd</code>旧（没有no-op日志），所以它最多能得到<code>u</code>的支持，从而当不了<code>Leader</code>；</li>
</ul>
<h4 id="可用性问题"><a href="#可用性问题" class="headerlink" title="可用性问题"></a>可用性问题</h4><p>单步成员变更每次只能增加或者减少一个成员，在做成员替换的时候需要分两次变更，第一次变更先将新成员加入进来，第二次变更再将老成员删除，中间如果如果网络分区，有可能会导致服务不可用。</p>
<p>比如原集群包含<code>3</code>个节点<code>abc</code>，现在要将<code>a</code>替换为<code>d</code>，则使用单节点变更算法的话，需要两次单节点变更：<code>abc -&gt; abcd -&gt; bcd</code>。</p>
<p>考虑<code>a、b、c</code>三个成员部署在三个机房，<code>d</code>跟<code>a</code>在同一个机房。则当集群为<code>abcd</code>时，有可能在出现二分的网络分区<code>(ad | bc)</code>导致整个集群不可用。</p>
<p>如果使用联合共识算法的话，则不存在正确性和可用性问题。</p>
<hr>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="配置变更提案"><a href="#配置变更提案" class="headerlink" title="配置变更提案"></a>配置变更提案</h4><p>使用Raft库的客户端，需要调用<code>node.ProposeConfChange</code>方法来发起一个配置变更的提案，这是配置变更的<font color=red>起点</font>。</p>
<p>和其他的提案一样，配置变更提案可能被丢弃，比如如果上一次的配置变更尚未完全应用，则本次配置变更提案会直接被丢弃。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">//ProposeConfChange方法可以接受pb.ConfChange消息（已废弃），也可以接受pb.ConfChangeV2消息。</span></span><br><span class="line"><span class="comment">//pb.ConfChangeV2消息支持任意节点变更的联合共识配置变更算法。</span></span><br><span class="line"><span class="comment">//使用ConfChangeV2消息必须保证集群中所有节点都认识V2的API。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ProposeConfChange(ctx context.Context, cc pb.ConfChangeI) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">confChangeToMsg</span><span class="params">(c pb.ConfChangeI)</span></span> (pb.Message, <span class="type">error</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">MarshalConfChange</span><span class="params">(c ConfChangeI)</span></span> (EntryType, []<span class="type">byte</span>, <span class="type">error</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Step(ctx context.Context, m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> step(ctx context.Context, m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> stepWithWaitOption(ctx context.Context, m pb.Message, wait <span class="type">bool</span>) <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p><code>node.PropseConfChange</code>可以接收<code>pb.ConfChange</code>结构体（早期版本消息，当前已废弃）或 <code>pb.ConfChangeV2</code>结构体作为参数。为了兼容这两种消息，<code>raftpb/confichange.go</code>中提供了<code>ConfChangeI</code>接口：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ConfChangeI <span class="keyword">interface</span> &#123;</span><br><span class="line">	AsV2() ConfChangeV2</span><br><span class="line">	AsV1() (ConfChange, <span class="type">bool</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后为<code>ConfChangeV2</code>和<code>ConfChange</code>结构添加<code>AsV1</code>和<code>AsV2</code>方法，从而它们都实现了该接口。<code>ConfChange</code>结构可以封装成<code>ConfChangeV2</code>结构，但是<code>ConfChangeV2</code>结构无法转换成<code>ConfChange</code>结构。</p>
<p><code>ProposeConfChange</code>内部会首先调用<code>confChangeToMsg</code>函数，将<code>ConfChange</code>或<code>ConfChangeV2</code>结构转换成<code>pb.MsgProp</code>消息，即封装成普通的提案消息，这种提案消息的格式是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pb.Message&#123;</span><br><span class="line">	Type: pb.MsgProp,</span><br><span class="line">    Entries: []pb.Entry&#123;&#123;Type: typ, Data: data&#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中<code>pb.Entry</code>的<code>Type</code>和<code>Data</code>就是<code>confChangeToMsg</code>中调用<code>raftpb.MarshalConfChange</code>实现的。该函数中判断：</p>
<ul>
<li>如果是<code>ConfChange</code>结构，则调用<code>ConfChange.Marshal()</code>得到具体的<code>Data</code>，而<code>Type</code>为<code>EntryConfChange</code>；</li>
<li>如果是<code>ConfChangeV2</code>结构，则调用<code>ConfChangeV2.Marshal()</code>得到具体的<code>Data</code>，而<code>Type</code>为<code>EntryConfChangeV2</code>。</li>
</ul>
<p>配置变更提案消息，经过<code>node.Step -&gt; node.step -&gt; node.stepWithWaitOption</code> 的调用后，发送到<code>node.propc</code>通道。</p>
<hr>
<h4 id="处理配置变更提案"><a href="#处理配置变更提案" class="headerlink" title="处理配置变更提案"></a>处理配置变更提案</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run() </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> Step(m pb.Message) <span class="type">error</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">stepLeader</span><span class="params">(r *raft, m pb.Message)</span></span> <span class="type">error</span></span><br></pre></td></tr></table></figure>

<p><code>node.run</code>中，从<code>node.propc</code>通道中获取配置变更提案消息，跟普通的提案消息一样，调用<code>raft.Step</code>，最终在<code>stepLeader</code>回调函数中进行处理。</p>
<p>在<code>stepLeader</code>中，针对提案消息中的每一个提案Entry，如果它是配置变更提案消息，即<code>pb.Entry</code>中的<code>Type</code>是<code>pb.ConfChange</code>或<code>pb.EntryConfChangeV2</code>，会做以下检查：</p>
<ul>
<li>根据<code>Type</code>，调用相应的<code>Unmarshal</code>方法将消息数据重新解码成对应的<code>ConfChange</code>或<code>ConfChangeV2</code>结构，如果解码失败，则直接panic；</li>
<li>当收到配置变更提案时，就会把<code>raft.pendingConfIndex</code>设置为当前条目对应的Index，所以当<code>raft.pendingConfIndex</code>大于<code>raft.raftLog.applied</code>时，表示尚有未应用的配置变更日志，这种情况下又收到了新的配置变更提案时，即<code>alreadyPending</code>为<code>true</code>，会被拒绝；</li>
<li>在联合共识算法中，第一阶段是从<code>Cold</code>过渡到<code>Cnew,old</code>，第二阶段是从<code>Cnew,old</code>过渡到<code>Cnew</code>，从<code>Cnew,old</code>过渡到<code>Cnew</code>，需要一个空的<code>ConfChangeV2</code>结构来驱动。如果当前判断<code>len(r.prs.Config.Voters[1])</code>大于<code>0</code>，表明已经处于<code>Cnew,old</code>阶段，即<code>alreadyJoint</code>为<code>true</code>，此时如果收到的<code>ConfChangeV2</code>结构非空，即<code>wantsLeaveJoint</code>为<code>true</code>，会被拒绝；</li>
<li>如果<code>alreadyJoint</code>为<code>false</code>的情况下，收到了空<code>ConfChangeV2</code>结构，即<code>wantsLeaveJoint</code> 为<code>true</code>，也会被拒绝；</li>
</ul>
<p>如果配置变更提案被拒绝，则将日志条目中的<code>Type</code>置为<code>pb.EntryNormal</code>，即普通的提案消息，相当于把配置变更提案忽略掉；如果没被拒绝，则将<code>raft.pendingConfIndex</code>设置为当前最后日志条目的Index；</p>
<p>剩下的流程，就跟普通的日志追加一样了，即现将日志条目追加到本地中，然后通过<code>pb.MsgProp</code>消息发送给所有<code>Follower</code>；<code>Follower</code>收到<code>pb.MsgProp</code>消息后，将日志条目追加到本地，回复给<code>Leader</code>；<code>Leader</code>收到回复，增加<code>commitIndex</code>，通过<code>Ready</code>通道将已经Commit的日志发送给Raft外部的Client，以便将其应用到状态机中。</p>
<hr>
<h4 id="状态机应用配置变更"><a href="#状态机应用配置变更" class="headerlink" title="状态机应用配置变更"></a>状态机应用配置变更</h4><p>在Raft库外部，当应用Raft库的Client收到<code>Ready</code>中的日志后，针对已提交日志中的<code>pb.EntryConfChange</code>或<code>pb.EntryConfChangeV2</code>类型的条目，调用<code>node.ApplyConfChange</code>，对配置变更进行应用。</p>
<p>比如在<code>raftexample/raft.go</code>的<code>raftNode.publishEntries</code>方法中，就会调用<code>node.ApplyConfChange</code>应用配置变更。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> ApplyConfChange(cc pb.ConfChangeI) *pb.ConfState</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> applyConfChange(cc pb.ConfChangeV2) pb.ConfState</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> switchToConfig(cfg tracker.Config, prs tracker.ProgressMap) pb.ConfState</span><br></pre></td></tr></table></figure>

<p>在<code>node.ApplyConfChange</code>方法中，调用<code>pb.ConfChangeI</code>接口的<code>AsV2</code>方法，得到一个<code>ConfChangeV2</code>结构（即使原来是<code>ConfChange</code>结构，也可以封装成<code>ConfChangeV2</code>），将其发送到<code>n.confc</code>通道，然后等待<code>n.confstatec</code>上返回的最新的配置状态<code>pb.confState</code>。</p>
<p>在<code>node.run</code>方法中，从<code>n.confc</code>通道上收到<code>ConfChangeV2</code>结构的<code>cc</code>后：</p>
<ul>
<li>调用<code>raft.applyConfChange(cc)</code>，该方法返回最新的配置状态<code>cs</code>。</li>
<li>判断如果当前节点在应用最新的配置变更之前还有对应<code>Progress</code>结构，而配置变更后已经没有对应的<code>Progress</code>结构的话，说明本节点在配置变更中被清除出集群了，所以设置<code>propc</code>通道为<code>nil</code>，即不再接收任何的提案消息；</li>
<li>将最新的配置状态发送到<code>n.confstatec</code>通道，这样Client端在<code>node.ApplyConfChange</code>中就能返回；</li>
</ul>
<p><code>raft.applyConfChange</code>是应用配置变更的核心函数。它的逻辑是：</p>
<ul>
<li>首先根据当前的<code>r.prs</code>和<code>r.raftlog.lastIndex</code>，构造新的<code>confchange.Changer</code>结构<code>changer</code>。然后调用<code>pb.ConfChangeV2</code>的<code>LeaveJoint</code>和<code>EnterJoint</code>方法判断当前使用什么配置变更算法以及所处的阶段，以此决定调用<code>changer.Simple</code>、<code>changer.EnterJoint</code>或<code>changer.LeaveJoint</code>方法，这些函数返回的<code>cfg</code>和<code>prs</code>都是<code>changer</code>的副本；</li>
<li>接下来以这些副本为参数，调用<code>r.switchToConfig(cfg, prs)</code>，将这些配置副本应用到Raft中；</li>
</ul>
<p><code>switchToConfig</code>函数是应用配置变更的最后一步，其逻辑是：</p>
<ul>
<li>首先将参数<code>cfg</code>和<code>prs</code>作为当前节点最新的配置，即将<code>r,prs.Config</code>置为<code>cfg</code>，将<code>r.prs.Progress</code>置为<code>prs</code>；</li>
<li>如果在<code>r.prs.Progress</code>中找不到当前节点对应的<code>Progress</code>了，说明本节点在新配置中被删除了；</li>
<li>如果本节点没有删除，并且对应的<code>Progress</code>中<code>IsLearner</code>为<code>true</code>，说明本节点变成<code>Learnner</code>了；</li>
<li>如果节点当前是<code>Leader</code>，并且节点被删除了，或者节点降级为<code>Learner</code>了，这种情况下，<code>Leader</code>需要下线，由其他<code>Follower</code>重新选举新的<code>Leader</code>，这里直接返回。<font color=red>注意</font>：代码中并未找到这种情况下<code>Leader</code>下线的具体逻辑，只是在<code>stepLeader</code>中，如果节点是被删除了，该节点不会对<code>pb.MsgProp</code>提案消息进行处理；但是下线的<code>Leader</code>好像还能发心跳；</li>
<li>如果节点不是<code>Leader</code>，或者最新的配置中，<code>Leader</code>没有<code>Follower</code>，这种情况下直接返回，不用下面的处理；</li>
<li>现在走到这里，说明本节点是<code>Leader</code>，并且有<code>Follower</code>，这种情况下，因为配置变更了，即有的节点可能被删除了，也可能新增了节点。所以：<ul>
<li>调用<code>r.maybeCommit</code>，查看是否因为删除了节点，导致<code>committed Index</code>需要更新，如果该函数为<code>true</code>，所以调用<code>r.bcastAppend</code>通知其他<code>Follower</code>更新<code>commitIndex</code>；</li>
<li>如果<code>r.maybeCommit</code>返回<code>false</code>，则针对所有<code>Follower</code>调用<code>r.maybeSendAppend</code>，以便将日志复制到新增节点上；</li>
<li>如果当前正在进行领导权转移，并且被转移的节点被删除了，则调用<code>r.abortLeaderTransfer</code>终止领导权转移；</li>
</ul>
</li>
</ul>
<hr>
<h4 id="判断如何进行配置变更"><a href="#判断如何进行配置变更" class="headerlink" title="判断如何进行配置变更"></a>判断如何进行配置变更</h4><p>在<code>raft.applyConfChange</code>应用配置变更时，它需要根据配置选项和当前状态，决定是使用单节点变更算法，还是联合共识算法，如果使用联合共识的话，需要决定当前是需要从<code>Cold</code>到<code>Cnew,old</code>，还是要从<code>Cnew,old</code>到<code>Cnew</code>。</p>
<p>相关的配置选项主要是<code>raft.pb.go</code>中定义的<code>ConfChangeTransition</code>，其定义如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ConfChangeTransition <span class="type">int32</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	<span class="comment">//优先使用单节点变更算法，即只要ConfChangeV2.Changes只包含一个变更动作，就使用但节点变更；</span></span><br><span class="line">	<span class="comment">//否则的话跟ConfChangeTransitionJointImplicit配置一样，即使用联合共识算法，并且自动过渡到Cnew；</span></span><br><span class="line">	<span class="comment">//大多数应用采用该配置</span></span><br><span class="line">	ConfChangeTransitionAuto ConfChangeTransition = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//使用联合共识算法，并且在RAFT库内自动的由Cnew,old过渡到Cnew状态。</span></span><br><span class="line">	<span class="comment">//这种配置适用于希望缩短联合状态，即Cnew,old的时间，并且不会在状态机中保存联合共识状态的应用程序</span></span><br><span class="line">	ConfChangeTransitionJointImplicit ConfChangeTransition = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//使用联合共识算法，并且由应用程序控制何时从Cnew,old过渡到Cnew状态。</span></span><br><span class="line">	<span class="comment">//这种配置适用于希望显式控制配置变更的应用程序。</span></span><br><span class="line">	ConfChangeTransitionJointExplicit ConfChangeTransition = <span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Raft中最初只实现了单节点变更算法，对应的是<code>pb.ConfChange</code>结构；后续为了实现联合共识，引入了<code>pb.ConfChangeV2</code>结构，该结构能兼容单节点变更算法和联合共识算法。实施什么算法，具体的判断逻辑是在<code>pb.ConfChangeV2</code>的<code>LeaveJoint</code>和<code>EnterJoint</code>方法中。</p>
<p><code>pb.ConfChange</code>和<code>pb.ConfChangeV2</code>的具体定义如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ConfChangeType <span class="type">int32</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">	ConfChangeAddNode        ConfChangeType = <span class="number">0</span></span><br><span class="line">	ConfChangeRemoveNode     ConfChangeType = <span class="number">1</span></span><br><span class="line">	ConfChangeUpdateNode     ConfChangeType = <span class="number">2</span></span><br><span class="line">	ConfChangeAddLearnerNode ConfChangeType = <span class="number">3</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ConfChange <span class="keyword">struct</span> &#123;</span><br><span class="line">	Type    ConfChangeType <span class="string">`protobuf:&quot;varint,2,opt,name=type,enum=raftpb.ConfChangeType&quot; json:&quot;type&quot;`</span></span><br><span class="line">	NodeID  <span class="type">uint64</span>         <span class="string">`protobuf:&quot;varint,3,opt,name=node_id,json=nodeId&quot; json:&quot;node_id&quot;`</span></span><br><span class="line">	Context []<span class="type">byte</span>         <span class="string">`protobuf:&quot;bytes,4,opt,name=context&quot; json:&quot;context,omitempty&quot;`</span></span><br><span class="line">	<span class="comment">// NB: this is used only by etcd to thread through a unique identifier.</span></span><br><span class="line">	<span class="comment">// Ideally it should really use the Context instead. No counterpart to</span></span><br><span class="line">	<span class="comment">// this field exists in ConfChangeV2.</span></span><br><span class="line">	ID <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,1,opt,name=id&quot; json:&quot;id&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ConfChangeSingle <span class="keyword">struct</span> &#123;</span><br><span class="line">	Type   ConfChangeType <span class="string">`protobuf:&quot;varint,1,opt,name=type,enum=raftpb.ConfChangeType&quot; json:&quot;type&quot;`</span></span><br><span class="line">	NodeID <span class="type">uint64</span>         <span class="string">`protobuf:&quot;varint,2,opt,name=node_id,json=nodeId&quot; json:&quot;node_id&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ConfChangeV2 <span class="keyword">struct</span> &#123;</span><br><span class="line">	Transition ConfChangeTransition <span class="string">`protobuf:&quot;varint,1,opt,name=transition,enum=raftpb.ConfChangeTransition&quot; json:&quot;transition&quot;`</span></span><br><span class="line">	Changes    []ConfChangeSingle   <span class="string">`protobuf:&quot;bytes,2,rep,name=changes&quot; json:&quot;changes&quot;`</span></span><br><span class="line">	Context    []<span class="type">byte</span>               <span class="string">`protobuf:&quot;bytes,3,opt,name=context&quot; json:&quot;context,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到<code>pb.ConfChange</code>只支持单节点变更，而<code>pb.ConfChangeV2</code>则既支持单节点变更，也支持多节点变更。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c ConfChangeV2)</span></span> EnterJoint() (autoLeave <span class="type">bool</span>, ok <span class="type">bool</span>)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c ConfChangeV2)</span></span> LeaveJoint() <span class="type">bool</span></span><br></pre></td></tr></table></figure>

<p><code>ConfChangeV2.EnterJoint</code>方法根据当前的配置选项，决定使用单节点变更算法还是联合共识算法，并且如果使用联合共识算法，判断是否自动有<code>Cnew,old</code>过渡到<code>Cnew</code>状态。该方法返回两个<code>bool</code>值，第二个<code>bool</code>值表示是否采用联合共识算法，第一个<code>bool</code>值表示如果采用联合共识，是否自动过渡状态，如果第二个<code>bool</code>值为<code>false</code>，则第一个肯定是<code>false</code>。<code>ConfChangeV2.EnterJoint</code>方法的具体逻辑是：</p>
<ul>
<li>如果<code>c.Transition</code>等于<code>ConfChangeTransitionAuto</code> 并且 <code>len(c.Changes)</code> 小于等于<code>1</code>，则采用单节点变更算法，因此返回<code>false, false</code>；</li>
<li>其他情况下，说明需要采用联合共识算法：<ul>
<li>如果<code>c.Transition</code>为<code>ConfChangeTransitionAuto</code>或<code>ConfChangeTransitionJointImplicit</code>，则自动由<code>Cnew,old</code>过渡到<code>Cnew</code>状态；返回<code>true, true</code>，表示采用联合共识算法，并且当前可以由<code>Cold</code>过渡到<code>Cnew,old</code>，并且后续可以自动的由<code>Cnew,old</code>过渡到<code>Cnew</code>；</li>
<li>如果<code>c.Transition</code>为<code>ConfChangeTransitionJointExplicit</code>，则需要手动过渡到<code>Cnew</code>状态；因此返回<code>false,true</code>，表示采用联合共识算法，并且当前可以由<code>Cold</code>过渡到<code>Cnew,old</code>，后续需要手动的由<code>Cnew,old</code>过渡到<code>Cnew</code>；</li>
</ul>
</li>
</ul>
<p><code>ConfChangeV2.LeaveJoint</code>方法用于判断是否需要由<code>Cnew,old</code>过渡到<code>Cnew</code>。判断依据很简单，只要当前的<code>c</code>是一个空的<code>pb.ConfChangeV2</code>结构，就返回<code>true</code>。</p>
<hr>
<h4 id="单节点变更"><a href="#单节点变更" class="headerlink" title="单节点变更"></a>单节点变更</h4><p>在<code>raft.applyConfChange</code>应用配置变更时，如果调用<code>ConfChangeV2.EnterJoint</code>返回<code>false,false</code>，说明需要进行单节点变更，这主要是通过调用<code>Changer.Simple</code>方法实现。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> Simple(ccs ...pb.ConfChangeSingle) (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p>单节点配置变更算法需要保证只变更一个节点。在<code>Changer.Simple</code>方法中，<code>outgoing</code>为空，直接在副本的<code>incoming</code>上中实施配置变更，<code>Changer.Simple</code>函数的逻辑如下：</p>
<ul>
<li>首先调用<code>Changer.checkAndCopy</code>方法得到<code>c.Tracker.Config</code>和<code>c.Tracker.Progress</code>的副本，后续的动作都是在副本上执行；</li>
<li>如果当前处于联合共识阶段，则返回<code>&quot;can&#39;t apply simple config change in joint config&quot;</code>错误；</li>
<li>在副本上调用<code>Changer.apply</code>，实施配置变更；这里主要是根据<code>ccs</code>中消息的类型，调用<code>Changer.makeVoter</code>、<code>Changer.makeLearner</code>或<code>Changer.remove</code>；</li>
<li>调用<code>symdiff</code>，比较副本<code>cfg</code>和<code>c.Tracker.Voters</code>，如果差值大于<code>1</code>，则返回<code>&quot;more than one voter changed without entering joint config&quot;</code>错误；</li>
<li>最后调用<code>checkAndReturn</code>对变更后的副本进行一致性检查，并返回副本；</li>
</ul>
<p>调用完<code>Changer.Simple</code>，得到变更后的<code>tracker.Config</code>和<code>tracker.ProgressMap</code>副本后，在<code>raft.applyConfChange</code>方法中，将副本传给<code>raft.switchToConfig</code>将副本应用，至此，单节点变更就完成了。</p>
<hr>
<h4 id="联合共识"><a href="#联合共识" class="headerlink" title="联合共识"></a>联合共识</h4><h5 id="EnterJoint"><a href="#EnterJoint" class="headerlink" title="EnterJoint"></a>EnterJoint</h5><p>在<code>raft.applyConfChange</code>应用配置变更时，如果调用<code>ConfChangeV2.EnterJoint</code>返回<code>true,true</code>或<code>false,true</code>，说明需要进行联合共识算法，并进行<code>Cold</code>到<code>Cnew,old</code>的过渡。这主要是通过调用<code>Changer.EnterJoint</code>方法实现。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> EnterJoint(autoLeave <span class="type">bool</span>, ccs ...pb.ConfChangeSingle) (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p><code>Changer.EnterJoint</code>函数将当前配置(<code>incoming</code>)复制一份(<code>outgoing</code>)，然后在原配置(<code>incoming</code>)上执行变更动作。在<code>Changer.EnterJoint</code>函数中：</p>
<ul>
<li>首先调用<code>checkAndCopy</code>方法得到<code>c.Tracker.Config</code>和<code>c.Tracker.Progress</code>的副本，后续的动作都是在副本上执行；</li>
<li>如果当前已经处于联合共识阶段，则返回<code>&quot;config is already joint&quot;</code>错误；</li>
<li>如果当前<code>incoming</code>为空，则返回<code>&quot;can&#39;t make a zero-voter config joint&quot;</code>错误；</li>
<li>复制<code>incoming</code>到<code>outgoing</code>中，即此时<code>incoming</code>与<code>outgoing</code>相同，然后调用<code>apply</code>函数，<code>apply</code>都作用在<code>incoming</code>上，<code>apply</code>之后，配置就变成了<code>Cnew,old</code>，<code>new</code>对应的是<code>incoming</code>，<code>old</code>对应的是<code>outgoing</code>；</li>
<li>最后调用<code>checkAndReturn</code>进行一致性检查，并返回副本；</li>
</ul>
<p>调用完<code>Changer.EnterJoint</code>，得到变更后的<code>tracker.Config</code>和<code>tracker.ProgressMap</code>副本后，在<code>raft.applyConfChange</code>方法中，将副本传给<code>raft.switchToConfig</code>将副本应用，至此，配置变更到了<code>Cnew,old</code>。</p>
<hr>
<h5 id="LeaveJoint"><a href="#LeaveJoint" class="headerlink" title="LeaveJoint"></a>LeaveJoint</h5><p>在<code>raft.applyConfChange</code>应用配置变更时，如果调用<code>ConfChangeV2.EnterJoint</code>返回<code>true,true</code>，第一个<code>true</code>表示可以自动的由<code>Cnew,old</code>过渡到<code>Cnew</code>，根据该值设置<code>Config.AutoLeave</code>。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Advance()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Advance(rd Ready)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> advance(rd Ready)</span><br></pre></td></tr></table></figure>

<p>外部Client处理完<code>Ready</code>中的日志条目之后，会调用<code>node.Advance</code>方法，以便通知Raft模块准备发送下一个<code>Ready</code>。<code>node.Advance</code>方法实际上就是向<code>n.advancec</code>管道中发一个<code>struct&#123;&#125;&#123;&#125;</code>，即发一个信号。</p>
<p>在<code>node.run</code>方法中，收到<code>n.advancec</code>管道中的信号之后，以刚刚发出去的<code>Ready</code>结构为参数，调用<code>RawNode.Advance</code>方法。<code>RawNode.Advance</code>方法主要就是调用<code>raft.advance</code>方法实现。</p>
<p><code>raft.advance</code>方法中，满足条件的情况下会向本地的日志中追加一条空的<code>ConfChangeV2</code>条目，以便开始<code>LeaveJoint</code>流程。具体逻辑是：</p>
<ul>
<li>首先从参数<code>Ready</code>结构中获取最新的<code>applyIndex</code>，即外部模块已经应用到状态机中的日志条目的最新的<code>applyIndex</code>: <code>newApplied</code>；</li>
<li>然后从当前<code>r.raftLog.applied</code>中得到<code>oldApplied</code>；然后用<code>newApplied</code>更新<code>r.raftLog.applied</code>字段；</li>
<li>同时满足下面几个条件的情况下，构造一个空的<code>pb.EntryConfChangeV2</code>类型的日志条目，调用<code>raft.appendEntry</code>将其追加到自己的本地日志中；并且更新<code>pendingConfIndex</code>为当前最后日志条目的Index：<ul>
<li>当前节点是<code>Leader</code>；</li>
<li>如果<code>r.prs.Config.AutoLeave</code>为True，说明使用的是联合共识的节点变更算法，且需要自动进入<code>LeaveJoint</code>阶段；</li>
<li>当前的<code>pendingconfIndex</code>处于<code>[oldApplied, newApplied]</code>范围内，说明外部模块应用日志时已经对配置变更做了应用，所以现在已经是<code>EnterJoint</code>阶段；</li>
</ul>
</li>
<li>注意，这里仅仅是将<code>pb.EntryConfChangeV2</code>日志条目追加到本地日志中，并没有立即同步到Follower上。</li>
</ul>
<p>经过<code>raft.advance</code>方法中针对配置变更的处理之后，接下来就像普通的日志追加流程一样，当<code>pb.EntryConfChangeV2</code>都同步到<code>Follower</code>中的大部分节点上之后，就可以提交，然后节点各自应用到状态机时，调用<code>raft.applyConfChange</code>应用配置变更，其中会调用<code>pb.ConfChangeV2.LeaveJoint</code>方法，判断当前是否可以由<code>Cnew,old</code>过渡到<code>Cnew</code>。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c ConfChangeV2)</span></span> LeaveJoint() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Changer)</span></span> LeaveJoint() (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p><code>ConfChangeV2.LeaveJoint</code>的逻辑很简单，就是判断当前的c是一个空的<code>pb.ConfChangeV2</code>结构，就返回<code>true</code>。</p>
<p>然后调用<code>changer.LeaveJoint</code>，进行<code>Cnew,old</code>到<code>Cnew</code>的过渡，并最终完成联合共识的配置变更流程。<code>changer.LeaveJoint</code>的逻辑如下：</p>
<ul>
<li>首先调用<code>checkAndCopy</code>方法得到<code>c.Tracker.Config</code>和<code>c.Tracker.Progress</code>的副本，后续的动作都是在副本上执行；</li>
<li>如果当前不处于<code>joint cunsensus</code>阶段，则返回<code>&quot;can&#39;t leave a non-joint config&quot;</code>错误；</li>
<li>如果<code>outgoing</code>为空，则返回<code>&quot;configuration is not joint&quot;</code>错误；</li>
<li>将<code>LearnersNext</code>中的<code>ID</code>记录到<code>Learners</code>中；然后将<code>LearnersNext</code>置为<code>nil</code>；</li>
<li>轮训<code>outgoing</code>中的<code>ID</code>，如果其既不属于<code>incoming</code>，也不属于<code>Learners</code>，则说明该节点被删除了，因此将该节点对应的<code>Progress</code>结构从<code>prs</code>中清除；</li>
<li>最后，将<code>outgoing</code>清除，将<code>AutoLeave</code>置为<code>false</code>；并调用<code>checkAndReturn</code>进行一致性检查，并返回副本；</li>
</ul>
<p>调用完<code>LeaveJoint</code>，得到变更后的<code>tracker.Config</code>和<code>tracker.ProgressMap</code>副本后，在<code>raft.applyConfChange</code>方法中，将副本传给<code>raft.switchToConfig</code>将副本应用，至此，配置变更到了<code>Cnew</code>。</p>
<hr>
<h4 id="配置变更信息的保存和使用"><a href="#配置变更信息的保存和使用" class="headerlink" title="配置变更信息的保存和使用"></a>配置变更信息的保存和使用</h4><p>当外部Client调用<code>node.ApplyConfChange</code>得到最新的集群配置状态<code>pb.ConfState</code>后，在创建<code>snapshot</code>时，会将其保存到<code>snapshot.Metadata.ConfState</code>中。<code>pb.ConfState</code>定义如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> ConfState <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">// The voters in the incoming config. (If the configuration is not joint,</span></span><br><span class="line">	<span class="comment">// then the outgoing config is empty).</span></span><br><span class="line">	Voters []<span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,1,rep,name=voters&quot; json:&quot;voters,omitempty&quot;`</span></span><br><span class="line">	<span class="comment">// The learners in the incoming config.</span></span><br><span class="line">	Learners []<span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,2,rep,name=learners&quot; json:&quot;learners,omitempty&quot;`</span></span><br><span class="line">	<span class="comment">// The voters in the outgoing config.</span></span><br><span class="line">	VotersOutgoing []<span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,3,rep,name=voters_outgoing,json=votersOutgoing&quot; json:&quot;voters_outgoing,omitempty&quot;`</span></span><br><span class="line">	<span class="comment">// The nodes that will become learners when the outgoing config is removed.</span></span><br><span class="line">	<span class="comment">// These nodes are necessarily currently in nodes_joint (or they would have</span></span><br><span class="line">	<span class="comment">// been added to the incoming config right away).</span></span><br><span class="line">	LearnersNext []<span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,4,rep,name=learners_next,json=learnersNext&quot; json:&quot;learners_next,omitempty&quot;`</span></span><br><span class="line">	<span class="comment">// If set, the config is joint and Raft will automatically transition into</span></span><br><span class="line">	<span class="comment">// the final config (i.e. remove the outgoing config) when this is safe.</span></span><br><span class="line">	AutoLeave <span class="type">bool</span> <span class="string">`protobuf:&quot;varint,5,opt,name=auto_leave,json=autoLeave&quot; json:&quot;auto_leave&quot;`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当节点崩溃重启时，或者<code>Follower</code>收到<code>Leader</code>发来的<code>snapshot</code>时，会调用<code>raft.restore</code>恢复状态，其中就需要恢复集群配置。这部分主要在<code>restore.go</code>中实现。两者的恢复原理是一样的，下面以节点崩溃恢复为例。</p>
<p>节点崩溃时，节点配置既有可能处于稳定态<code>Cnew</code>，也有可能处于联合共识过程中的<code>Cnew,old</code>状态。<code>Cnew,old</code>可以兼容稳定态<code>Cnew</code>。比如<code>NVS</code>中保存的配置信息是：<code>voters=(1 2 3) learners=(5) outgoing=(1 2 4 6) learners_next=(4)</code>，这就说明崩溃时处于<code>Cnew,old</code>状态，<code>Cold</code>是<code>(1 2 4 6)</code>，<code>Cnew</code>是<code>(1 2 3)</code>，<code>learners_next</code>是<code>(4)</code>，即<code>Cnew</code>是要从<code>Cold</code>中删除节点<code>6</code>，并将节点<code>4</code>降级为<code>Learner</code>，至于节点<code>5</code>，它有可能在配置变更前就是<code>Learner</code>，也有可能是配置变更时新增的新<code>Learner</code>，不过这不重要。</p>
<p>针对上面的例子，节点重启后，当前状态为空，<font color=red>要恢复到节点崩溃时的状态，实际上就是根据<code>snapshot.Metadata.ConfState</code>，执行一系列的操作，恢复出<code>raft.ProgressTracker</code>的过程。</font>对于<code>outgoing</code>，要执行的动作序列是：<code>add 1; add 2; add 4; add 6</code>，针对voters，要执行的动作序列是：<code>remove 1; remove 2; remove 4; remove 6;  add 1; add 2; add 3; add-learner 5; add-learner 4</code>。</p>
<h5 id="toConfChangeSingle"><a href="#toConfChangeSingle" class="headerlink" title="toConfChangeSingle"></a>toConfChangeSingle</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">toConfChangeSingle</span><span class="params">(cs pb.ConfState)</span></span> (out []pb.ConfChangeSingle, in []pb.ConfChangeSingle)</span><br></pre></td></tr></table></figure>

<p><code>toConfChangeSingle</code>函数，就是针对<code>NVS</code>中保存的配置信息<code>pb.ConfState</code>，生成一系列的配置变更消息，确切的说是针对<code>incoming</code>和<code>outgoing</code>，生成两个<code>消息Slice</code>。它的逻辑如下：</p>
<ul>
<li>轮训<code>cs.VotersOutgoing</code>，针对其中的每个<code>节点ID</code>，将<code>pb.ConfChangeAddNode</code>消息添加到<code>out</code>中；</li>
<li>轮训<code>cs.VotersOutgoing</code>，针对其中的每个<code>节点ID</code>，将<code>pb.ConfChangeRemoveNode</code>消息添加到<code>in</code>中；</li>
<li>轮训<code>cs.Voters</code>，针对其中的每个<code>节点ID</code>，将<code>pb.ConfChangeAddNode</code>消息添加到<code>in</code>中；</li>
<li>轮训<code>cs.Learners</code>，针对其中的每个<code>节点ID</code>，将<code>pb.ConfChangeAddLearnerNode</code>消息添加到<code>in</code>中；</li>
<li>轮训<code>cs.LearnersNext</code>，针对其中的每个<code>节点ID</code>，将<code>pb.ConfChangeAddLearnerNode</code>消息添加到<code>in</code>中；</li>
</ul>
<hr>
<h5 id="Restore"><a href="#Restore" class="headerlink" title="Restore"></a>Restore</h5><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Restore</span><span class="params">(chg Changer, cs pb.ConfState)</span></span> (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">chain</span><span class="params">(chg Changer, ops ...<span class="keyword">func</span>(Changer)</span></span> (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)) (tracker.Config, tracker.ProgressMap, <span class="type">error</span>)</span><br></pre></td></tr></table></figure>

<p><code>Restore</code>函数就是恢复节点配置的函数，它会针对<code>toConfChangeSingle</code>生成的每一条消息，执行<code>Simple</code>或者<code>EnterJoint</code>配置变更。具体逻辑如下：</p>
<ul>
<li>调用<code>toConfChangeSingle(cs)</code>，生成<code>outgoing</code>和<code>incoming</code>消息Slice；</li>
<li>如果<code>outgoing</code>为空，说明崩溃时不是在配置变更，因此，直接针对<code>incoming</code>中的每条消息，执行<code>Simple</code>动作即可；</li>
<li>否则，说明崩溃时正处于<code>Cnew,old</code>状态。因此，需要首先针对<code>outgoing</code>的每条消息，执行<code>Simple</code>动作，恢复到进入配置变更之前的状态，然后针对<code>incoming</code>消息Slice，直接执行<code>EnterJoint</code>恢复到配置变更状态即可；</li>
</ul>
<hr>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>问：<code>etcd/raft</code>的实现中，节点什么时候使用新配置，即按照新配来决定<code>Leader</code>选举和日志追加？</p>
<p>答：不管是<code>Leader</code>选举，还是日志追加，都是<font color=red>以<code>raft.ProgressTracker</code>中的配置来进行判断的</font>。根据实现，变更日志应用之后，经过<code>Changer</code>的<code>Simple/EnterJoint/LeaveJoint</code>的修改，以及<code>raft.switchToConfig</code>的应用之后，<code>raft.ProgressTracker</code>才得以修改（具体就是<code>raft.ProgressTracker</code>中的<code>tracker.Config</code>和<code>tracker.ProgressMap</code>）。</p>
<p>问：<code>etcd/raft</code>的实现中，是否会有脑裂的情况，即会不会造成集群中出现不相交的Majority集合？</p>
<p>答：先考虑一次配置变更，不考虑多个配置变更的场景。不管是单节点配置变更，还是联合共识算法，它们的本质都是保证一次变更前后的两种配置，它们的Majority集合肯定有交集：</p>
<ul>
<li>单节点变更：变更由<code>Cold</code>过渡到<code>Cnew</code>一次性完成，而单节点变更本身的约束，保证了<code>Cold</code>和<code>Cnew</code>之间肯定有交集。</li>
<li>联合共识算法：变更过程分两步完成，第一步是<code>Cold</code>到<code>Cnew,old</code>，第二步是<code>Cnew,old</code>到<code>Cnew</code>。不管哪一步的变更，它们的新旧配置的Majority也肯定有交集。</li>
</ul>
<p>所以，只看一次变更的场景，不会出现脑裂的情况，因为<font color=red>相邻的两个配置，即变更前后的配置的Majority集合肯定有交集。</font></p>
<p>现在考虑多次变更的场景，一旦涉及多次变更，则集群中的配置可能出现不相邻的情况，即集群中一部分节点采用<code>Cold</code>，一部分节点采用<code>Cnew</code>，还有采用<code>Cnew&#39;</code>的，那采用<code>Cold</code>和<code>Cnew&#39;</code>的节点间一旦出现分区，就有可能出现脑裂的情况。</p>
<p>首先看一下<code>etcd/raft</code>为保证多次变更不出现脑裂，做了哪些约束：</p>
<ol>
<li>在<code>stepLeader</code>函数中，处理<code>pb.MsgProp</code>提案消息时，判断提案是配置变更提案后，会把<code>raft.pendingConfIndex</code>置为该提案的Index，后续再收到配置变更的提案时，只要之前的<code>raft.pendingConfIndex</code>还小于<code>raft.raftLog.applied</code>，说明上一个配置变更尚未应用，则直接拒绝本次的配置变更；在<code>raft.advance</code>方法中，追加驱动<code>LeaveJoint</code>的<code>pb.EntryConfChangeV2</code>的条目时也是这样的处理；</li>
<li><code>Candidate</code>节点竞选成功变成<code>Leader</code>之后，在<code>raft.becomeLeader</code>方法中，直接将<code>raft.pendingConfIndex</code>设置为当前日志的最后条目的Index，即最后条目应用之前，都不会接收新的配置变更提案。这样保证了之前如果有配置变更条目未应用的话，不会开始新的配置变更；</li>
<li>在<code>raft.hup</code>方法中，通过调用<code>numOfPendingConf</code>，判断如果当前本节点的已提交未应用的日志条目中，如果有配置变更条目的话，则无法参加选举成为新<code>Leader</code>；</li>
</ol>
<p>有了上面的约束后，可以保证不会出现多次配置变更造成脑裂的情况。考虑集群配置<code>abc-&gt;abcd-&gt;abcde</code>的变更场景：</p>
<ul>
<li><code>abc</code>三节点组成的集群，<code>c</code>是<code>Leader</code>；即此时<code>Cold=&#123;abc&#125;</code>；</li>
<li><code>c</code>收到配置变更提案，需要增加<code>d</code>，即<code>Cnew=&#123;abcd&#125;</code>；</li>
<li><code>Cnew</code>提交之前，所有节点（<code>abc</code>）都是采用的<code>Cold</code>，不会出现脑裂；</li>
<li><code>Cnew</code>在<code>c</code>上提交后，未应用之前，所有节点采用的都是<code>Cold</code>，不会出现脑裂；</li>
<li><code>Cnew</code>在一些节点上提交了，则集群中一些节点采用的是<code>Cnew</code>，另外一些采用的是<code>Cold</code>，因<code>Cnew</code>和<code>Cold</code>是连续的，肯定有交集，所以不会出现脑裂；</li>
<li>现在<code>c</code>收到了新的配置变更提案，需要增加<code>e</code>，即<code>Cnew&#39;=&#123;abcde&#125;</code>；现在<code>c</code>需要将<code>Cnew&#39;</code>这条日志复制到<code>Cnew</code>的Majority节点上；</li>
<li><code>Cnew&#39;</code>在提交之前，所有节点采用的是<code>Cold</code>或<code>Cnew</code>，不会出现脑裂；</li>
<li><font color=red><code>Cnew&#39;</code>得以提交，说明<code>Cnew&#39;</code>这条日志，至少复制到了<code>Cnew</code>的Majority节点上，这些收到了<code>Cnew&#39;</code>条目的节点，他们的<code>commitIndex</code>肯定也得到了了更新（<code>pb.MsgApp</code>中附带了<code>commitIndex</code>），因而这些节点上肯定都提交了<code>Cnew</code>；</font>因<code>Cnew=&#123;abcd&#125;</code>，假设<code>bcd</code>是这个Majority集合；</li>
<li><code>Cnew&#39;</code>提交之后，在一些节点上应用了，所以目前集群中一些节点采用的<code>Cnew</code>，一些节点采用的是<code>Cnew&#39;</code>，还有一些节点采用的是<code>Cold</code>；</li>
<li>对于采用<code>Cold</code>配置的节点，说明它未应用<code>Cnew</code>，甚至可能未提交<code>Cnew</code>；其中未提交<code>Cnew</code>的节点，它肯定没有收到<code>Cnew&#39;</code>，没有收到<code>Cnew&#39;</code>的节点，即使是在<code>Cold</code>的配置中肯定也无法形成Majority，它的日志不够新，所以它肯定当不了<code>Leader</code>；而对于已提交未应用<code>Cnew</code>的节点，因为上面的第<code>3</code>条约束，它也无法当选<code>Leader</code>；</li>
<li>所以，集群中只有<code>Cnew</code>和<code>Cnew&#39;</code>的节点才能竞选成为<code>Leader</code>，因为<code>Cnew</code>和<code>Cnew&#39;</code>是连续的，Majority有交集的，所以不会出现脑裂。</li>
</ul>
<hr>
<h3 id="其他实现细节"><a href="#其他实现细节" class="headerlink" title="其他实现细节"></a>其他实现细节</h3><h4 id="becomeLeader时设置pendingConfIndex"><a href="#becomeLeader时设置pendingConfIndex" class="headerlink" title="becomeLeader时设置pendingConfIndex"></a><code>becomeLeader</code>时设置<code>pendingConfIndex</code></h4><p><code>becomeLeader</code>中，有下面的逻辑：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Conservatively set the pendingConfIndex to the last index in the</span></span><br><span class="line"><span class="comment">// log. There may or may not be a pending config change, but it&#x27;s</span></span><br><span class="line"><span class="comment">// safe to delay any future proposals until we commit all our</span></span><br><span class="line"><span class="comment">// pending log entries, and scanning the entire tail of the log</span></span><br><span class="line"><span class="comment">// could be expensive.</span></span><br><span class="line">r.pendingConfIndex = r.raftLog.lastIndex()</span><br></pre></td></tr></table></figure>

<p>这里直接将<code>raft.pendingConfIndex</code>设置到当前最后日志条目的最后Index，目的就是为了防止多个配置变更同时进行。这种逻辑最早可以追溯到<code>2014</code>年的<code>commit</code>：<code>d293c4915</code>，以及后续的<code>ff6705b94b</code>，当时的实现方式是轮训当前本地日志中的所有条目，发现有配置变更类型的日志，就将一个<code>bool</code>类型的<code>pendingConf</code>变量置为<code>true</code>。</p>
<p>到了<code>2017</code>年的<code>commit</code>：<code>8d8f3195e</code>，以及后续的<code>20422c5b4</code>中，才将<code>pendingConf</code>改成了现在的<code>pendingConfIndex</code>，并且在<code>becomeLeader</code>中直接将其设置为最后日志条目的索引，目的是为了节省轮训日志的时间。根据注释，作者发现新<code>Leader</code>花费太多时间在轮训日志上，导致没有及时的发送心跳消息，从而导致了新一轮的选举。<code>8d8f3195e</code>的<code>commit</code>注释如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Scanning the uncommitted portion of the raft log to determine whether there are any pending config changes can be expensive. In cockroachdb/cockroach#18601, we&#x27;ve seen that a new leader can spend so</span><br><span class="line">much time scanning its log post-election that it fails to send its first heartbeats in time to prevent a second election from starting immediately.</span><br><span class="line"></span><br><span class="line">Instead of tracking whether a pending config change exists with a boolean, this commit tracks the latest log index at which a pending config change *could* exist. This is a less expensive solution to</span><br><span class="line">the problem, and the impact of false positives should be minimal since a newly-elected leader should be able to quickly commit the tail ofits log.</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Follower竞选拉票前判断是否有已commit未apply的配置变更日志条目"><a href="#Follower竞选拉票前判断是否有已commit未apply的配置变更日志条目" class="headerlink" title="Follower竞选拉票前判断是否有已commit未apply的配置变更日志条目"></a><code>Follower</code>竞选拉票前判断是否有已commit未apply的配置变更日志条目</h4><p>在<code>raft.hub</code>方法中，有下面的逻辑：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">ents, err := r.raftLog.slice(r.raftLog.applied+<span class="number">1</span>, r.raftLog.committed+<span class="number">1</span>, noLimit)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">	r.logger.Panicf(<span class="string">&quot;unexpected error getting unapplied entries (%v)&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> n := numOfPendingConf(ents); n != <span class="number">0</span> &amp;&amp; r.raftLog.committed &gt; r.raftLog.applied &#123;</span><br><span class="line">	r.logger.Warningf(<span class="string">&quot;%x cannot campaign at term %d since there are still %d pending configuration changes to apply&quot;</span>, r.id, r.Term, n)</span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>即如果当前<code>Follower</code>中，存在已经commit，但是尚未apply的配置变更日志，则该<code>Follower</code>无法成为<code>Leader</code>。这是在<code>d0e4fe56a5dbe0d</code>中的<code>commit</code>。</p>
<p>这么做的原因参考“思考”一节。</p>
<hr>
<h4 id="重要的commit"><a href="#重要的commit" class="headerlink" title="重要的commit"></a>重要的commit</h4><h5 id="配置变更功能的第一次提交2014-06-06：c03fbf68d6"><a href="#配置变更功能的第一次提交2014-06-06：c03fbf68d6" class="headerlink" title="配置变更功能的第一次提交2014-06-06：c03fbf68d6"></a>配置变更功能的第一次提交2014-06-06：c03fbf68d6</h5><pre><code>raft: add conf safety

To make configuration change safe without adding configuration protocol:

1. We only allow to add/remove one node at a time.

2. We only allow one uncommitted configuration entry in the log.

These two rules can make sure there is no disjoint quorums in both current cluster and the
future(after applied any number of committed entries or uncommitted entries in log) clusters.

We add a type field in Entry structure for two reasons:

1. Statemachine needs to know if there is a pending configuration change.

2. Configuration entry should be executed by raft package rather application who is using raft.
</code></pre>
<h5 id="在becomeLeader中，避免扫描日志2017-12-30：8d8f3195e；2018-06-27：20422c5b4"><a href="#在becomeLeader中，避免扫描日志2017-12-30：8d8f3195e；2018-06-27：20422c5b4" class="headerlink" title="在becomeLeader中，避免扫描日志2017-12-30：8d8f3195e；2018-06-27：20422c5b4"></a>在becomeLeader中，避免扫描日志2017-12-30：8d8f3195e；2018-06-27：<code>20422c5b4</code></h5><h5 id="支持联合共识算法2019-07-16：aa158f36b"><a href="#支持联合共识算法2019-07-16：aa158f36b" class="headerlink" title="支持联合共识算法2019-07-16：aa158f36b"></a>支持联合共识算法2019-07-16：aa158f36b</h5><pre><code>raft: internally support joint consensus

This commit introduces machinery to safely apply joint consensus configuration changes to Raft.

The main contribution is the new package, `confchange`, which offers the primitives `Simple`, `EnterJoint`, and `LeaveJoint`.

The first two take a list of configuration changes. `Simple` only declares success if these configuration changes (applied atomically) change the set of voters by at most one (i.e. it&#39;s fine to add or remove any number of learners, but change only one voter). `EnterJoint` makes the configuration joint and then applies the changes to it, in preparation of the caller returning later and transitioning out of the joint config into the final desired configuration via `LeaveJoint()`.

This commit streamlines the conversion between voters and learners, which is now generally allowed whenever the above conditions are upheld (i.e. it&#39;s not possible to demote a voter and add a new voter in the context of a Simple configuration change, but it is possible via EnterJoint). Previously, we had the artificial restriction that a voter could not be demoted to a learner, but had to be removed first. Even though demoting a learner is generally less useful than promoting a learner (the latter is used to catch up future voters), demotions could see use in improved handling of temporary node unavailability, where it is desired to remove voting power from a down node, but to
preserve its data should it return.

An additional change that was made in this commit is to prevent the use of empty commit quorums, which was previously possible but for no good reason; this: Closes #10884.

The work left to do in a future PR is to actually expose joint configurations to the applications using Raft. This will entail mostly API design and the addition of suitable testing, which to be carried out ergonomically is likely to motivate a larger refactor.

Touches #7625.
</code></pre>
<h5 id="支持pb-EntryConfChangeV2接口2019-07-23：b67303c6a240708"><a href="#支持pb-EntryConfChangeV2接口2019-07-23：b67303c6a240708" class="headerlink" title="支持pb.EntryConfChangeV2接口2019-07-23：b67303c6a240708"></a>支持pb.EntryConfChangeV2接口2019-07-23：b67303c6a240708</h5><pre><code>raft: allow use of joint quorums

This change introduces joint quorums by changing the Node and RawNode API to accept pb.ConfChangeV2 (on top of pb.ConfChange).

pb.ConfChange continues to work as today: it allows carrying out a single configuration change. A pb.ConfChange proposal gets added to the Raft log as such and is thus also observed by the app during Ready handling, and fed back to ApplyConfChange.

ConfChangeV2 allows joint configuration changes but will continue to carry out configuration changes in &quot;one phase&quot; (i.e. without ever entering a joint config) when this is possible.
</code></pre>
<h5 id="竞选拉票前检查是否有未应用的配置变更2020-07-23：d0e4fe56a5dbe0d"><a href="#竞选拉票前检查是否有未应用的配置变更2020-07-23：d0e4fe56a5dbe0d" class="headerlink" title="竞选拉票前检查是否有未应用的配置变更2020-07-23：d0e4fe56a5dbe0d"></a>竞选拉票前检查是否有未应用的配置变更2020-07-23：d0e4fe56a5dbe0d</h5><pre><code> raft: check pending conf change before campaign (#12134)

* raft: check conf change before campaign

Signed-off-by: Jay Lee &lt;BusyJayLee@gmail.com&gt;

* raft: extract hup function

Signed-off-by: Jay Lee &lt;BusyJayLee@gmail.com&gt;

* raft: check pending conf change for transferleader

Signed-off-by: Jay Lee &lt;BusyJayLee@gmail.com&gt;
</code></pre>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>etcd_raft源码解析-15Ready的处理</title>
    <url>/2022/05/16/%E5%88%86%E5%B8%83%E5%BC%8F/raft/etcd_raft%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/15Ready%E7%9A%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p><code>raft状态机</code>以消息为输入，状态机运转之后产生输出，输出以<code>Ready</code>结构的形式传递给外部模块。</p>
<p><code>raft状态机</code>的输出全都包含在<code>Ready</code>结构中，比如需要保存到<code>NVS</code>中的日志条目、Term、快照，以及需要向外发送的各种消息等等。</p>
<span id="more"></span>
<h3 id="Ready结构"><a href="#Ready结构" class="headerlink" title="Ready结构"></a>Ready结构</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">type</span> SoftState <span class="keyword">struct</span> &#123;</span><br><span class="line">	Lead      <span class="type">uint64</span> <span class="comment">// must use atomic operations to access; keep 64-bit aligned.</span></span><br><span class="line">	RaftState StateType</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HardState <span class="keyword">struct</span> &#123;</span><br><span class="line">	Term   <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,1,opt,name=term&quot; json:&quot;term&quot;`</span></span><br><span class="line">	Vote   <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,2,opt,name=vote&quot; json:&quot;vote&quot;`</span></span><br><span class="line">	Commit <span class="type">uint64</span> <span class="string">`protobuf:&quot;varint,3,opt,name=commit&quot; json:&quot;commit&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> ReadState <span class="keyword">struct</span> &#123;</span><br><span class="line">	Index      <span class="type">uint64</span></span><br><span class="line">	RequestCtx []<span class="type">byte</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Ready <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">//包含集群Leader的id，以及当前节点的状态；只有发生变化时才有具体的值，否则为nil</span></span><br><span class="line">	*SoftState</span><br><span class="line"></span><br><span class="line">	<span class="comment">//在发送响应到CLIENT之前必须保存到NVS中的状态，包括节点的Term，Vote，以及Committed Index;</span></span><br><span class="line">	<span class="comment">//如果没有变化，则是空的状态；</span></span><br><span class="line">	pb.HardState</span><br><span class="line"></span><br><span class="line">	<span class="comment">//ReadStates用于处理读请求，其中包含了收到读请求时最新的committed Index，以及外部模块用于</span></span><br><span class="line">    <span class="comment">//区分读请求的上下文RequestCtx；</span></span><br><span class="line">	ReadStates []ReadState</span><br><span class="line"></span><br><span class="line">	<span class="comment">//在发送响应给CLIENT之前，需要保存到NVS中的日志条目</span></span><br><span class="line">	Entries []pb.Entry</span><br><span class="line"></span><br><span class="line">	<span class="comment">//需要保存到NVS中的snapshot；</span></span><br><span class="line">	Snapshot pb.Snapshot</span><br><span class="line"></span><br><span class="line">	<span class="comment">//可用于提交给状态机的日志条目；再次之前他们肯定已经保存到NVS中了；</span></span><br><span class="line">	CommittedEntries []pb.Entry</span><br><span class="line"></span><br><span class="line">	<span class="comment">//在日志条目保存到NVS中之后，需要发送给外部的消息</span></span><br><span class="line">	Messages []pb.Message</span><br><span class="line"></span><br><span class="line">	<span class="comment">//为true表示HardState和Entries必须同步写入磁盘，否则则允许异步写入；</span></span><br><span class="line">	MustSync <span class="type">bool</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Ready的处理流程"><a href="#Ready的处理流程" class="headerlink" title="Ready的处理流程"></a>Ready的处理流程</h3><h4 id="发送Ready"><a href="#发送Ready" class="headerlink" title="发送Ready"></a>发送Ready</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> HasReady() <span class="type">bool</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> readyWithoutAccept() Ready</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newReady</span><span class="params">(r *raft, prevSoftSt *SoftState, prevHardSt pb.HardState)</span></span> Ready</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> acceptReady(rd Ready)</span><br></pre></td></tr></table></figure>

<p>在<code>node.run</code>这个Goroutine中，每次循环的开始，都会调用<code>RawNode.HasReady</code>查看是否有新的<code>Ready</code>结构待发送。在此之前，<code>readyc</code>和<code>advancec</code>这两个Channel都是<code>nil</code>。<code>RawNode.HasReady</code>方法的逻辑很简单，就是判断是否可以由包含到<code>Ready</code>中的新内容。下面的条件满足其一就返回<code>true</code>，否则返回<code>false</code>：</p>
<ul>
<li>调用<code>raft.softState</code>返回的<code>softState</code>不同于之前的<code>RawNode.prevSoftSt</code>；</li>
<li>调用<code>raft.hardState</code>返回的<code>hardState</code>不同于之前的<code>RawNode.prevHardSt</code>；</li>
<li>调用<code>raftLog.hasPendingSnapshot</code>返回<code>true</code>，即表示<code>raftLog.unstable.snapshot</code>不为<code>nil</code>；</li>
<li><code>raft.msgs</code>不为空；</li>
<li><code>raft.raftLog.unstableEntries</code>不为空；</li>
<li><code>raft.raftLog.hasNextEnts</code>返回<code>true</code>，即有待应用的日志；</li>
<li><code>raft.readStates</code>不为空；</li>
</ul>
<p>当调用<code>RawNode.HasReady</code>返回<code>true</code>后，说明可以有新的<code>Ready</code>结构了，所以调用<code>newReady</code>函数获取新的<code>Ready</code>结构，然后设置<code>readyc</code>为<code>node.readyc</code>，并将<code>Ready</code>结构通过该管道发送给外部模块。<code>newReady</code>函数就是构造新的<code>Ready</code>结构，其中：</p>
<ul>
<li>设置<code>Entries</code>为<code>raft.raftLog.unstableEntries</code>;</li>
<li>设置<code>CommittedEntries</code>为<code>raft.raftLog.nextEnts</code>，即已commit但尚未apply的日志条目；</li>
<li>设置<code>Messages</code>为<code>r.msgs</code>；</li>
<li>设置<code>SoftState</code>为<code>raft.softState</code>的返回值；</li>
<li>设置<code>HardState</code>；</li>
<li>设置<code>Snapshot</code>为<code>raft.raftLog.unstable.snapshot</code>；</li>
<li>设置<code>ReadStates</code>为<code>raft.readStates</code>；</li>
<li>如果<code>Entries</code>中有新内容，或者<code>hardState</code>发生了变化，则<code>MustSync</code>为<code>true</code>，即必须要<code>Entries</code>和<code>hardState</code>保存到<code>NVS</code>中之后才能继续处理；</li>
</ul>
<p>外部模块收到之后，在<code>node.run</code>中，调用<code>RawNode.acceptReady</code>方法重置一些状态：</p>
<ul>
<li>设置<code>RawNode.prevSoftSt</code>；</li>
<li>设置<code>RawNode.raft.readStates</code>为<code>nil</code>；</li>
<li>设置<code>RawNode.raft.msgs</code>为nil；</li>
</ul>
<p>然后设置<code>advancec</code>为<code>node.advancec</code>，并等待该管道上的信号。</p>
<hr>
<h4 id="处理Ready"><a href="#处理Ready" class="headerlink" title="处理Ready"></a>处理Ready</h4><p>外部应用收到<code>Ready</code>结构后进行处理，以<code>raftexample</code>为例：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> rd := &lt;-rc.node.Ready():</span><br><span class="line">	rc.wal.Save(rd.HardState, rd.Entries)</span><br><span class="line">	<span class="keyword">if</span> !raft.IsEmptySnap(rd.Snapshot) &#123;</span><br><span class="line">		rc.saveSnap(rd.Snapshot)</span><br><span class="line">		rc.raftStorage.ApplySnapshot(rd.Snapshot)</span><br><span class="line">		rc.publishSnapshot(rd.Snapshot)</span><br><span class="line">	&#125;</span><br><span class="line">	rc.raftStorage.Append(rd.Entries)</span><br><span class="line">	rc.transport.Send(rd.Messages)</span><br><span class="line">	applyDoneC, ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries))</span><br><span class="line">	<span class="keyword">if</span> !ok &#123;</span><br><span class="line">		rc.stop()</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	rc.maybeTriggerSnapshot(applyDoneC)</span><br><span class="line">	rc.node.Advance()</span><br></pre></td></tr></table></figure>

<p>在该Goroutine中，从<code>rc.node.Ready()</code>返回的Channel中阻塞接收<code>Ready</code>结构，收到新的<code>Ready</code>结构后：</p>
<ul>
<li>调用<code>rc.wal.Save(rd.HardState, rd.Entries)</code>，把需要保存到<code>NVS</code>中的<code>HardState</code>、日志条目进行保存；</li>
<li>如果<code>Ready</code>中包含了<code>Snapshot</code>，则首先调用<code>rc.saveSnap(rd.Snapshot)</code>，把<code>snapshot</code>保存到<code>NVS</code>中；然后调用<code>rc.raftStorage.ApplySnapshot(rd.Snapshot)</code>，替换<code>Storage</code>中的<code>snapshot</code>，并清空其中的日志；最后调用<code>rc.publishSnapshot</code>应用该snapshot；</li>
<li>调用<code>rc.raftStorage.Append(rd.Entries)</code>，将日志条目追加到<code>Storage</code>中；</li>
<li>调用<code>rc.transport.Send(rd.Messages)</code>，将<code>Ready</code>中的<code>Messages</code>中发送出去；</li>
<li>调用<code>rc.publishEntries(rc.entriesToApply(rd.CommittedEntries))</code>，将可应用的日志条目应用到状态机，并更新<code>applyIndex</code>；如果其中包含了配置变更的日志条目，则进行响应的配置变更；</li>
<li>调用<code>rc.maybeTriggerSnapshot(applyDoneC)</code>，满足条件的情况下创建snapshot；</li>
<li>调用<code>rc.node.Advance()</code>，通知<code>raft模块</code>进行下一步动作；</li>
</ul>
<h4 id="raft模块进行下一步动作"><a href="#raft模块进行下一步动作" class="headerlink" title="raft模块进行下一步动作"></a>raft模块进行下一步动作</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> Advance()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *node)</span></span> run()</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rn *RawNode)</span></span> Advance(rd Ready)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> advance(rd Ready)</span><br></pre></td></tr></table></figure>

<p>外部模块使用完<code>Ready</code>后，会调用<code>node.Advance</code>方法通知<code>raft模块</code>，就是向<code>node.advance</code>管道中发一个信号；</p>
<p>在<code>node.run</code>这个<code>Goroutine</code>中，接收到<code>node.advance</code>上的信号之后，就调用<code>RawNode.Advance</code>方法，保存<code>Ready.HardState</code>为<code>RawNode.prevHardSt</code>；并调用<code>raft.advance</code>完成最后的动作，主要就是完成一些状态的更改，包括：</p>
<ul>
<li>调用<code>r.reduceUncommittedSize(rd.CommittedEntries)</code>，参考下一篇文章；</li>
<li>如果<code>Ready</code>的待应用日志中包含了配置变更条目的话，则发送一个空的<code>pb.EntryConfChangeV2</code>消息，驱动配置变更完成第二阶段的动作；</li>
<li>调用<code>raftLog.stableTo</code>，清除<code>unstable</code>中的相应日志条目；</li>
<li>调用<code>raftLog.stableSnapto</code>，清除<code>unstable</code>中的snapshot；</li>
</ul>
<hr>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="send"><a href="#send" class="headerlink" title="send"></a>send</h4><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *raft)</span></span> send(m pb.Message)</span><br></pre></td></tr></table></figure>

<p><code>raft.send</code>方法是<code>raft模块</code>中最底层的”消息发送“方法，其他如<code>sendAppend</code>、<code>sendHeartbeat</code>等最终都会调用到该方法。但是实际上该方法并没有真正的发送动作，它仅仅是把消息追加到<code>r.msgs</code>中，这些<code>msgs</code>中的消息最终会附加到<code>Ready</code>结构中，发送到<code>raft</code>模块外部。</p>
<p>该方法在将消息追加到<code>r.msgs</code>之前，会检查并设置消息中的<code>Term</code>等属性。具体的逻辑是：</p>
<ul>
<li>如果<code>m.From</code>尚未设置，则将其置为<code>r.id</code>，即本节点的<code>id</code>；</li>
<li>对于竞选消息，包括：<code>pb.MsgVote</code>（拉票消息）、<code>pb.MsgVoteResp</code>（拉票回应）、<code>pb.MsgPreVote</code>（预拉票消息）、<code>pb.MsgPreVoteResp</code>（预拉票回应），这些消息的<code>Term</code>值，在调用<code>send</code>之前必须已经设置好了，否则直接<code>panic</code>；</li>
<li>其他类型的消息，在调用<code>send</code>之前不能设置，否则直接<code>panic</code>；<ul>
<li>如果消息既不是<code>pb.MsgProp</code>提案消息，也不是<code>pb.MsgReadIndex</code>消息，则设置其<code>Term</code>为当前<code>Term</code>；</li>
<li>而对于<code>pb.MsgProp</code>提案消息和<code>pb.MsgReadIndex</code>消息，他们实际上都是要转发给<code>Leader</code>的消息，所以按照本地消息的方式对待，无需设置其<code>Term</code>。</li>
</ul>
</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>etcd_raft源码解析</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-01引言</title>
    <url>/2021/10/05/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/11%E5%BC%95%E8%A8%80/</url>
    <content><![CDATA[<h2 id="Chapter-1-Introduction-引言"><a href="#Chapter-1-Introduction-引言" class="headerlink" title="Chapter 1 Introduction     引言"></a>Chapter 1 Introduction     引言</h2><p>当前的数据中心系统和应用程序运行在高度动态的环境中。它们使用额外的服务器资源进行横向扩展，并根据需要进行扩展和收缩。服务器和网络的故障也很常见：每年约有2–4%的磁盘驱动器发生故障[103]，服务器也经常崩溃[22]，每天都有数十条网络链路发生问题[31]。</p>
<p>Today’s datacenter systems and applications run in highly dynamic environments. They scale out by leveraging the resources of additional servers, and they grow and shrink according to demand. Server and network failures are also commonplace: about 2–4% of disk drives fail each year [103], servers crash about as often [22], and tens of network links fail every day in modern datacenters [31].</p>
<span id="more"></span>
<p>因此，系统正常运行期间必须能够处理节点的上线和下线。他们必须对变化做出反应，并在几秒内自动适应；系统的不可用对用户而言是不可接受的。这是当今系统的一大挑战；在这种环境中，故障处理、协调、服务发现和配置管理都很难处理。</p>
<p>As a result, systems must deal with servers coming and going during normal operations. They must react to changes and adapt automatically within seconds; outages that are noticeable to humans are typically not acceptable. This is a major challenge in today’s systems; failure handling, coordination, service discovery, and configuration management are all difficult in such dynamic environments.</p>
<p>好在分布式共识算法可以应对这些挑战。共识算法使得多个机器像一个协同的群体一样工作，允许若干机器的失败。在一个共识系统中，故障使用一种有原则的、行之有效的方式来处理的。由于共识系统是高度可用和可靠的，所以<font color=red>其他系统组件可以使用共识系统作为容错的基础组件</font>，所以，共识系统在构建可靠的大规模软件系统中起着关键作用。</p>
<p>Fortunately, distributed consensus can help with these challenges. Consensus allows a collection of machines to work as a coherent group that can survive the failures of some of its members. Within a consensus group, failures are handled in a principled and proven way. Because consensus groups are highly available and reliable, other system components can use a consensus group as the foundation for their own fault tolerance. Thus, consensus plays a key role in building reliable large-scale software systems.</p>
<p>当我们开始研究Raft时，共识的必要性越来越明显，但很多系统仍在努力解决共识可以解决的问题。一些大型系统仍然受到单个协调服务器作为单点故障的限制（如HDFS），其他包括使用临时共识算法的系统无法安全的处理失败（如MongoDB 和 Redis）。新系统则几乎没有现成的共识实现可供选择（ZooKeeper 是最受欢迎的），这使得系统构建者要么遵循上面的约束，要么自己实现。</p>
<p>When we started this work, the need for consensus was becoming clear, but many systems still struggled with problems that consensus could solve. Some large-scale systems were still limited by a single coordination server as a single point of failure (e.g., HDFS [81, 2]). Many others included ad hoc replication algorithms that handled failures unsafely (e.g., MongoDB and Redis [44]). New systems had few options for readily available consensus implementations (ZooKeeper [38] was the most popular), forcing systems builders to conform to one or build their own.</p>
<p>那些选择自己实现共识系统的人通常会求助于Paxos[48,49]。在过去20年中，Paxos主导了共识算法的讨论：共识的大多数实现都基于Paxos或受其影响，Paxos已成为教授学生共识系统的主要工具。</p>
<p>Those choosing to implement consensus themselves usually turned to Paxos [48, 49]. Paxos had dominated the discussion of consensus algorithms over the last two decades: most implementations of consensus were based on Paxos or influenced by it, and Paxos had become the primary vehicle used to teach students about consensus.</p>
<p>不幸的是，尽管有很多尝试使 Paxos 变得更加平易近人，但它还是很难理解。此外，它的架构需要大量的修改才能应用在实际系统中，并且基于 Paxos 构建的系统需要开发一些扩展，而这些扩展的详细信息尚未发布或有待商定。结果就是，系统构建者和学生都在Paxos 中挣扎。</p>
<p>Unfortunately, Paxos is quite difficult to understand, in spite of numerous attempts to make it more approachable. Furthermore, its architecture requires complex changes to support practical systems, and building a complete system based on Paxos requires developing several extensions for which the details have not been published or agreed upon. As a result, both system builders and students struggle with Paxos.</p>
<p>另外两种著名的共识算法是Viewstamped Replication[83,82,66]和ZooKeeper中使用的Zab算法[42]。尽管我们认为这两种算法在结构上都比 Paxos 更好，但都没有明确提出这一论点。它们的设计并非以简单或易于理解为主要目标。理解和实现这些算法的仍然很难。</p>
<p>The two other well-known consensus algorithms are Viewstamped Replication [83, 82, 66] and Zab [42], the algorithm used in ZooKeeper. Although we believe both of these algorithms are incidentally better in structure that Paxos for building systems, neither has explicitly made this argument; they were not designed with simplicity or understandability as a primary goal. The burden of understanding and implementing these algorithms is still too high.</p>
<p>这些共识算法都比较难理解和难实现。不幸的是，当使用经验证的算法实现共识的成本太高时，构建者就不得不做出艰难的决定，他们可能完全避免达成共识，牺牲系统的容错性或一致性，或者是开发自己的特殊算法，这通常会导致危险行为。另外，当解释和理解共识成本太高时，导致有的教师都不在教授它，有的学生也无法理解共识。共识与两阶段提交一样重要。理想情况下，应有尽可能多的学生学习（即使从根本上来说，学习共识更为困难）。</p>
<p>Each of these consensus options was difficult to understand and difficult to implement. Unfortunately, when the cost of implementing consensus with proven algorithms was too high, systems builders were left with a tough decision. They could avoid consensus altogether, sacrificing the fault tolerance or consistency of their systems, or they could develop their own ad hoc algorithm, often leading to unsafe behavior. Moreover, when the cost of explaining and understanding consensus was too high, not all instructors attempted to teach it, and not all students succeeded in learning it. Consensus is as fundamental as two-phase commit; ideally, as many students should learn it (even though consensus is fundamentally more difficult).</p>
<p>经过在Paxos中的一番挣扎之后，我们开始着手研究一种新的共识算法，以作为系统构建和教授的良好基础。我们的方法与众不同，因为我们的<font color=red>主要目标是可理解性</font>：我们可以为实际系统定义一个一致的算法，并以一种比 Paxos 更容易学习的方式来描述它吗？此外，我们希望该算法能够促进对系统构建者的直接的开发，这不仅对算法本身很重要，对于算法的原理也很重要。</p>
<p>After struggling with Paxos ourselves, we set out to find a new consensus algorithm that could provide a better foundation for system building and education. Our approach was unusual in that our primary goal was understandability: could we define a consensus algorithm for practical systems and describe it in a way that is significantly easier to learn than Paxos? Furthermore, we wanted the algorithm to facilitate the development of intuitions that are essential for system builders. It was important not just for the algorithm to work, but for it to be obvious why it works.</p>
<p>该算法还必须足够完整，以便解决构建实际系统方方面面的问题，并且必须能在实际部署中表现良好。核心算法不仅能够明确收到消息后的效果，还能描述会发生什么以及何时发生，这些对于系统构建者而言同样重要。类似地，它必须保证一致性，而且必须尽可能的保证可用性。它还必须能解决系统中达成共识之外的诸多问题，比如共识小组成员的变更。这在实际系统中是必须考虑的，如果将这一负担留给系统构建者，则可能会有风险。</p>
<p>This algorithm also had to be complete enough to address all aspects of building a practical system, and it had to perform well enough for practical deployments. The core algorithm not only had to specify the effects of receiving a message but also describe what should happen and when; these are equally important for systems builders. Similarly, it had to guarantee consistency, and it also had to provide availability whenever possible. It also had to address the many aspects of a system that go beyond reaching consensus, such as changing the members of the consensus group. These are necessary in practice, and leaving this burden to systems builders would risk ad hoc, suboptimal, or even incorrect solutions.</p>
<p>这项工作的结果就是称为Raft的共识算法。在设计Raft时，我们使用了特定的技术来提高可理解性，包括分解问题（将Raft分解为Leader选举、日志复制和安全性保证）和缩减状态空间（Raft降低了不确定性的程度以及服务器之间可能不一致的方式）。我们还解决了构建一个完整共识系统所需的所有问题。我们仔细考虑了每个设计选择，这不仅给我们实现它带来了好处，对于其他想实现Raft的构建者也一样。</p>
<p>The result of this work is a consensus algorithm called Raft. In designing Raft we applied specific techniques to improve understandability, including decomposition (Raft separates leader election, log replication, and safety) and state space reduction (Raft reduces the degree of nondeterminism and the ways servers can be inconsistent with each other). We also addressed all of the issues needed to build a complete consensus-based system. We considered each design choice carefully, not just for the benefit of our own implementation but also for the many others we hope to enable.</p>
<p>我们相信Raft要比Paxos以及其他共识算法都要优秀，不管对于教学而言，还是对于提供实现的基础算法而言都是这样。它要比其他算法更简单和更容易理解；它的描述足够完整，可以满足构建实际系统的需要；它有多个开源实现，并被多家公司使用；它的安全性已得到正式的明确和证明；而且它的效率跟其他算法也不相上下。</p>
<p>We believe that Raft is superior to Paxos and other consensus algorithms, both for educational purposes and as a foundation for implementation. It is simpler and more understandable than other algorithms; it is described completely enough to meet the needs of a practical system; it has several open-source implementations and is used by several companies; its safety properties have been formally specified and proven; and its efficiency is comparable to other algorithms.</p>
<p>本文的主要贡献如下：</p>
<ul>
<li>Raft共识算法的设计、实现和评估。Raft与现有的共识算法在很多方面都很相似，但是它是为可理解性而设计的。因此Raft有自己的特点，比如Raft使用了更强的领导者模式，这简化了对日志的管理，并使得Raft更容易理解。</li>
<li>对于可理解性的评估：一项针对两所大学43个学生的研究表明，Raft比Paxos更容易理解：在学习了两种算法之后，其中33名学生对于Raft的问题的回到要比Paxos问题好很多。我们相信这是第一次基于教和学来评估共识算法的科学研究。</li>
<li>Raft中Leader选举的设计、实现和评估。很多公式算法都没有规定特定的领导者选举算法，Raft中涉及了Leader选举的特定算法，其中包含随机时间策略。这只是在很多共识算法所必须的心跳机制上增加了少量机制，却简单且快速的解决了很多冲突。对Leader选举的评估调查了其行为和表现，可以得出这种简单的方法在各种实际环境中都是足够的结论。Raft通常会在集群中单向网络延迟时间的20倍以内的时间中选出Leader。</li>
<li>Raft集群成员变更机制的设计和实现。Raft允许一次增加或删除一个集群节点；这种单次变更很简单的保证了安全性，因为变更期间新旧配置的Majority节点集合至少会有一个节点的重叠。更复杂的成员变更可以通过一系列单节点变更来实现。Raft允许集群在成员变更期间正常运行，成员变更算法只需要对基本算法进行少许的扩展就能实现。</li>
<li>对于构建一个完整的共识系统所需的其他组件的深入讨论和实现，包括与Client的交互、日志压缩等。虽然Raft在这些方面的设计上并不是特别新颖，但是对于这些组件的完整描述对于理解和构建真正的系统而言都非常重要。我们已经构建了一个完整的基于共识的服务，以探索和解决其中涉及的设计决策。</li>
<li>对于Raft算法安全性的证明以及证明的规范性。证明的规范性有助于仔细的推理算法，并澄清非正式描述中的各种细节。安全性证明有助于建立对Raft正确性的信心，而且有助于对Raft进行扩展时能澄清扩展对Raft安全性的影响。</li>
</ul>
<p>The primary contributions of this dissertation are as follows:</p>
<ul>
<li>The design, implementation, and evaluation of the Raft consensus algorithm. Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov’s Viewstamped Replication [83, 66]), but it is designed for understandability. This led to several novel features. For example, Raft uses a stronger form of leadership than other consensus algorithms. This simplifies the management of the replicated log and makes Raft easier to understand.</li>
<li>The evaluation of Raft’s understandability. A user study with 43 students at two universities shows that Raft is significantly easier to understand than Paxos: after learning both algorithms, 33 of these students were able to answer questions about Raft better than questions about Paxos. We believe this is the first scientific study to evaluate consensus algorithms based on teaching and learning.</li>
<li>The design, implementation, and evaluation of Raft’s leader election mechanism. While many consensus algorithms do not prescribe a particular leader election algorithm, Raft includes a specific algorithm involving randomized timers. This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly. The evaluation of leader election investigates its behavior and performance, concluding that this simple approach is sufficient in a wide variety of practical environments. It typically elects a leader in under 20 times the cluster’s one-way network latency.</li>
<li>The design and implementation of Raft’s cluster membership change mechanism. Raft allows adding or removing a single server at a time; these operations preserve safety simply, since at least one server overlaps any majority during the change. More complex changes in membership are implemented as a series of single-server changes. Raft allows the cluster to continue operating normally during changes, and membership changes can be  implemented with only a few extensions to the basic consensus algorithm.</li>
<li>A thorough discussion and implementation of the other components necessary for a complete consensus-based system, including client interaction and log compaction. Although we do not believe these aspects of Raft to be particularly novel, a complete description is important for understandability and to enable others to build real systems. We have implemented a complete consensus-based service to explore and address all of the design decisions involved.</li>
<li>A proof of safety and formal specification for the Raft algorithm. The level of precision in the formal specification aids in reasoning carefully about the algorithm and clarifying details in the algorithm’s informal description. The proof of safety helps build confidence in Raft’s correctness. It also aids others who wish to extend Raft by clarifying the implications for safety of their extensions.</li>
</ul>
<p>我们已经实现了一个名为LogCabin的开源系统，其中实现了本文中的很多设计。LogCabin是我们验证Raft新想法的测试平台，也是验证我们理解构建完整实用系统的方法。在本文的第10章我们会更详细的描述LogCabin。</p>
<p>We have implemented many of the designs in this dissertation in an open-source implementation of Raft called LogCabin [86]. LogCabin served as our test platform for new ideas in Raft and as a way to verify that we understood the issues of building a complete and practical system. The implementation is described in more detail in Chapter 10.</p>
<p>本文的剩余部分介绍了复制状态机，并讨论了Paxos的优缺点（第2章）；介绍了Raft共识算法的核心算法、集群成员变更和日志压缩的扩展，以及Client如何与Raft交互（第3-6章）；评估了Raft的可理解性、正确性、领导人选举和日志复制性能（第7-10章）；并讨论了相关工作（第11章）。</p>
<p>The remainder of this dissertation introduces the replicated state machine problem and discusses the strengths and weaknesses of Paxos (Chapter 2); presents the Raft consensus algorithm, its extensions for cluster membership changes and log compaction, and how clients interact with Raft (Chapters 3–6); evaluates Raft for understandability, correctness, and leader election and log replication performance (Chapters 7–10); and discusses related work (Chapter 11).</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-02RAFT共识,状态机,动机</title>
    <url>/2021/10/23/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/12RAFT%E5%85%B1%E8%AF%86,%E7%8A%B6%E6%80%81%E6%9C%BA,%E5%8A%A8%E6%9C%BA/</url>
    <content><![CDATA[<p>本章介绍了共识算法和复制状态机的关系，以及设计RAFT的动机。重点在共识算法和复制状态机的关系。</p>
<h3 id="2-Motivation-RAFT动机"><a href="#2-Motivation-RAFT动机" class="headerlink" title="2 Motivation RAFT动机"></a>2 Motivation RAFT动机</h3><p>共识是容错系统中的基本问题：即多个服务节点间，即使是有节点失效的情况下，如何就shared state达成一致。共识问题产生于需要提供高可用且不能损害一致性的各种系统中。因此，几乎所有的一致性大规模系统中都需要解决共识问题。2.1节描述了共识通常如何用于创建复制状态机，这是容错系统的通用构建块；2.2节讨论了在大型系统中使用复制状态机的各种方式；2.3节讨论了Paxos共识协议的问题，这也是设计RAFT协议的初衷。</p>
<p>Consensus is a fundamental problem in fault-tolerant systems: how can servers reach agreement on shared state, even in the face of failures? This problem arises in a wide variety of systems that need to provide high levels of availability and cannot compromise on consistency; thus, consensus is used in virtually all consistent large-scale storage systems. Section 2.1 describes how consensus is typically used to create replicated state machines, a general-purpose building block for fault tolerant systems; Section 2.2 discusses various ways replicated state machines are used in larger systems; and Section 2.3 discusses the problems with the Paxos consensus protocol, which Raft aims to address.</p>
<h4 id="2-1-Achieving-fault-tolerance-with-replicated-state-machines-通过复制状态机实现容错"><a href="#2-1-Achieving-fault-tolerance-with-replicated-state-machines-通过复制状态机实现容错" class="headerlink" title="2.1 Achieving fault tolerance with replicated state machines 通过复制状态机实现容错"></a>2.1 Achieving fault tolerance with replicated state machines 通过复制状态机实现容错</h4><p>共识算法通常出现在复制状态机（replicated state machines）的上下文中。在这种方法中，状态机运行于多个服务节点上，操作相同的状态副本，即使某些服务器下线，也可以继续运行。复制状态机用于解决分布式系统中的各种容错问题。复制状态机的实例，比如Chubby和ZooKeeper，他们用于为小规模配置数据提供分层键值对的存储。它们提供了基本的get和put操作，还提供了compare-and-swap等同步原语，使并发客户端能够安全地进行协调操作。</p>
<p>Consensus algorithms typically arise in the context of replicated state machines [102]. In this approach, state machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems, as described in Section 2.2. Examples of replicated state machines include Chubby [11] and ZooKeeper [38], which both provide hierarchical key-value stores for small amounts of configuration data. In addition to basic operations such as get and put, they also provide synchronization primitives like compare-and-swap, enabling concurrent clients to coordinate safely.</p>
<p><font color=red>复制状态机通常通过复制日志（replicated log）的方式来实现，</font>比如下面的图2.1展示的那样。每个服务器存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。由于状态机是确定的，因此每个状态机操作相同的状态，并产生相同的输出序列。</p>
<p>图2.1: <font color=red>复制状态机的架构。一致性算法管理复制日志，日志中包含来自客户端的状态机命令。状态机处理来自日志的相同命令序列，因此它们产生相同的输出。</font></p>
<p><img src="/img/00RAFT/image-20210922190656584.png" alt="image-20210922190656584"></p>
<p>Replicated state machines are typically implemented using a replicated log, as shown in Figure 2.1. Each server stores a log containing a series of commands, which its state machine executes in order. Each log contains the same commands in the same order, so each state machine processes the same sequence of commands. Since the state machines are deterministic, each computes the same state and the same sequence of outputs.</p>
<p><font color=red>共识算法负责保持复制日志的一致性</font>。服务节点上的共识模块从客户端接收命令并将其添加到日志中。本节点的共识模块与其他服务节点上的共识模块通信，以确保每个日志最终以相同的顺序包含相同的请求，即使某些服务节点出现故障。一旦命令被正确复制，它们就被称为已提交的。<font color=red>每个服务节点上的状态机按日志顺序处理提交的命令，并将输出返回给客户端。</font>因此，多个服务节点就形成了一个单一的、高度可靠的状态机。</p>
<p><em><font color=red>个人理解</font>：共识算法用于保证日志的一致性，而复制状态机以日志中的命令为输入，多个节点上的状态机，以相同的顺序处理相同的命令，从而产生相同的输出。</em></p>
<p>Keeping the replicated log consistent is the job of the consensus algorithm. The consensus module on a server receives commands from clients and adds them to its log. It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are properly replicated, they are said to be committed. Each server’s state machine processes committed commands in log order, and the outputs are returned to clients. As a result, the servers appear to form a single, highly reliable state machine.</p>
<p>实际系统中，共识算法通常具有如下属性：</p>
<ul>
<li>它们可以确保在所有非拜占庭条件下的安全性（从不返回错误结果），非拜占庭条件包括网络延迟、网络分区、数据包丢失、重复和乱序。</li>
<li>只要大多数服务节点（超过半数）可以运行，且可以相互通信，且可以与客户机通信，它们就可以正常工作（可用）。因此，比如由五台服务节点组成的典型集群可以容忍任意两台服务节点的故障。假定服务节点因宕机而发生故障；它们可能稍后就能从稳定存储上的状态恢复并重新加入集群。</li>
<li>它们不依赖于时间来确保日志的一致性：错误的时钟和极端的消息延迟在最坏的情况下会导致可用性问题。也就是说，它们保证异步模型下的安全，在异步模型中，消息和处理器以任意速度运行。</li>
<li>在一般情况下，只要集群的大多数成员响应了一轮RPC，命令就可以完成；少数速度较慢的服务器不会影响总体系统性能。</li>
</ul>
<p>Consensus algorithms for practical systems typically have the following properties:</p>
<ul>
<li>They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering.</li>
<li>They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster.</li>
<li>They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems. That is, they maintain safety under an asynchronous model [71], in which messages and processors proceed at arbitrary speeds.</li>
<li>In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.</li>
</ul>
<h4 id="2-2-Common-use-cases-for-replicated-state-machines-复制状态机的常见用例"><a href="#2-2-Common-use-cases-for-replicated-state-machines-复制状态机的常见用例" class="headerlink" title="2.2 Common use cases for replicated state machines  复制状态机的常见用例"></a>2.2 Common use cases for replicated state machines  复制状态机的常见用例</h4><p>复制状态机是使系统容错的通用模块。它们可以以多种方式使用，本节将讨论一些典型的使用模式。</p>
<p>Replicated state machines are a general-purpose building block for making systems fault-tolerant. They can be used in a variety of ways, and this section discusses some typical usage patterns.</p>
<p>最常见的共识部署通常只有三到五台服务器构成一个复制状态机。然后，其他服务器可以使用此状态机来协调其活动，如图2.2（a）所示。这些系统通常使用复制状态机来提供组织成员、配置管理，或分布式锁等功能。一个更具体的例子，复制状态机可以提供一个容错工作队列，其他服务器可以使用复制状态机进行协同操作，给自己分配工作队列中的工作任务。</p>
<p>Most common deployments of consensus have just three or five servers forming one replicated state machine. Other servers can then use this state machine to coordinate their activities, as shown in Figure 2.2(a). These systems often use the replicated state machine to provide group membership, configuration management, or locks [38]. As a more specific example, the replicated state machine could provide a fault-tolerant work queue, and other servers could coordinate using the replicated state machine to assign work to themselves.</p>
<p>2.2(a)：集群中的节点通过读写状态机来进行协同工作；</p>
<p>2.2(b)：领导职管理集群中的其他节点，使用复制状态机记录状态。其他节点处于被动状态，直到领导节点崩溃；</p>
<p><img src="/img/00RAFT/image-20210922193457068.png" alt="image-20210922193457068"></p>
<p>图2.2（b）显示了这种用法的常见简化。在这种模式中，一台服务器充当领导者，管理其余的服务器。领导者将其关键数据存储在共识系统中。如果领导者失败，则其他备用服务器将争夺领导者的位置，如果成功，它们将使用共识系统中的数据继续运行。许多大型存储系统具有单一领导者，如GFS[30]、HDFS[105]和RAMCloud[90]，都使用这种方法。</p>
<p>A common simplification to this usage is shown in Figure 2.2(b). In this pattern, one server acts as leader, managing the rest of the servers. The leader stores its critical data in the consensus system. In case it fails, other standby servers compete for the position of leader, and if they succeed, they use the data in the consensus system to continue operations. Many large-scale storage systems that have a single cluster leader, such as GFS [30], HDFS [105], and RAMCloud [90], use this approach.</p>
<p>共识系统有时也用于复制大规模数据，如图2.3所示。大型存储系统，如Megastore[5]、Panner[20]和District[32]，因存储的数据太多，无法容纳在单个服务器组中。因此它们通过多个复制状态机将数据进行分区，而跨多个分区的操作使用两阶段提交协议（2PC）来保持一致性。</p>
<p>Consensus is also sometimes used to replicate very large amounts of data, as shown in Figure 2.3. Large storage systems, such as Megastore [5], Spanner [20], and Scatter [32], store too much data to fit in a single group of servers. They partition their data across many replicated state machines, and operations that span multiple partitions use a two-phase commit protocol (2PC) to maintain consistency.</p>
<p>图2.3：使用共识的大型分区存储系统。为了扩展性，数据在多个复制状态机之间进行分区。跨分区的操作使用两阶段提交协议。</p>
<p><img src="/img/00RAFT/image-20210922194212887.png" alt="image-20210922194212887"></p>
<h4 id="2-3-What’s-wrong-with-Paxos-Paxos算法的问题"><a href="#2-3-What’s-wrong-with-Paxos-Paxos算法的问题" class="headerlink" title="2.3 What’s wrong with Paxos?  Paxos算法的问题"></a>2.3 What’s wrong with Paxos?  Paxos算法的问题</h4><p>在过去十年中，Leslie Lamport的Paxos协议几乎就是共识的同义词：它是课程中最常教的协议，大多数共识系统都以其为起点进行实现。Paxos首先定义了一个能够就单个决策达成一致的协议，例如单个复制日志的条目。我们将该子集称为single-decree Paxos。然后，Paxos将single-decree Paxos的多个实例组合起来，以促进一系列决策的达成，这就是所谓的Multi-Paxos。图2.4总结了Single-decree Paxos，图A.5总结了Multi-Paxos。Paxos确保了安全性和活跃性（它最终达成了共识，并且假设使用足够的故障检测器来避免proposer的活锁问题），并且其正确性已经得到了证明。Multi-Paxos在正常情况下是有效的，而且Paxos支持集群成员的变更。</p>
<p>Over the last ten years, Leslie Lamport’s Paxos protocol [48] has become almost synonymous with consensus: it is the protocol most commonly taught in courses, and most implementations of consensus use it as a starting point. Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry. We refer to this subset as single-decree Paxos. Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log (Multi-Paxos). Single-decree Paxos is summarized in Figure 2.4, and Multi-Paxos is summarized<br>in Figure A.5. Paxos ensures safety and liveness (it eventually reaches consensus, assuming an adequate failure detector is used to avoid proposer livelock), and its correctness has been proven. Multi-Paxos is efficient in the normal case, and Paxos supports changes in cluster membership [69].</p>
<p><img src="/img/00RAFT/image-20210922195317445.png" alt="image-20210922195317445"></p>
<p>不幸的是，Paxos有两个显著的缺点。第一个缺点是Paxos非常难以理解。Paxos的完整解释[48]是出了名的不透明；只有很少的人，在付出巨大努力的情况下，才能理解它。因此，有人试图用更简单的术语解释Paxos[49,60,61]。这些解释侧重于single-decree subset，但即使如此仍然具有挑战性。在2012年NSDI的一项非正式调查中，我们发现很少有人对Paxos感到满意，即使是在经验丰富的研究人员中也是如此。我们也是在阅读了几篇Paxos的解释，并设计了我们自己的替代方案之后，才能理解完整的Paxos方案，这一过程花了将近一年的时间。</p>
<p>Unfortunately, Paxos has two significant drawbacks. The first drawback is that Paxos is exceptionally difficult to understand. The full explanation [48] is notoriously opaque; few people succeed in understanding it, and only with great effort. As a result, there have been several attempts to explain Paxos in simpler terms [49, 60, 61]. These explanations focus on the single-decree subset, yet they are still challenging. In an informal survey of attendees at NSDI 2012, we found few people who were comfortable with Paxos, even among seasoned researchers. We struggled with Paxos ourselves; we were not able to understand the complete protocol until after reading several explanations and designing our own alternative protocol, a process that took almost a year.</p>
<p>我们认为Paxos的不透明性源于其选择Single-decree Paxos子集作为基础。Single-decree Paxos是笨拙而微妙的：它分为两个阶段，没有简单的直观解释，无法独立理解。因此，很难对Single-decree Paxos协议的工作原理形成直觉。而Multi-Paxos的组合规则增加了显著的额外复杂性和微妙性。我们认为，就多个决策（即，一个日志而不是一个条目）达成共识的总体问题可以以其他更直接和明显的方式进行分解。</p>
<p>We hypothesize that Paxos’ opaqueness stems from its choice of the single-decree subset as its foundation. Single-decree Paxos is dense and subtle: it is divided into two stages that do not have simple intuitive explanations and cannot be understood independently. Because of this, it is difficult to develop intuitions about why the single-decree protocol works. The composition rules for Multi-Paxos add significant additional complexity and subtlety. We believe that the overall problem of reaching consensus on multiple decisions (i.e., a log instead of a single entry) can be decomposed<br>in other ways that are more direct and obvious.</p>
<p>Paxos的第二个问题是它没有为构建实际的实现提供良好的基础。一个原因是，对于Multi-Paxos，没有广泛认可的算法。Lamport的描述大多是关于Single-decree Paxos；他勾勒出了实现Multi-Paxos的可能方法，但缺少许多细节。有几次尝试充实和优化Paxos，如[77]、[108]和[46]，但它们彼此不同，也不同于Lamport的草图。Chubby[15]等系统已经实现了类似Paxos的算法，但在大多数情况下，它们的详细信息尚未公布。</p>
<p>The second problem with Paxos is that it does not provide a good foundation for building practical implementations. One reason is that there is no widely agreed-upon algorithm for Multi-Paxos. Lamport’s descriptions are mostly about single-decree Paxos; he sketched possible approaches to Multi-Paxos, but many details are missing. There have been several attempts to flesh out and optimize Paxos, such as [77], [108], and [46], but these differ from each other and from Lamport’s sketches. Systems such as Chubby [15] have implemented Paxos-like algorithms, but in most cases their details have not been published.</p>
<p>此外，Paxos体系结构对于构建实际系统来说是一个糟糕的体系结构；这是single-decree Paxos分解的另一个结果。例如，独立地选择一组日志条目，然后将它们合并到一个顺序日志中，没有什么好处；这只会增加复杂性。直接围绕日志设计系统更简单也更有效，在日志中，新条目按受限顺序逐条追加。另一个问题是Paxos的核心采用了对称的点对点方法（尽管它也暗示了作为性能优化的一种弱领导形式）。这在一个简化的世界中是有意义的，在这个世界中只会做出一个决定，但很少有实际的系统使用这种方法。如果必须做出一系列决策，那么首先选举一位领导者，然后让领导者协调决策会更简单、更快。（第11章讨论了平等主义Paxos（Egalitarian Paxos），Paxos的一个最新变体，它不使用leader，但在某些情况下可能比使用leader的算法更有效；然而，该算法比基于leader的算法复杂得多。）</p>
<p>Furthermore, the Paxos architecture is a poor one for building practical systems; this is another consequence of the single-decree decomposition. For example, there is little benefit to choosing a collection of log entries independently and then melding them into a sequential log; this just adds complexity. It is simpler and more efficient to design a system around a log, where new entries are appended sequentially in a constrained order. Another problem is that Paxos uses a symmetric peer-to-peer approach at its core (though it also suggests a weak form of leadership as a performance optimization). This makes sense in a simplified world where only one decision will be made, but few practical systems use this approach. If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions. (Chapter 11 discusses Egalitarian Paxos, a recent variant of Paxos that does not use a leader but in some situations can be more efficient than algorithms that do; however, this algorithm is much more complex than leader based algorithms.)</p>
<p>因此，实际系统与Paxos几乎没有相似之处。每个实现都从Paxos开始，发现实现它的困难，然后开发一个显著不同的体系结构。这既耗时又容易出错，理解Paxos的困难更加加剧了问题。Paxos的公式对于证明其正确性的定理来说可能是一个很好的公式，但实际实现与Paxos如此不同，以至于证明几乎没有价值。Chubby的作者的以下评论是典型的：</p>
<blockquote>
<p>Paxos算法的描述与现实系统的需求之间存在着巨大的鸿沟，…，最终系统将基于未经验证的协议来构建[15]。</p>
</blockquote>
<p>由于上面这些问题，我们得出结论，Paxos 没有提供一个良好的基础，无论是为系统建设，还是教授给学生。考虑到共识算法在大规模软件系统中的重要性，我们决定看看是否可以设计一种比Paxos具有更好的共识算法。RAFT就是这个实验的结果。</p>
<p>As a result, practical systems bear little resemblance to Paxos. Each implementation begins with Paxos, discovers the difficulties in implementing it, and then develops a significantly different architecture. This is time-consuming and error-prone, and the difficulties of understanding Paxos exacerbate the problem. Paxos’ formulation may be a good one for proving theorems about its correctness, but real implementations are so different from Paxos that the proofs have little value. The following comment from the Chubby implementers is typical：</p>
<p>There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system. …  the final system will be based on an unproven protocol [15].</p>
<p>Because of these problems, we concluded that Paxos does not provide a good foundation either for system building or for education. Given the importance of consensus in large-scale software systems, we decided to see if we could design an alternative consensus algorithm with better properties than Paxos. Raft is the result of that experiment.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-04-1成员变更总结</title>
    <url>/2021/12/20/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/14-1%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>以下内容，是对Raft博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》中关于配置变更的总结，内容来自于“4 Cluster membership changes”。</p>
<p>成员变更的最大风险，是可能会同时出现 2 个领导者。比如有一个由节点 A、B、C 组成的 Raft 集群，现在需要增加另外 2 个节点，扩展为由节点 A、B、C、D、E， 5 个节点组成的新集群。在进行成员变更时，节点 A、B 和 C 之间发生了分区错误，节点 A、B 组成旧配置中的“大多数”，也就是变更前的 3 节点集群中的“大多数”，那么这时节点 A和B采用旧配置，可以选出Leader，而节点 C 和新节点 D、E 组成了新配置的“大多数”，它们可能会选举出新的Leader。那么这时，就出现了同时存在 2 个领导者的情况。这就破坏了 Raft 集群的领导者唯一性，影响了集群的运行。</p>
<span id="more"></span>
<img src="/img/03RAFT成员变更(博士论文)/v2-26c7a68e9aae047a910c2071bcff4831_720w.jpg" alt="img" style="zoom: 50%;" />

<p>所以，成员变更的首要任务和挑战就是维护安全性。即在成员变更期间不能在同一个Term内选举出两个Leader。因集群中所有节点不可能同时自动切换到新配置，所以在变更期间，需要防止集群分裂为两个不相交的Majority集合。</p>
<p>Raft推荐用一种简单的方法来处理这个问题，即限制允许的配置变更类型：集群中一次只能添加或删除一个节点。更复杂的成员变更可以通过一系列的单节点变更来实现。这就是单节点变更算法。</p>
<p>向集群中添加或删除一个节点，那旧集群配置的任一Majority节点集合，与新集群配置的任一Majority节点集合肯定有交集。这种集合的相交，防止了集群被分裂成两个独立的Majority节点集合，保证了集群成员变更的安全性。如图：</p>
<p><img src="/img/14%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4_%E6%80%BB%E7%BB%93/image-20220419081632386.png" alt="image-20220419081632386"></p>
<p>单节点变更算法具体流程：</p>
<ul>
<li>当Leader收到请求需要在当前配置（Cold）中增加或删除一个节点时，它把新配置（Cnew）作为一个日志条目，使用常规的Raft日志复制机制，将其复制到其他节点上。</li>
<li>其他节点只要收到该条目，将该条目追加到本地日志后，就认为新配置生效了；这就表示节点无需等到Cnew条目提交后才是用新配置，它们直接使用日志中能找到的最新配置。</li>
<li>Cnew条目只会复制到Cnew的节点中，使用Cnew中的Majority节点来决定Cnew的提交。</li>
</ul>
<p>一旦Cnew条目提交后，就认为本次配置变更完成了。从此刻起，Leader就知道Cnew节点中的过半节点已经采用了Cnew的配置。而且可以明确Cnew之外的节点不可能形成Majority集合了，从而Cnew之外的节点就无法选出Leader了。Cnew条目的提交可以让下面的操作得以继续：</p>
<ul>
<li>Leader可以确认配置变更已经完成；</li>
<li>如果配置变更是移除了某个节点，则该节点可以下线了；</li>
<li>可以开始新的配置变更了。如果在此之前进行新的配置变更，则重叠的配置变更可能会导致类似上图那样的危险场景（重叠的配置变更可能会导致新旧配置的Majority节点不相交的场景：考虑<code>abc -&gt; abcd -&gt; abcde</code>的过程，如果原<code>abc</code>中<code>c</code>为Leader，现在加入<code>d</code>，新配置<code>abcd</code>只复制给了<code>c</code>和<code>d</code>，尚未提交时，又来了新的配置变更，增加<code>e</code>，最新配置<code>abcde</code>复制到了<code>c</code>、<code>d</code>和<code>e</code>，那现在<code>ab</code>使用最老的配置，<code>cde</code>使用最新的配置，一旦分区就出现了脑裂）；</li>
</ul>
<p>节点直接使用日志中最新的配置，而不用等到配置提交后才使用。这可以使得Leader在当前配置未提交时，不会开始新的配置变更，从而避免了配置变更并行的场景（上述的第三点）。只有当Cnew配置提交后，开始进行新的配置变更才是安全的。</p>
<p>如果节点等到Cnew日志提交后才使用Cnew的配置，那Leader是无法感知是否有Majority的节点已经使用Cnew配置了。这就需要Leader去获取其他节点的CommitIndex，其他节点也需要将CommitIndex进行持久化，这两种机制都不是Raft所需的。因此，当节点将Cnew日志追加到日志表中，回复了AppendEntries RPC之后，节点就可以采用Cnew配置了，Leader收到Majority节点关于Cnew日志的AppendEntries RPC的成功回复，就可以将Cnew提交，进而开始下一次的配置变更了。不过，这种机制可能会使得配置日志被删除（当Leader发生变更时），所以节点必须能回退到日志中上一个配置。</p>
<hr>
<p>在Raft中，使用调用者（RPC发起者）的配置来实现共识，而非被调用者（RPC接收者）的配置。比如对于投票和日志复制而言：</p>
<ul>
<li>收到RequestVote RPC的节点a，在满足条件的基础上，可以投票给它不认识的节点b（即a节点本地日志中最新配置中之外的节点）。比如，Cold为[a, b, c]三个节点，leader为a，Cnew为[a, b, c, d]，即新增d节点。如果配置变更时a已经将Cnew复制给了a, b, d，而c节点没有收到，此时Cnew也能生效。但接着节点a崩溃了，此时新节点d成为Candidate，它需要b，c的投票，如果c因为其配置中不包含d而拒绝投票的话，d就无法成为leader，降低了集群的可用性。</li>
<li>节点a收到了节点b发来的AppendEntries RPC，即使a不认识节点b，它也会接受该RPC。还是继续上面的例子，新节点d成为新Leader后开始发送AppendEntries RPC，如果c不接受该RPC，则d收到的AppendEntries RPC回复永远无法成为Majority（此时Cnew配置为[a, b, c, d]，a崩溃了，则Majority只能是[b, c, d]），导致集群不可用。</li>
</ul>
<p>因此，节点收到RPC后，无需查询当前的配置信息。</p>
<hr>
<p>配置变更对于集群可用性，有以下几点影响：</p>
<ul>
<li><p>新加入集群的节点的日志需要追上Leader：无日志的新节点加入集群时，如果它直接成为集群的一员，那他的日志需要很长一段时间才能追上Leader，这段时间内，集群很容易处于不可用的状态。比如，三节点的集群（a, b, c），如果不包含任何日志的第四个节点（d）加入到集群中，并且原来的节点c崩溃了，则d的日志在追上Leader之前，集群是无法提交任何的新日志。</p>
<p>为了避免这种可用性问题，Raft在进行配置变更之前引入了新的阶段，在这个阶段内，新节点会作为非投票成员（non-voting member）加入到集群中。这个节点，Leader会向新节点同步日志，而在投票或者日志复制时，不会把它当做形成Majority的条件。当新节点的日志追上集群后，配置变更才可以继续进行。</p>
</li>
<li><p>如果Cnew中是需要移除当前的Leader，则该Leader必须在某个时间点进行下线。</p>
<p>Raft最初开发了一种配合联合共识的Leader下线方法：旧配置中的Leader直接进行配置变更，然后再下线。这种方法可能会使集群处于一种尴尬的场景中：Leader管理的配置中不包含自己。这种方法中，Leader在Cnew提交后就会下线，如果Leader在此之前下线，则它还有可能会选举超时并且再次成为leader，从而拖慢了整体进度。在更极端的2节点场景中，只能是该旧Leader重新成为Leader后集群才得以继续运行</p>
</li>
<li><p>被移出的节点会影响新集群的可用性：Leader创建好Cnew日志后，Cnew之外的节点就再也收不到心跳包了，所以这些节点最终会超时，并且开始新的选举，新的Term发送RequestVote RPC，这就导致Leader转为Follower状态。虽然最终Cnew中的某个节点可能会当选，但是破坏性节点还是会超时并重复选举过程，这对集群可用性造成很大影响。</p>
<p>首先想到的解决办法是增加PreVote阶段。但是实际上PreVote并不能解决该问题，一旦Leader创建了Cnew日志后，被移除的节点就已经具有破坏性了。PreVote方法无法解决该问题，因为它的日志可能要跟Majority一样新。</p>
<p>最终的解决方法是使用心跳来检测有效Leader的存在（租约）。在Raft中，只要Leader能维持与Follower的心跳消息，该Leader就被认为是active的。因此，移除节点就无法破坏当前的Leader了。修改RequestVote RPC机制：如果节点在当前Leader的最小选举超时时间内收到了RequestVote请求，则它不会更新term，也不会投票。它可以忽略该请求，或者返回一个拒绝投票的结果。</p>
</li>
</ul>
<hr>
<p>联合共识算法，可以一次性处理任意数量的节点变更。</p>
<p>为了保证配置变更期间的安全性，集群首先进入到一种过渡配置状态，称之为联合共识（joint consensus），联合共识既包含Cold，也包含Cnew。所谓联合共识，就是指选举或日志提交，需要在Cold的Majority，以及Cnew的Majority中同时满足（保证了不会出现两个Leader）。一旦联合共识被提交了，则系统进入到新配置。具体而言：</p>
<ul>
<li>Leader收到配置变更请求，要从Cold转为Cnew，Leader将Cold,new作为一条日志项，将其复制到Cold以及Cnew的节点上；就像单节点变更算法一样，节点只要保存了配置日志，就开始将其作为最新的配置项使用。这就意味着Leader将使用Cold,new的配置来决定何时将Cold,new提交。如果Leader挂了，则新Leader可能是Cold中的某节点，也可能是Cold,new中的节点，这取决于该节点是否收到了Cold,new日志。不管是哪种情况，此阶段中Cnew不能单方面做出决定。</li>
<li>一旦Cold,new被提交，则Cold和Cnew都不能在未经对方批准的情况下做出决定，而且Leader完备性保证了只有拥有Cold,new的节点才能选举成功。Cold,new提交后，Leader可以创建Cnew配置，并将其复制到Cnew节点上，只要节点收到并保存了该配置，则节点就使用该配置做决策了。当Cnew日志项在Cnew的Majority中被提交了，则Cold就无关紧要了，不在Cnew中的节点可以关机了。</li>
<li>这个过程中，不会存在Cold和Cnew同时独立决策的情况，这就保证了安全性。</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-04-1集群节点变更</title>
    <url>/2021/12/16/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/14-0%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E5%8F%98%E6%9B%B4/</url>
    <content><![CDATA[<h2 id="4-Cluster-membership-changes-集群节点变更"><a href="#4-Cluster-membership-changes-集群节点变更" class="headerlink" title="4 Cluster membership changes    集群节点变更"></a>4 Cluster membership changes    集群节点变更</h2><p>到现在为止我们都是假设集群配置（集群中参与共识算法的节点集合）是保持不变的。但是实际场景中，经常需要更改配置，比如需要替换坏掉的节点，或者增加复制的程度。可以手动通过下面的方法进行修改：</p>
<ul>
<li><p>将这个集群下线，更新配置文件，然后重启集群。但是这种方法使得更改期间集群不可用；</p>
</li>
<li><p>新节点通过配置旧节点的地址的方式替代旧节点。但是管理员需要保证旧节点不会再次出现，否则系统直接丧失了安全性的保证。</p>
<span id="more"></span>
<p>Up until now we have assumed that the cluster configuration (the set of servers participating in the consensus algorithm) is fixed. In practice, it will occasionally be necessary to change the configuration, for example to replace servers when they fail or to change the degree of replication. This could be done manually, using one of two approaches:</p>
</li>
<li><p>Configuration changes could be done by taking the entire cluster off-line, updating configuration files, and then restarting the cluster. However, this would leave the cluster unavailable during the changeover.</p>
</li>
<li><p>Alternatively, a new server could replace a cluster member by acquiring its network address. However, the administrator must guarantee that the replaced server will never come back up, or else the system would lose its safety properties (for example, there would be an extra vote).</p>
</li>
</ul>
<p>上面两种方法都有很大的缺点，更确切的说一旦有手动的步骤，则都会有出错的可能。</p>
<p>为了避免该问题，我们开发了自动变更机制，并将其整合到Raft共识算法中。Raft允许在变更期间继续处理客户端的请求，而且仅需对现有的机制做很少的更改。下图总结了用于执行配置变更的RPC。</p>
<p>Both of these approaches to membership changes have significant downsides, and if there are any manual steps, they risk operator error.</p>
<p>In order to avoid these issues, we decided to automate configuration changes and incorporate them into the Raft consensus algorithm. Raft allows the cluster to continue operating normally during changes, and membership changes can be implemented with only a few extensions to the basic consensus algorithm. Figure 4.1 summarizes the RPCs used to change cluster membership, whose elements are described in the remainder of this chapter.</p>
<p><img src="/img/03RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87)/image-20210817184116440.png" alt="image-20210817184116440"></p>
<p><em>AddServer RPC，管理端通过该RPC向集群中添加节点：</em></p>
<ul>
<li><em>参数：新增节点的地址；</em></li>
<li><em>结果：</em><ul>
<li><em>status：添加成功则返回OK；</em></li>
<li><em>leaderHint：Leader的地址；</em></li>
</ul>
</li>
<li><em>实现：</em><ul>
<li><em>如果收到该RPC的节点不是Leader，则返回NOT_LEADER错误；</em></li>
<li><em>通过固定的轮数，使得新增节点的日志追上当前Leader。如果新节点在选举超时时间内没有进展，或者其最后一轮的时间超过了选举超时时间，则返回TIMEOUT错误；</em></li>
<li><em>等待之前的日志被提交；</em></li>
<li><em>将新配置添加到日志中，并复制其他节点中（包括新节点），如果Majority节点复制成功，则将其commit；</em></li>
<li><em>回复OK；</em></li>
</ul>
</li>
</ul>
<p><em>RemoveServer RPC，管理端通过该RPC从集群中移除节点：</em></p>
<ul>
<li><em>参数：移除节点的地址；</em></li>
<li><em>结果：</em><ul>
<li><em>status：移除成功则返回OK；</em></li>
<li><em>leaderHint：Leader的地址；</em></li>
</ul>
</li>
<li><em>实现：</em><ul>
<li><em>如果收到该RPC的节点不是Leader，则返回NOT_LEADER错误；</em></li>
<li><em>等待之前的日志被提交；</em></li>
<li><em>将新配置添加到日志中，并复制其他节点中（不包括移除节点），如果Majority节点复制成功，则将其commit；</em></li>
<li><em>回复OK；</em></li>
</ul>
</li>
</ul>
<h3 id="4-1-Safety-安全"><a href="#4-1-Safety-安全" class="headerlink" title="4.1 Safety     安全"></a>4.1 Safety     安全</h3><p><font color=red>配置变更的首要挑战就是维护安全性。为了保证安全性，在配置变更（成员变更）期间不能在同一个Term内选举出两个Leader。</font>如果一次配置变更增加&#x2F;删除多个节点，则从旧配置直接切换到新配置就是不安全的；集群中所有节点不可能同时自动切换到新配置，所以在变更期间，集群有可能分裂为两个不相交的Majority集合（见下图4.2）。</p>
<p><img src="/img/14RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220418083647795.png" alt="image-20220418083647795"></p>
<p><em>图4.2：从一个配置直接切换到另一个配置是不安全的，因为不同的节点会在不同的时间进行切换。比如上图中，3节点的集群扩展为5节点的集群就会有问题。总会有一个时间点，<font color=red>同一个Term内可能会选出两个不同的Leader来，一个是由旧配置（Cold）下的过半节点选出，一个是由新配置（Cnew）下的过半节点选出。</font></em></p>
<p>Preserving safety is the first challenge for configuration changes. For the mechanism to be safe, there must be no point during the transition where it is possible for two leaders to be elected for the same term. If a single configuration change adds or removes many servers, switching the cluster directly from the old configuration to the new configuration can be unsafe; it isn’t possible to atomically switch all of the servers at once, so the cluster can potentially split into two independent majorities during the transition (see Figure 4.2).</p>
<p>大多数成员变更算法都需要引入额外的机制来处理上面的问题。我们最初设计Raft时也是这么做的，不过之后我们又想到了一种简单的方法，<font color=red>即不允许执行会导致不相交Majority节点集合的配置变更。所以，Raft限制了允许的配置变更类型：集群中一次只能添加或删除一个节点。</font>更复杂的成员变更可以通过一系列的单节点变更来实现。本章的大部分内容都是描述这种单节点变更算法，这种算法比比我们最初的方法更容易理解。为了完整性，4.3节也描述了最初的方法，这种方法为了能应对任意的节点变更引入了额外的复杂性。我们在LogCabin中实现了这种复杂的方法，在撰写本文时，LogCabin仍然使用的是该方法。</p>
<p>Most membership change algorithms introduce additional mechanism to deal with such problems. This is what we did for Raft initially, but we later discovered a simpler approach, which is to disallow membership changes that could result in disjoint majorities. Thus, Raft restricts the types of changes that are allowed: only one server can be added or removed from the cluster at a time. More complex changes in membership are implemented as a series of single-server changes. Most of this chapter describes the single-server approach, which is easier to understand than our original approach. For completeness, Section 4.3 describes the original approach, which incurs additional complexity to handle arbitrary configuration changes. We implemented the more complex approach in LogCabin prior to discovering the simpler single-server change approach; it still uses the more complex approach at the time of this writing.</p>
<p>如图4.3，向集群中添加一个节点或是删除一个节点，那旧集群配置的任一Majority节点集合，与新集群配置的任一Majority节点集合肯定有交集。这种集合的相交，防止了集群被分裂成两个独立的Majority节点集合；根据3.6.3节的安全论证，它保证了论证中“投票者”的存在。因此，当进行单节点变更时，直接切换到新配置是安全的。Raft根据这种相交的特性，使用很少的额外机制，保证了集群成员变更的安全性。</p>
<p><img src="/img/14RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220419081632386.png" alt="image-20220419081632386"></p>
<p><em>图4.3：分别列出了奇数个节点和偶数个节点的集群中添加&#x2F;删除一个节点的场景。<font color=red>在所有的单节点变更场景中，旧Majority和新Majority都会有交集。</font>比如(b)中，Cold中的Majority集合必须包含3个节点中的2个节点，在Cnew中的Majority集合必须包含3个节点，其中肯定有2个来自于Cold中的配置</em></p>
<p>When adding a single server to a cluster or removing a single server from a cluster, any majority of the old cluster overlaps with any majority of the new cluster; see Figure 4.3. This overlap prevents the cluster from splitting into two independent majorities; in terms of the safety argument of Section 3.6.3, it guarantees the existence of “the voter”. Thus, when adding or removing just a single server, it is safe to switch directly to the new configuration. Raft exploits this property to change cluster membership safely using little additional mechanism.</p>
<p>在复制的日志中，使用特殊条目来保存集群配置，并进行节点间的交互。这就有效利用了Raft中的现有机制来复制和保存配置信息。这也使得进行配置变更时继续处理Client的请求成为了可能（即允许这两种条目同时在Pipeline中进行复制）。</p>
<p>Cluster configurations are stored and communicated using special entries in the replicated log. This leverages the existing mechanisms in Raft to replicate and persist configuration information. It also allows the cluster to continue to service client requests while configuration changes are in progress, by imposing ordering between configuration changes and client requests (while allowing both to be replicated concurrently in a pipeline and&#x2F;or in batches).</p>
<p>当Leader收到请求需要在当前配置（Cold）中增加或删除一个节点时，它会把新配置（Cnew）作为一个日志条目，使用常规的Raft日志复制机制，将其复制到其他节点上。<font color=red>其他节点只要收到该条目，将该条目追加到本地日志后，就认为新配置生效了：Cnew条目只会复制到Cnew的节点中，使用Cnew中的Majority节点来决定Cnew的提交。这就表示节点无需等到Cnew条目提交后才是用新配置，它们直接使用日志中能找到的最新配置。</font></p>
<p>When the leader receives a request to add or remove a server from its current configuration (Cold), it appends the new configuration (Cnew) as an entry in its log and replicates that entry using the normal Raft mechanism. The new configuration takes effect on each server as soon as it is added to that server’s log: the Cnew entry is replicated to the Cnew servers, and a majority of the new configuration is used to determine the Cnew entry’s commitment. This means that servers do not wait for configuration entries to be committed, and each server always uses the latest configuration found in its log.</p>
<p>一旦Cnew条目提交后，就认为本次配置变更完成了。从此刻起，Leader就知道Cnew节点中的过半节点已经采用了Cnew的配置。而且可以明确Cnew之外的节点不可能形成Majority集合了，从而Cnew之外的节点就无法选出Leader了。Cnew条目的提交可以让下面的操作得以继续：</p>
<ol>
<li>Leader可以确认配置变更已经完成；</li>
<li>如果配置变更是移除了某个节点，则该节点可以下线了；</li>
<li><font color=red>可以开始新的配置变更了。如果在此之前进行新的配置变更，</font>则重叠的配置变更可能会导致类似图4.2那样的危险场景(考虑<code>abc -&gt; abcd -&gt; abcde</code>的过程，如果原<code>abc</code>中<code>c</code>为Leader，现在加入<code>d</code>，新配置<code>abcd</code>只复制给了<code>c</code>和<code>d</code>，尚未提交时，又来了新的配置变更，增加<code>e</code>，最新配置<code>abcde</code>复制到了<code>c</code>、<code>d</code>和<code>e</code>，那现在<code>ab</code>使用最老的配置，<code>cde</code>使用最新的配置，一单分区就出现了脑裂)。</li>
</ol>
<p>The configuration change is complete once the Cnew entry is committed. At this point, the leader knows that a majority of the Cnew servers have adopted Cnew. It also knows that any servers that have not moved to Cnew can no longer form a majority of the cluster, and servers without Cnew cannot be elected leader. Commitment of Cnew allows three things to continue:</p>
<ol>
<li>The leader can acknowledge the successful completion of the configuration change.</li>
<li>If the configuration change removed a server, that server can be shut down.</li>
<li>Further configuration changes can be started. Before this point, overlapped configuration changes could degrade to unsafe situations like the one in Figure 4.2.</li>
</ol>
<p><font color=red>Note:</font> 按照之前的描述，节点直接使用日志中最新的配置，而不用等到配置提交后才使用。这可以使得Leader在当前配置未提交时，不会开始新的配置变更，从而避免了配置变更并行的场景（上述的第三点）。只有当Cold中的Majority节点都已经转为Cnew配置后，开始进行新的配置变更才是安全的。如果节点等到Cnew日志提交后才使用Cnew的配置，<font color=red>那Leader是无法感知是否有Majority的节点已经使用Cnew配置了。</font>这就需要Leader去获取其他节点的CommitIndex，其他节点也需要将CommitIndex进行持久化，这两种机制都不是Raft所需的。因此，当节点将Cnew日志追加到日志表中，回复了AppendEntries RPC之后，节点就可以采用Cnew配置了，Leader收到Majority节点关于Cnew日志的AppendEntries RPC的成功回复，就可以将Cnew提交，进而开始下一次的配置变更了。不过，这种机制可能会使得配置日志被删除（当Leader发生变更时），所以节点必须能回退到日志中上一个配置。</p>
<p>As stated above, servers always use the latest configuration in their logs, regardless of whether that configuration entry has been committed. This allows leaders to easily avoid overlapping configuration changes (the third item above), by not beginning a new change until the previous change’s entry has committed. It is only safe to start another membership change once a majority of the old cluster has moved to operating under the rules of Cnew. If servers adopted Cnew only when they learned that Cnew was committed, Raft leaders would have a difficult time knowing when a majority of the old cluster had adopted it. They would need to track which servers know of the entry’s commitment, and the servers would need to persist their commit index to disk; neither of these mechanisms is required in Raft. Instead, each server adopts Cnew as soon as that entry exists in its log, and the leader knows it’s safe to allow further configuration changes as soon as the Cnew entry has been committed. Unfortunately, this decision does imply that a log entry for a configuration change can be removed (if leadership changes); in this case, a server must be prepared to fall back to the previous configuration in its log.</p>
<p><font color=red>Note:</font> 在Raft中，使用调用者（RPC发起者）的配置来实现共识，而非被调用者（RPC接收者）的配置。比如对于投票和日志复制而言：</p>
<ul>
<li>收到RequestVote RPC的节点a，在满足条件的基础上，可以投票给它不认识的节点b（即a节点本地日志中最新配置中之外的节点）。比如，Cold为[a, b, c]三个节点，leader为a，Cnew为[a, b, c, d]，即新增d节点。如果配置变更时a已经将Cnew复制给了a, b, d，而c节点没有收到，此时Cnew也能生效。但接着节点a崩溃了，此时新节点d成为Candidate，它需要b，c的投票，如果c因为其配置中不包含d而拒绝投票的话，d就无法成为leader，降低了集群的可用性。</li>
<li>节点a收到了节点b发来的AppendEntries RPC，即使a不认识节点b，它也会接受该RPC。还是继续上面的例子，新节点d成为新Leader后开始发送AppendEntries RPC，如果c不接受该RPC，则d收到的AppendEntries RPC回复永远无法成为Majority（此时Cnew配置为[a, b, c, d]，a崩溃了，则Majority只能是[b, c, d]），导致集群不可用。</li>
</ul>
<p>因此，节点收到RPC后，无需查询当前的配置信息。</p>
<p>In Raft, it is the caller’s configuration that is used in reaching consensus, both for voting and for log replication:</p>
<ul>
<li>A server accepts AppendEntries requests from a leader that is not part of the server’s latest configuration. Otherwise, a new server could never be added to the cluster (it would never accept any log entries preceding the configuration entry that adds the server).</li>
<li>A server also grants its vote to a candidate that is not part of the server’s latest configuration (if the candidate has a sufficiently up-to-date log and a current term). This vote may occasionally be needed to keep the cluster available. For example, consider adding a fourth server to a three-server cluster. If one server were to fail, the new server’s vote would be needed to form a majority and elect a leader.</li>
</ul>
<p>Thus, servers process incoming RPC requests without consulting their current configurations.</p>
<hr>
<h3 id="4-2-Availability-可用性"><a href="#4-2-Availability-可用性" class="headerlink" title="4.2 Availability     可用性"></a>4.2 Availability     可用性</h3><p>配置变更对于集群可用性，有以下几点影响：</p>
<ul>
<li>新加入集群的节点的日志需要追上Leader（4.2.1）；</li>
<li>Leader被移出怎么办（4.2.2）；</li>
<li>被移出的节点会影响新集群的Leader（4.2.3）；</li>
</ul>
<p>4.2.4节论证为什么我们的配置变更算法足以保证配置变更期间的可用性。</p>
<p>Cluster membership changes introduce several issues in preserving the cluster’s availability. Section 4.2.1 discusses catching up new servers before they’re added to the cluster, so that they do not stall commitment of new log entries; Section 4.2.2 addresses how to phase out an existing leader if it is removed from the cluster; and Section 4.2.3 describes how to prevent removed servers from disrupting the leader of the new cluster. Finally, Section 4.2.4 closes with an argument for why the resulting membership change algorithm is sufficient to preserve availability during any membership change.</p>
<h4 id="4-2-1-Catching-up-new-servers-新节点的日志追赶"><a href="#4-2-1-Catching-up-new-servers-新节点的日志追赶" class="headerlink" title="4.2.1 Catching up new servers     新节点的日志追赶"></a>4.2.1 Catching up new servers     新节点的日志追赶</h4><p>新节点加入集群时，它本身肯定无任何日志，如果它直接成为集群的一员，那他的日志需要很长一段时间才能追上Leader，这段时间内，集群很容易处于不可用的状态。比如，三节点的集群（a, b, c）可以在不损失可用性的情况下，允许一个节点的崩溃。但是如果第四个节点（d）加入到集群中，并且其不包含任何日志，并且原来的节点c崩溃了，则d的日志在追上Leader之前，集群是无法提交任何的新日志的（下图4.4(a)）。另一个例子是：三节点集群（a, b, c），如果现在相继成功加入了三个新节点（d，e，f），并且这三个节点都不包含任何日志，但是因为新集群的Majority至少需要4个节点，在（d，e，f）中任一节点的日志追上Leader之前，集群是不可用的（下图4.4(b)）。</p>
<p>When a server is added to the cluster, it typically will not store any log entries. If it is added to the cluster in this state, its log could take quite a while to catch up to the leader’s, and during this time, the cluster is more vulnerable to unavailability. For example, a three-server cluster can normally tolerate one failure with no loss in availability. However, if a fourth server with an empty log is added to the same cluster and one of the original three servers fails, the cluster will be temporarily unable to commit new entries (see Figure 4.4(a)). Another availability issue can occur if many new servers are added to a cluster in quick succession, where the new servers are needed to form a majority of the cluster (see Figure 4.4(b)). In both cases, until the new servers’ logs were caught up to the leader’s, the clusters would be unavailable.</p>
<img src="/img/03RAFT成员变更(博士论文)/image-20210814100758120.png" alt="image-20210814100758120" style="zoom: 67%;" />

<p>为了避免这种可用性问题，Raft在进行配置变更之前引入了新的阶段，在这个阶段内，新节点会作为非投票成员（non-voting member）加入到集群中。这个节点，Leader会向新节点同步日志，而在投票或者日志复制时，不会把它当做形成Majority的条件。当新节点的日志追上集群后，配置变更才可以继续进行。（这种非投票成员的机制在其他场景中也很有用，比如，将日志复制到大量节点上，在relaxed consistency场景下这些节点可以处理只读请求）。</p>
<p>In order to avoid availability gaps, Raft introduces an additional phase before the configuration change, in which a new server joins the cluster as a non-voting member. The leader replicates log entries to it, but it is not yet counted towards majorities for voting or commitment purposes. Once the new server has caught up with the rest of the cluster, the reconfiguration can proceed as described above. (The mechanism to support non-voting servers can also be useful in other contexts;for example, it can be used to replicate the state to a large number of servers, which can serve readonly<br>requests with relaxed consistency.)</p>
<p>Leader需要判断新节点的日志是否已经足够接近当前集群，从而开始配置变更。这就需要小心对待，因为如果节点太早添加，则根据上面的讨论，集群的可用性就会有风险。<strong>我们的目标是让不可用时间小于选举超时时间</strong>（即日志追赶的时间小于选举超时时间），因为客户必须已经能够容忍这种规模的偶尔不可用期（在领导者失败的情况下）。当然，如果可能的话，我们希望通过使新服务器的日志更接近Leader的日志来进一步减少不可用时间。</p>
<p>The leader needs to determine when a new server is sufficiently caught up to continue with the configuration change. This requires some care to preserve availability: if the server is added too soon, the cluster’s availability may be at risk, as described above. Our goal was to keep any temporary unavailability below an election timeout, since clients must already be able to tolerate occasional unavailability periods of that magnitude (in case of leader failures). Moreover, if possible, we wanted to minimize unavailability further by bringing the new server’s log even closer to the leader’s.</p>
<p>当新节点崩溃，或者复制日志太慢以至于永远追不上的情况下，Leader需要中断配置变更过程。向集群中添加崩溃的节点，或者太慢的节点是一种错误。</p>
<p>The leader should also abort the change if the new server is unavailable or is so slow that it will never catch up. This check is important: Lamport’s ancient Paxos government broke down because they did not include it. They accidentally changed the membership to consist of only drowned sailors and could make no more progress [48]. Attempting to add a server that is unavailable or slow is often a mistake. In fact, our very first configuration change request included a typo in a network port number; the system correctly aborted the change and returned an error.</p>
<p>我们建议使用以下算法来确定新节点的日志是否已经足够接近，从而得以加入到集群中：将复制日志到新服务器分为若干轮（round），如图4.5所示。每一轮都会将round开始时领导日志中的所有日志项复制到新服务器的日志中。在复制本轮日志时，客户端的新日志也会不断的到达Leader；这些新日志将在下一轮中复制。随着复制的进行，每一轮的持续时间会缩短。算法等待固定的轮数（如10轮），如果最后一轮的持续时间少于选举超时时间，假设剩下的的未复制条目不会造成显著的可用性问题，那么领导者会将新服务器添加到集群中。</p>
<p>否则的话，Leader就会中断配置变更过程，并且返回给CLIENT一个错误，CLIENT后续会重试（下次的成功率更高，因为新节点已经有了部分日志了）。</p>
<p>We suggest the following algorithm to determine when a new server is sufficiently caught up to add to the cluster. The replication of entries to the new server is split into rounds, as shown in Figure 4.5. Each round replicates all the log entries present in the leader’s log at the start of the round to the new server’s log. While it is replicating entries for its current round, new entries may arrive at the leader; it will replicate these during the next round. As progress is made, the round durations shrink in time. The algorithm waits a fixed number of rounds (such as 10). If the last round lasts less than an election timeout, then the leader adds the new server to the cluster, under the assumption that there are not enough unreplicated entries to create a significant availability gap.</p>
<p>Otherwise, the leader aborts the configuration change with an error. The caller may always try again (it will be more likely to succeed the next time, since the new server’s log will already be partially caught up).</p>
<p><img src="/img/03RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87)/image-20210814104827516.png" alt="image-20210814104827516"></p>
<p><em>图4.5：为了让新节点能追赶上当前的日志进度，将日志复制到新节点时分成多轮。每轮将该轮开始时包含的日志复制完之后就结束。此时，Leader可能接收到了新的条目，这些条目在下一轮开始复制。</em></p>
<p>为了添加新节点，Leader首先肯定要发现新节点的日志是空的，如果通过原来的AppendEntries回复失败进行检测，直到<code>nextIndex</code>减为1之后才得出日志为空的结论，这样性能太差了。有很多办法可以解决这个问题，最简单的就是新节点回复AppendEntries错误时，附带上自己的日志长度。</p>
<p>As the first step to catching up a new server, the leader must discover that the new server’s log is empty. With a new server, the consistency check in AppendEntries will fail repeatedly until the leader’s nextIndex finally drops to one. This back-and-forth might be the dominant factor in the performance of adding a new server to the cluster (after this phase, log entries can be transmitted to the follower with fewer RPCs by using batching). Various approaches can make nextIndex converge to its correct value more quickly, including those described in Chapter 3. The simplest approach<br>to solving this particular problem of adding a new server, however, is to have followers return the length of their logs in the AppendEntries response; this allows the leader to cap the follower’s nextIndex accordingly.</p>
<h4 id="4-2-2-Removing-the-current-leader-移除当前Leader"><a href="#4-2-2-Removing-the-current-leader-移除当前Leader" class="headerlink" title="4.2.2 Removing the current leader     移除当前Leader"></a>4.2.2 Removing the current leader     移除当前Leader</h4><p>如果Cnew中是需要移除当前的Leader，则该Leader必须在某个时间点进行下线。最直接的办法是第三章的办法：当前Leader首先转移领导权到其他节点，然后新Leader节点进行正常的配置变更。</p>
<p>If the existing leader is asked to remove itself from the cluster, it must step down at some point. One straightforward approach is to use the leadership transfer extension described in Chapter 3: a leader that is asked to remove itself would transfer its leadership to another server, which would then carry out the membership change normally.</p>
<p>我们最初开发了一种不同的方法，即旧配置中的Leader直接进行配置变更，然后再下线。这种方法可能会使集群处于一种尴尬的场景中：Leader管理的配置中不包含自己。之所以最初开发这种方法，是为了配合4.3中介绍的任意配置变更算法（即joint consensus），这种配置变更算法中，新旧配置中可能没有可用于转移领导权的节点。当然这种方法也适用于无法实现领导权转移的其他系统中。</p>
<p>We initially developed a different approach for Raft, in which the existing leader carries out the membership change to remove itself, then it steps down. This puts Raft in a somewhat awkward mode of operation while the leader temporarily manages a configuration in which it is not a member. We initially needed this approach for arbitrary configuration changes (see Section 4.3), where the old configuration and the new configuration might not have any servers in common to which leadership could be transferred. The same approach is also viable for systems that do not implement leadership transfer.</p>
<p>这种方法中，Leader在Cnew提交后就会下线，如果Leader在此之前下线，则它还有可能会选举超时并且再次成为leader，从而拖慢了整体进度。在更极端的2节点场景中，只能是该旧Leader重新成为Leader后集群才得以继续运行，即下图的内容：</p>
<p>In this approach, a leader that is removed from the configuration steps down once the Cnew entry is committed. If the leader stepped down before this point, it might still time out and become leader again, delaying progress. In an extreme case of removing the leader from a two-server cluster, the server might even have to become leader again for the cluster to make progress; see Figure 4.6.</p>
<p><img src="/img/03RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87)/image-20210814122205814.png" alt="image-20210814122205814"></p>
<p><em>图4.6：在Cnew日志项提交之前，被移除的Leader需要继续领导集群。此时S1是Leader，而S2在收到Cnew配置之前它是无法成为新Leader的（因为S2还是旧配置，他就需要S1的投票，而S1不会给他投票，因为它的日志不够新）。</em></p>
<p>因此，Leader需要等到Cnew提交之后才能下线。只有到了这个时间点，新配置才可以在没有被移除的Leader参与的情况下继续运行集群：即Cnew中可以选举出新的Leader。当旧Leader下线之后，Cnew中的某个节点会选举超时，然后赢得选举。这种不可用区间应该是可以接受的，因为它类似于Leader崩溃的场景。</p>
<p>Thus, the leader waits until Cnew is committed to step down. This is the first point when the new configuration can definitely operate without participation from the removed leader: it will always be possible for the members of Cnew to elect a new leader from among themselves. After the removed leader steps down, a server in Cnew will time out and win an election. This small availability gap should be tolerable, since similar availability gaps arise when leaders fail.</p>
<p>这个方法可能会决策（decision-making）导致两种比较奇怪（但是无害）的影响：</p>
<ul>
<li>会有一段时间（在提交Cnew之前），Leader管理的集群不包含自己，它向其他节点复制日志，但是并不把自己算作Majority；</li>
<li>在Cnew提交之前，旧Leader依然可以开始新的选举，即使该节点不存在于自己最新的配置中（比如4.6中的场景，S1如果在Cnew之前崩溃重启了，因S2无法成为新Leader，所以只有S1才能成为Leader），只不过这种情况下它不会投票给自己。</li>
</ul>
<p>This approach leads to two implications about decision-making that are not particularly harmful but may be surprising. First, there will be a period of time (while it is committing Cnew) when a leader can manage a cluster that does not include itself; it replicates log entries but does not count itself in majorities. Second, a server that is not part of its own latest configuration should still start new elections, as it might still be needed until the Cnew entry is committed (as in Figure 4.6). It does not count its own vote in elections unless it is part of its latest configuration.</p>
<h4 id="4-2-3-Disruptive-servers-破坏性节点"><a href="#4-2-3-Disruptive-servers-破坏性节点" class="headerlink" title="4.2.3 Disruptive servers     破坏性节点"></a>4.2.3 Disruptive servers     破坏性节点</h4><p>到现在为止，如果没有额外的机制，不存在于Cnew中的节点实际上会破坏集群的可用性。<strong>当Leader创建好Cnew日志后，Cnew之外的节点就再也收不到心跳包了</strong>，所以该节点最终会超时，并且开始新的选举。因为该节点收不到Cnew日志，也就不会知道它的提交，也认识不到自己已经被移除了。所以该节点会以新的term发送RequestVote RPC，这就导致Leader转为Follower状态。最终Cnew中的某个节点可能会当选，但是破坏性节点还是会超时并重复选举过程，这就对可用性造成了很大影响。如果多个节点被移除的话，这种情况还会加剧。</p>
<p>Without additional mechanism, servers not in Cnew can disrupt the cluster. Once the cluster leader has created the Cnew entry, a server that is not in Cnew will no longer receive heartbeats, so it will time out and start new elections. Furthermore, it will not receive the Cnew entry or learn of that entry’s commitment, so it will not know that it has been removed from the cluster. The server will send RequestVote RPCs with new term numbers, and this will cause the current leader to revert to follower state. A new leader from Cnew will eventually be elected, but the disruptive server will time out again and the process will repeat, resulting in poor availability. If multiple servers have been removed from the cluster, the situation could degrade further.</p>
<p>我们首先想到的解决办法是，节点在开始选举之前，它需要首先检查它是否有机会赢得选举。该方法在选举时引入了一个新的阶段，称为PreVote阶段。一个Candidate在真正的拉票之前，首先会询问其他节点，自己的日志是否与他们的日志一样新。只有当Candidate收到Majority节点的Yes回复之后，他才增加term，开始正式的选举过程。</p>
<p>Our first idea for eliminating disruptions was that, if a server is going to start an election, it would first check that it wouldn’t be wasting everyone’s time—that it had a chance to win the election. This introduced a new phase to elections, called the Pre-Vote phase. A candidate would first ask other servers whether its log was up-to-date enough to get their vote. Only if the candidate believed it could get votes from a majority of the cluster would it increment its term and start a normal election.</p>
<p>然而，这种<font color=red>PreVote方法无法解决这种破坏性节点问题</font>：某些场景下，破坏性节点的日志也是足够新的，比如下图中的例子。一旦Leader创建了Cnew日志后，被移除的节点就已经具有破坏性了。PreVote方法无法解决该问题，因为它的日志可能要跟Majority一样新。（注：尽管PreVote方法无法解决该问题，但是它依然是提升领导者选举健壮性的好方法，参考第九章）</p>
<p>Unfortunately, the Pre-Vote phase does not solve the problem of disruptive servers: there are situations where the disruptive server’s log is sufficiently up-to-date, but starting an election would still be disruptive. Perhaps surprisingly, these can happen even before the configuration change completes. For example, Figure 4.7 shows a server that is being removed from a cluster. Once the leader creates the Cnew log entry, the server being removed could be disruptive. The Pre-Vote check does not help in this case, since the server being removed has a log that is more up-to-date than a majority of either cluster. (Though the Pre-Vote phase does not solve the problem of disruptive servers, it does turn out to be a useful idea for improving the robustness of leader election in general; see Chapter 9.)</p>
<p><img src="/img/03RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87)/image-20210816081740316.png" alt="image-20210816081740316"></p>
<p><em>图4.7：Cnew日志条目提交之前，某个节点可能会干扰集群的可用性，而且PreVote无法解决该问题。上图中，4节点集群中要移除S1，当前Leader为S4，它已经创建好了Cnew日志，但是尚未复制它。即使Cnew提交之前，S1也不会收到Leader发来的心跳了，所以S1选举超时，增加term，发送RequestVote RPC，这就导致S4会转为Follower。这种情况下，PreVote方法没用，因为S1的日志与集群中其他节点一样新。</em></p>
<p>由于这种情况，我们现在认为，仅基于比较日志（如PreVote检查）的解决方案不足以判断选举是否会造成破坏。我们不能要求服务器在开始选举之前检查Cnew中每个服务器的日志，因为Raft必须始终能够容忍故障。我们也不想假设领导者能够可靠地以足够快的速度复制条目，从而快速超越图4.7所示的场景；这在实践中可能是可行的，但这取决于我们更愿意避免的更强有力的假设，即寻找日志发散点的性能以及复制日志条目的性能。</p>
<p>Because of this scenario, we now believe that no solution based on comparing logs alone (such as the Pre-Vote check) will be sufficient to tell if an election will be disruptive. We cannot require a server to check the logs of every server in Cnew before starting an election, since Raft must always be able to tolerate faults. We also did not want to assume that a leader will reliably replicate entries fast enough to move past the scenario shown in Figure 4.7 quickly; that might have worked in practice, but it depends on stronger assumptions that we prefer to avoid about the performance of finding where logs diverge and the performance of replicating log entries.</p>
<p><font color=red>Raft的解决方法是使用心跳来检测有效Leader的存在（租约）</font>。在Raft中，只要Leader能维持与Follower的心跳消息，该Leader就被认为是active的。因此，移除节点就无法破坏当前的leader了。为了使用此方法，我们修改了RequestVote RPC机制：如果节点在当前Leader的最小选举超时时间内收到了RequestVote请求，则它不会更新term，也不会投票。它可以忽略该请求，或者返回一个拒绝投票的结果。这种机制不会影响正常的选举。因为所有节点至少会等到最小选举超时时间后，才会发起新的选举。</p>
<p>Raft’s solution uses heartbeats to determine when a valid leader exists. In Raft, a leader is considered active if it is able to maintain heartbeats to its followers (otherwise, another server will start an election). Thus, servers should not be able to disrupt a leader whose cluster is receiving heartbeats. We modify the RequestVote RPC to achieve this: if a server receives a RequestVote request within the minimum election timeout of hearing from a current leader, it does not update its term or grant its vote. It can either drop the request, reply with a vote denial, or delay the request; the result is essentially the same. This does not affect normal elections, where each server waits at least a minimum election timeout before starting an election. However, it helps avoid disruptions from servers not in Cnew: while a leader is able to get heartbeats to its cluster, it will not be deposed by larger term numbers.</p>
<p><font color=red>Note:</font>这种修改与第三章描述的领导权转移机制有些冲突，转移机制中，节点不会等到选举超时就会开始新的选举，这种情况下，其他节点即使在当前有active Leader的情况下也会处理RequestVote消息。这种冲突可以通过在RequestVote消息中包含特殊标志，表明“我可以取代leader，它告诉我的”。</p>
<p>This change conflicts with the leadership transfer mechanism as described in Chapter 3, in which a server legitimately starts an election without waiting an election timeout. In that case, RequestVote messages should be processed by other servers even when they believe a current cluster leader exists. Those RequestVote requests can include a special flag to indicate this behavior (“I have permission to disrupt the leader—it told me to!”).</p>
<h3 id="4-2-4-Availability-argument-可用性论证"><a href="#4-2-4-Availability-argument-可用性论证" class="headerlink" title="4.2.4 Availability argument     可用性论证"></a>4.2.4 Availability argument     可用性论证</h3><p>这一节我们论证上面的解决方案可以保证配置变更期间的可用性。因为Raft的配置变更时基于Leader的，我们会论证上面的方案会在变更期间维持或替换Leader，并且Leader会处理CLIENT的请求，并完成配置变更。我们假设在这期间，Cold中的Majority是available 的（直到Cnew提交之前），而且Cnew中的Majority也是available的。</p>
<p>This section argues that the above solutions are sufficient to maintain availability during membership changes. Since Raft’s membership changes are leader-based, we show that the algorithm will be able to maintain and replace leaders during membership changes and that the leader(s) will both service client requests and complete the configuration changes. We assume, among other things, that a majority of the old configuration is available (at least until Cnew is committed) and that a majority of the new configuration is available.</p>
<ol>
<li><p>在配置变更的所有节点，总会有一个Leader被选举出来：</p>
<ul>
<li>如果在Cnew中的某个节点具有最新的日志，并且它拥有Cnew日志，它可以获得Cnew的Majority的投票，并且成为Leader；</li>
<li>否则的话，Cnew日志一定尚未提交。此时在所有Cold和Cnew节点中，具有最新日志的节点可以从Cold的Majority，以及Cnew的Majority中获得选票，所以不管该节点使用的是什么配置，它总可以成为Leader；</li>
</ul>
</li>
<li><p>Leader被选举出来之后，只要他的心跳能够发送给其配置中的Follower，其领导权就能得到保障，除非它因为自己不属于Cnew而在Cnew提交后主动下线。</p>
<ul>
<li>只要Leader可以向其配置中的Follower发送心跳，则它自己和其Follower都不会采用更大的term；他们不会选举超时开始新的选举，并且对于附带更高term的RequestVote 消息会视而不见。因此，该Leader不会被迫下线；</li>
<li>如果Leader在提交Cnew后，因自己已经不属于Cnew而下线，Raft将会选出新的Leader。新Leader肯定是Cnew的一员，并且会完成配置变更流程。然而，可能具有较小的可能性，已下线的旧Leader又重新成为了新Leader，如果它确实当选了，则会再次确认Cnew然后接着再次下线，然后Cnew中的某个节点将会在下次选举中获得领导权。</li>
</ul>
</li>
<li><p>Leader在进行配置变更期间还是可以处理客户端发来的请求。</p>
<ul>
<li>Leader可以在整个变更过程中继续将客户机请求附加到其日志中；</li>
<li>因新节点在加入到集群之前可以追上日志进度，所以Leader可以更新其commitIndex，并且及时回复客户端；</li>
</ul>
</li>
<li><p>领导者将通过提交Cnew来完成配置更改，并在必要时下线以允许Cnew中的服务器成为领导者；</p>
</li>
<li><p>A leader can be elected at all steps of the configuration change:</p>
<ul>
<li>If the available server with the most up-to-date log in the new cluster has the Cnew entry, it can collect votes from a majority of Cnew and become leader.</li>
<li>Otherwise, the Cnew entry must not yet be committed. The available server with the most up-to-date log among both the old and new clusters can collect votes from a majority of Cold and a majority of Cnew, so no matter which configuration it uses, it can become leader.</li>
</ul>
</li>
<li><p>A leader is maintained once elected, assuming its heartbeats get through to its configuration, unless it intentionally steps down because it is not in Cnew but has committed Cnew.</p>
<ul>
<li>If a leader can reliably send heartbeats to its own configuration, then neither it nor its followers will adopt a higher term: they will not time out to start any new elections, and they will ignore any RequestVote messages with a higher term from other servers. Thus, the leader will not be forced to step down.</li>
<li>If a server that is not in Cnew commits the Cnew entry and steps down, Raft will then elect a new leader. It is likely that this new leader will be part of Cnew, allowing the configuration change to complete. However, there is some (small) risk that the server that stepped down might become leader again. If it was elected again, it would confirm the commitment of the Cnew entry and soon step down, and it is again likely that a server in Cnew would succeed the next time.</li>
</ul>
</li>
<li><p>The leader(s) will service client requests throughout the configuration change.</p>
<ul>
<li>Leaders can continue to append client requests to their logs throughout the change.</li>
<li>Since new servers are caught up before being added to the cluster, a leader can advance its commit index and reply to clients in a timely manner.</li>
</ul>
</li>
<li><p>The leader(s) will progress towards and complete the configuration change by committing Cnew, and, if necessary, stepping down to allow a server in Cnew to become leader.</p>
</li>
</ol>
<h3 id="4-3-Arbitrary-configuration-changes-using-joint-consensus-使用联合共识算法进行任意配置更改"><a href="#4-3-Arbitrary-configuration-changes-using-joint-consensus-使用联合共识算法进行任意配置更改" class="headerlink" title="4.3 Arbitrary configuration changes using joint consensus     使用联合共识算法进行任意配置更改"></a>4.3 Arbitrary configuration changes using joint consensus     使用联合共识算法进行任意配置更改</h3><p>本节描述的是一种更加复杂的成员变更方法，该方法可以一次性处理任意数量的节点变更。比如2个节点同时加入集群，或者是同时替换集群中所有5个节点。该方法是我们实现的第一种成员变更方法，引入该方法仅仅是为了RAFT算法的完整性。现在我们已经知道了更加简单的单节点成员变更算法，所以我们更加推荐该方法，因为一次性处理任意节点的变更需要额外的复杂性。尽管任意变更通常是本文中假定的成员变更操作方式，但是实际环境中我们不认为这种灵活性是必须的，因为多次的单节点成员变更总是可以实现最终的变更效果。</p>
<p>This section presents a more complex approach to cluster membership changes that handles arbitrary changes to the configuration at one time. For example, two servers can be added to a cluster at once, or all of the servers in a five server cluster can be replaced at once. This was the first approach to membership changes that we came up with, and it is described only for completeness. Now that we know about the simpler single-server approach, we recommend that one instead, since handling arbitrary changes requires extra complexity. Arbitrary changes are typically the way membership changes are assumed to operate in the literature, but we don’t think this flexibility is needed in real systems, where a series of single-server changes can change the cluster membership to any desired configuration.</p>
<p>为了保证任意配置变更期间的安全性，集群首先进入到一种过渡配置状态，我们称之为联合共识（joint consensus），一旦联合共识被提交了，则系统进入到新配置。联合共识既包含Cold，也包含Cnew：</p>
<ul>
<li>Cold-new日志项会被复制到Cold以及Cnew的节点上；</li>
<li>Cold或Cnew中的节点都可以成为leader；</li>
<li>共识（比如选举，日志提交）需要在Cold的Majority，以及Cnew的Majority中同时满足（用于保证不会出现两个Leader）。比如当3节点集群扩展到9节点集群过程中，在Cold中的2节点，以及Cnew中的5节点上，必须同时达到共识才行；</li>
</ul>
<p>To ensure safety across arbitrary configuration changes, the cluster first switches to a transitional configuration we call joint consensus; once the joint consensus has been committed, the system then transitions to the new configuration. The joint consensus combines both the old and new configurations:</p>
<ul>
<li>Log entries are replicated to all servers in both configurations.</li>
<li>Any server from either configuration may serve as leader.</li>
<li>Agreement (for elections and entry commitment) requires separate majorities from both the old and new configurations. For example, when changing from a cluster of 3 servers to a different cluster of 9 servers, agreement requires both 2 of the 3 servers in the old configuration and 5 of the 9 servers in the new configuration.</li>
</ul>
<p>联合共识允许单个服务器在不影响安全性的前提下，在不同的时间在不同的配置之间转换。此外，联合共识允许集群在整个配置更改过程中继续为CLIENT请求提供服务。</p>
<p>The joint consensus allows individual servers to transition between configurations at different times without compromising safety. Furthermore, joint consensus allows the cluster to continue servicing client requests throughout the configuration change.</p>
<p><img src="/img/03RAFT%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%8D%9A%E5%A3%AB%E8%AE%BA%E6%96%87)/image-20210830200406801.png"></p>
<p><em>图4.8：上图展示了联合共识算法的时间线。虚线表示配置项已经创建，但是尚未提交，实线表示最新的已提交的配置项。Leader首先创建Cold,new，然后将其提交到Cold,new（即Cold的Majority，以及Cnew的Majority），然后创建Cnew配置项，并将其提交到Cnew中的Majority。任何阶段，都不会存在Cold和Cnew同时独立决策的场景出现。</em></p>
<p>联合共识方法实际上是通过中间态日志来对单节点变更方法进行的扩展。上图展示了联合共识的过程：Leader收到配置变更请求，要从Cold转为Cnew，Leader将联合共识作为一条日志项（即图中的Cold,new），并将其复制到集群中的节点。就像单节点变更算法一样，节点只要保存了配置日志，就开始将其作为最新的配置项使用。这就意味着Leader将使用Cold,new的配置来决定何时将Cold,new提交。如果Leader挂了，则新Leader可能是Cold中的某节点，也可能是Cold,new中的节点，这取决于该节点是否收到了Cold,new日志。不管是哪种情况，此阶段中Cnew不能单方面做出决定。</p>
<p>This approach extends the single-server membership change algorithm with an intermediate log entry for the joint configuration; Figure 4.8 illustrates the process. When the leader receives a request to change the configuration from Cold to Cnew, it stores the configuration for joint consensus (Cold,new in the figure) as a log entry and replicates that entry using the normal Raft mechanism. As with the single-server configuration change algorithm, each server starts using a new configuration as soon as it stores the configuration in its log. This means that the leader will use the rules of<br>Cold,new to determine when the log entry for Cold,new is committed. If the leader crashes, a new leader may be chosen under either Cold or Cold,new, depending on whether the winning candidate has received Cold,new. In any case, Cnew cannot make unilateral decisions during this period.</p>
<p>一旦Cold,new被提交，则Cold和Cnew都不能在未经对方批准的情况下做出决定，而且Leader完备性保证了只有拥有Cold,new的节点才能选举成功。Cold,new提交后，Leader可以创建Cnew配置，并将其复制到其他节点，只要节点收到并保存了该配置，则节点就使用该配置做决策了。当Cnew日志项在Cnew的Majority中被提交了，则Cold就无关紧要了，不在Cnew中的节点可以关机了。就像上图4.8展示的那样，不会存在Cold和Cnew同时独立决策的情况，这就保证了安全性。</p>
<p>Once Cold,new has been committed, neither Cold nor Cnew can make decisions without approval of the other, and the Leader Completeness Property ensures that only servers with the Cold,new log entry can be elected as leader. It is now safe for the leader to create a log entry describing Cnew and replicate it to the cluster. Again, this configuration will take effect on each server as soon as it is seen. When the Cnew log entry has been committed under the rules of Cnew, the old configuration is irrelevant and servers not in the new configuration can be shut down. As shown in Figure 4.8, there is no time when Cold and Cnew can both make unilateral decisions; this guarantees safety.</p>
<p>联合共识算法可以更加的一般化，从而允许前一个配置更改尚未完成时，开始新的配置变更。但是，实际上这样做没有太大的好处。因此，leader可以在进行配置变更时拒绝新的配置变更请求（上一次的配置变更尚未提交）。</p>
<p>The joint consensus approach could be generalized to allow a configuration change to begin while a prior change was still in progress. However, there would not be much practical advantage to doing this. Instead, a leader rejects additional configuration changes when a configuration change is already in progress (when its latest configuration is not committed or is not a simple majority). Changes that are rejected in this way can simply wait and try again later.</p>
<p>联合共识算法要比单节点变更算法更为复杂，因为它有中间状态。联合共识算法需要更改投票选举和日志提交的决策策略：leader必须检查Cold的Majority节点，和Cnew的Majority节点。</p>
<p>This joint consensus approach is more complex than the single-server changes precisely because it requires transitioning to and from an intermediate configuration. Joint configurations also require changes to how all voting and commitment decisions are made; instead of simply counting servers, the leader must check if the servers form a majority of the old cluster and also form a majority of the new cluster. Implementing this required finding and changing about six comparisons in our Raft implementation [86].</p>
<h3 id="4-4-System-integration"><a href="#4-4-System-integration" class="headerlink" title="4.4 System integration"></a>4.4 System integration</h3><p>Raft的真正实现中，可能会与本章描述的节点变更机制略有不同。</p>
<p>Raft implementations may expose the cluster membership change mechanism described in this chapter in different ways. For example, the AddServer and RemoveServer RPCs in Figure 4.1 can be invoked by administrators directly, or they can be invoked by a script that uses a series of singleserver steps to change the configuration in arbitrary ways.</p>
<p>你可能想自动调用成员变更以响应节点故障等事件。然而，这只能在合理的策略下进行。例如，自动删除发生故障的节点可能是危险的，因为这样可能会留下太少的副本，无法满足预期的容错要求。一种合理的方法是让系统管理员配置所需的集群大小，并在在这种限制下，可用节点可以自动替换出现故障的节点。</p>
<p>It may be desirable to invoke membership changes automatically in response to events like server failures. However, this should only be done according to a reasonable policy. For example, it can be dangerous for the cluster to automatically remove failed servers, as it could then be left with too few replicas to satisfy the intended durability and fault-tolerance requirements. One reasonable approach is to have the system administrator configure a desired cluster size, and within<br>that constraint, available servers could automatically replace failed servers.</p>
<p>在进行需要多次单节点变更时，<font color=red>添加节点最好放在删除节点之前</font>。例如，要替换3节点集群中的某个节点，添加一个节点，然后再删除一个节点，这可以允许系统在整个过程中容忍一个节点故障。但是，如果在添加节点之前先删除了一个节点，系统将暂时无法容忍任何故障（因为两个节点群集要求两节点都可用）。</p>
<p>When making cluster membership changes that require multiple single-server steps, it is preferable to add servers before removing servers. For example, to replace a server in a three-server cluster, adding one server and then removing the other allows the system to handle one server failure at all times throughout the process. However, if one server was first removed before the other was added, the system would temporarily not be able to mask any failures (since two-server clusters require both servers to be available).</p>
<p>节点变更机制可以引入一种新的集群初始化启动的方法。如果没有动态成员变更，每个节点都会有包含配置信息的静态文件。有了动态成员变更机制之后，则不再需要静态配置文件，因为系统会使用Raft日志中的配置；静态配置的方法还更容易出错（例如，应该使用哪种配置初始化新服务器？）。相反，我们建议在第一次创建集群时，单节点被初始化为其日志条目中的第一条就是配置项，该配置项中只包含自己一个节点，这样该节点自己就形成了Majority，所以可以将配置提交。其他节点被初始化为空的日志条目，它们被添加到集群中，并通过节点变更机制了解当前配置。</p>
<p>Membership changes motivate a different approach to bootstrapping a cluster. Without dynamic membership, each server simply has a static file listing the configuration. With dynamic membership changes, the static configuration file is no longer needed, since the system manages configurations in the Raft log; it is also potentially error-prone (e.g., with which configuration should a new server be initialized?). Instead, we recommend that the very first time a cluster is created, one server is initialized with a configuration entry as the first entry in its log. This configuration lists only that one<br>server; it alone forms a majority of its configuration, so it can consider this configuration committed. Other servers from then on should be initialized with empty logs; they are added to the cluster and learn of the current configuration through the membership change mechanism.</p>
<p>成员变更机制也使得一种动态的方法来让客户机找到集群成为必要；第6章对此进行了讨论。</p>
<p>Membership changes also necessitate a dynamic approach for clients to find the cluster; this is discussed in Chapter 6.</p>
<h3 id="4-5-Conclusion-结论"><a href="#4-5-Conclusion-结论" class="headerlink" title="4.5 Conclusion 结论"></a>4.5 Conclusion 结论</h3><p>本章描述了Raft中的成员变更机制，这是一个完整的共识系统中重要的组成部分，因为容错的需求会随着时间的推移而变化，最终需要更换出现故障的节点。</p>
<p>This chapter described an extension to Raft for handling cluster membership changes automatically. This is an important part of a complete consensus-based system, since fault-tolerance requirements can change over time, and failed servers eventually need to be replaced.</p>
<p>共识算法必须从根本上在配置变更中保持安全性，因为新配置会影响“Majority”的含义。本章介绍了一种一次添加或删除单个节点的简单方法。这些操作简单地维护了安全性，因为在更改期间Cnew和Cold集合，Majority至少有一个节点的交集。多个节点的变更可以执行多步单节点变更。在变更期间Raft集群可以继续处理Client发来的请求。</p>
<p>The consensus algorithm must fundamentally be involved in preserving safety across configuration changes, since a new configuration affects the meaning of “majority”. This chapter presented a simple approach that adds or removes a single server at a time. These operations preserve safety simply, since at least one server overlaps any majority during the change. Multiple single-server changes may be composed to modify the cluster more drastically. Raft allows the cluster to continue operating normally during membership changes.</p>
<p>在配置更改期间保持可用性需要处理几个非常重要的问题。特别是，Cnew中排除的节点可能会破坏集群中有效的Leader，在确定基于心跳的解决方案之前，我们在基于日志比较的几个不足的解决方案中苦苦挣扎了很久。</p>
<p>Preserving availability during configuration changes requires handling several non-trivial issues. In particular, the issue of a server not in the new configuration disrupting valid cluster leaders was surprisingly subtle; we struggled with several insufficient solutions based on log comparisons before settling on a working solution based on heartbeats.</p>
<hr>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-03基本Raft算法</title>
    <url>/2021/11/08/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/13%E5%9F%BA%E6%9C%ACRaft%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="Chapter-3-Basic-Raft-algorithm-基本Raft算法"><a href="#Chapter-3-Basic-Raft-algorithm-基本Raft算法" class="headerlink" title="Chapter 3 Basic Raft algorithm     基本Raft算法"></a>Chapter 3 Basic Raft algorithm     基本Raft算法</h2><p>本章介绍基本的Raft算法。我们<font color=red>设计Raft的目标是尽可能的容易理解</font>；第一部分描述了为了容易理解而采取的方法；剩下的部分介绍了算法本身，包括为了便于理解而做出的方案选择。</p>
<p>This chapter presents the Raft algorithm. We designed Raft to be as understandable as possible; the first section describes our approach to designing for understandability. The following sections describe the algorithm itself and include examples of design choices we made for understandability.</p>
<span id="more"></span>
<h3 id="3-1-Designing-for-understandability-为可理解性而设计"><a href="#3-1-Designing-for-understandability-为可理解性而设计" class="headerlink" title="3.1 Designing for understandability     为可理解性而设计"></a>3.1 Designing for understandability     为可理解性而设计</h3><p>我们在设计Raft时有几个目标：它必须为构建实际系统提供一个完整且实用的基础，从而减少开发人员所需的设计工作；它必须保证在所有场景下是安全的，保证典型操作下的可用性，同时兼顾常规操作下的效率；但是我们最重要（也是最大的挑战）的设计目标还是容易理解。另外，需要尽可能的使算法直观，以便开发者可以在实现时进行不可避免的扩展；</p>
<p>We had several goals in designing Raft: it must provide a complete and practical foundation for system building, so that it significantly reduces the amount of design work required of developers; it must be safe under all conditions and available under typical operating conditions; and it must be efficient for common operations. But our most important goal—and most difficult challenge—was understandability. It must be possible for a large audience to understand the algorithm comfortably. In addition, it must be possible to develop intuitions about the algorithm, so that system builders can make the extensions that are inevitable in real-world implementations.</p>
<p>Raft的设计过程中，有很多设计点，都需要在多个备选方案中进行选择。这种情况下，我们根据是否容易理解来评估备选方案。</p>
<p>There were numerous points in the design of Raft where we had to choose among alternative approaches. In these situations we evaluated the alternatives based on understandability: how hard is it to explain each alternative (for example, how complex is its state space, and does it have subtle implications?), and how easy will it be for a reader to completely understand the approach and its implications?</p>
<p>我们认识到这种分析方法太主观了；我们使用两种普遍适用的技术：<font color=red>第一种就是分解问题：可能的话，将问题分为子问题，子问题可以独立的解决、解释和理解。比如我们将Raft的设计分成了领导者选举，日志复制和安全性。</font></p>
<p>We recognize that there is a high degree of subjectivity in such analysis; nonetheless, we used two techniques that are generally applicable. The first technique is the well-known approach of problem decomposition: wherever possible, we divided problems into separate pieces that could be solved, explained, and understood relatively independently. For example, in Raft we separated leader election, log replication, and safety.</p>
<p>第二种方法是通过减少要考虑的状态的数量来简化状态空间，从而使系统更加一致，并且在可能的情况下消除不确定性。具体而言，Raft中的日志必须是连续的，而且Raft限制了节点间日志会出现不一致的行为。虽然多数情况下，我们试图消除不确定性，但某些情况下，不确定性实际上提高了可理解性。比如，随机方法引入了不确定性，但随机方法倾向于以类似的方式来处理所有可能的情况来减少状态空间（任选一种，无所谓）。我们使用了随机方法来简化Leader选举算法。</p>
<p>Our second approach was to simplify the state space by reducing the number of states to consider, making the system more coherent and eliminating nondeterminism where possible. Specifically, logs are not allowed to have holes, and Raft limits the ways in which logs can become inconsistent with each other. Although in most cases we tried to eliminate nondeterminism, there are some situations where nondeterminism actually improves understandability. In particular, randomized approaches introduce nondeterminism, but they tend to reduce the state space by handling all possible choices in a similar fashion (“choose any; it doesn’t matter”). We used randomization to simplify the Raft leader election algorithm.</p>
<h3 id="3-2-Raft-overview-Raft概览"><a href="#3-2-Raft-overview-Raft概览" class="headerlink" title="3.2 Raft overview     Raft概览"></a>3.2 Raft overview     Raft概览</h3><p>Raft是一种用于管理2.1节（下图）所述的复制日志的算法。图3.1以简明形式总结了该算法；图3.2列出了该算法的关键特性；这些关键特性将在本章其余部分逐节讨论。</p>
<p><img src="/img/02%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/image-20220404122954818.png" alt="image-20220404122954818"></p>
<p><em>图2.1：复制状态机架构。共识算法管理包含从Client收到的状态机命令的复制日志。多节点上的状态机以相同的顺序执行日志中的命令，所以他们产生的结果也是相同的。</em></p>
<p>Raft is an algorithm for managing a replicated log of the form described in Section 2.1. Figure 3.1 summarizes the algorithm in condensed form for reference, and Figure 3.2 lists key properties of the algorithm; the elements of these figures are discussed piecewise over the rest of this chapter.</p>
<p>Raft通过选择一个节点作为Leader来实现共识，即由该Leader全权负责管理复制的日志。Leader从Client接收日志条目，然后将日志条目复制到其他节点上，并且告诉其他节点何时可以将日志条目应用到状态机。通过Leader可以简化对复制日志的管理。比如Leader可以无需咨询其他节点，自行决定在日志中放置新条目的位置，数据简单的从Leader流向其他节点。Leader节点也可能崩溃，或者与其他节点断连，这种情况下会选出新的Leader。</p>
<p>Raft implements consensus by first electing a server as leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. Having a leader simplifies the management of the replicated log. For example, the leader can decide where to place new entries in the log without consulting other servers, and data flows in a simple fashion from the leader to other servers. A leader can fail or become disconnected from the other servers, in which case a new leader is elected.</p>
<p>基于Leader的方案，Raft将共识问题分解为三个相对独立的子问题，将在以下小节中讨论：</p>
<ul>
<li>Leader选举：在启动集群和现有Leader失败时，必须选择新的Leader（3.4节）；</li>
<li>日志复制：Leader从Client接收日志条目，并将其复制到集群中的其他节点，强制其他节点的日志与自己的日志保持一致（3.5节）；</li>
<li>安全性：Raft的安全性的关键就是图3.2中列出的<font color=red>状态机的安全性（最终目标，其他的安全性都是为这一条服务的）：如果任一节点已经将特定日志条目应用到状态机了，则其他节点在相同的日志索引上不能应用一个不同的日志条目。</font>3.6节描述了Raft如何保证该安全性，解决方案就是在Leader选举机制上增加额外的限制。</li>
</ul>
<p>Given the leader approach, Raft decomposes the consensus problem into three relatively independent subproblems, which are discussed in the subsections that follow:</p>
<ul>
<li>Leader election: a new leader must be chosen when starting the cluster and when an existing leader fails (Section 3.4).</li>
<li>Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own (Section 3.5).</li>
<li>Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3.2: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index. Section 3.6 describes how Raft ensures this property; the solution involves an additional restriction on the election mechanism described in Section 3.4.</li>
</ul>
<p>在介绍完共识算法之后，本章讨论了系统的可用性，和系统中定时器的作用（3.9节），以及在节点之间转移领导权的可选扩展（3.10节）。</p>
<p>After presenting the consensus algorithm, this chapter discusses the issue of availability and the role of timing in the system (Section 3.9), and an optional extension to transfer leadership between servers (Section 3.10).</p>
<p><img src="/img/02%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/image-20220404085204436.png" alt="image-20220404085204436"></p>
<p><img src="/img/02%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/image-20220404085227703.png" alt="image-20220404085227703"></p>
<p><em>图3.2：Raft保证任何时候都满足下面的<font color=red>安全属性</font>：</em></p>
<ul>
<li><em>选举安全：给定的Term下只会有一个节点当选为Leader（3.4节）；</em></li>
<li><em>Leader只会追加日志：Leader不会覆盖和删除自己的日志，它只会追加日志（3.5节）；</em></li>
<li><em>日志匹配：如果不同节点上的两个日志条目具有相同的Index和Term，那节点的日志中该条目以及之前的日志条目肯定都是相同的（3.5节）；</em></li>
<li><em>Leader完整性：如果某个特定任期内提交了一个日志条目，则该日志条目肯定存在于后续任期的Leader的日志中（3.6节）；</em></li>
<li><em><font color=red>状态机安全性</font>：如果一个节点将某个Index上的日志条目应用到状态机了，那其他节点在相同Index上不会应用不同的日志条目（3.6.3）；</em></li>
</ul>
<hr>
<h3 id="3-3-Raft-basics-基本Raft算法"><a href="#3-3-Raft-basics-基本Raft算法" class="headerlink" title="3.3 Raft basics     基本Raft算法"></a>3.3 Raft basics     基本Raft算法</h3><p>Raft集群包含多个节点，比如5个节点的集群，该集群可以容忍2个节点的故障。任意时间，节点都处于三种状态之一：Leader，Follower，Candidate。正常操作下，只能有一个Leader，其他节点都是Follower。Follower是被动状态，它不会发出请求，只是回应领导人和候选人的请求。Leader处理Client发来的请求（如果Client向Follower发请求，则Follower会把该请求重定向到Leader）。Candidate状态用于选举新的Leader，下面的图3.3展示了这三种状态，以及他们之前的转换流程：</p>
<p><img src="/img/02%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/image-20220404163459728.png" alt="image-20220404163459728"></p>
<p><em>图3.3：节点状态。Follower仅仅是响应其他节点的请求。如果Follower长时间收不到任何消息，则它转为Candidate并开始选举。如果Candidate能从集群中的过半节点上收到选票，则转为Leader。Leader执行操作，直到崩溃。</em></p>
<p>A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader, follower, or candidate. In normal operation there is exactly one leader and all of the other servers are followers. Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates. The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader). The third state, candidate, is used to elect a new leader as described in Section 3.4. Figure 3.3 shows the states and their transitions; the transitions are discussed below.</p>
<p>Raft将时间划分为任意长度的Term（任期），如下图所示。任期以连续的正整数的形式进行编号。每个任期都是从选举开始，即一个或多个Candidate尝试选举为Leader（3.4节）。如果某个Candidate赢得了选举，则该任期内它就是新的Leader。某些场景下，选举时选票可能会被瓜分（没有节点获得过半的选票），那该任期内就没有Leader，新的任期（新的选举）会马上开始。Raft保证在给定任期内最多只有一个Leader。</p>
<p><img src="/img/02%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/image-20220404164052069.png" alt="image-20220404164052069"></p>
<p><em>图3.4：按照任期划分时间，每个任期都是以选举开始。选举成功后，Leader管理集群，直到任期结束。如果选举失败，则这样的任期直到结束时都没能选举出Leader。不同的节点可能在不同的时间观察到任期的转换。</em></p>
<p>Raft divides time into terms of arbitrary length, as shown in Figure 3.4. Terms are numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader as described in Section 3.4. If a candidate wins the election, then it serves as leader for the rest of the term. In some situations an election will result in a split vote. In this case the term will end with no leader; a new term (with a new election) will begin shortly. Raft ensures that there is at most one leader in a given term.</p>
<p>不同的节点可能会在不同的时间观察到任期间的过渡，某些场景下一个节点可能不会观察到选举，甚至观察不到整个任期。实际上Raft中<font color=red>任期起到的是逻辑时钟的作用，通过Term，节点可以检测到过时信息</font>，比如过时的Leader。每个节点都会将Term保存到硬盘中，Term值随着时间单调增加。节点间通信时会附带自己的Term值，如果某个节点的Term小于其他节点的，则该节点会更新自己的Term为更大的值。如果Candidate或Leader发现自己的Term是过时的（小于收到的消息中的Term），那它会立即转为Follower状态。如果节点收到的请求消息中的Term是过期的（消息中的Term小于本地Term），那它会拒绝该请求。</p>
<p>Different servers may observe the transitions between terms at different times, and in some situations a server may not observe an election or even entire terms. Terms act as a logical clock [47] in Raft, and they allow servers to detect obsolete information such as stale leaders. Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one server’s current term is smaller than the other’s, then it updates its current term to the larger value. If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request.</p>
<p>Raft节点使用RPC进行通信，基本的Raft共识算法中，只需要两种RPC：RequestVote RPC是由Candidate在选举时发出（3.4节），AppendEntries则由Leader在复制日志给Follower时发出，同时这种类型的RPC还起到了心跳的作用（3.5节）。领导权转移（3.10节）以及后续章节会引入额外的RPC类型。</p>
<p>Raft servers communicate using remote procedure calls (RPCs), and the basic consensus algorithm requires only two types of RPCs between servers. RequestVote RPCs are initiated by candidates during elections (Section 3.4), and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat (Section 3.5). Leadership transfer (Section 3.10) and the mechanisms described in subsequent chapters introduce additional RPCs beyond the two in the core consensus algorithm.</p>
<p>我们选择RPC作为通信结构是为了简化通信模式。每个请求类型都有相应的响应类型。Raft允许请求RPC和响应RPC在网络中丢失；如果没有及时收到响应消息，则请求者有责任重发请求RPC。节点为了获得高性能会并行发送RPC，而且Raft允许这些RPC请求在网络中的乱序；</p>
<p>We chose to structure communication in Raft as RPCs to simplify its communication patterns. Each request type has a corresponding response type, which also serves as the request’s acknowledgment. Raft assumes RPC requests and responses may be lost in the network; it is the requester’s responsibility to retry the RPC if it does not receive a response in a timely manner. Servers issue RPCs in parallel for best performance, and Raft does not assume the network preserves ordering between RPCs.</p>
<h3 id="3-4-Leader-election-领导者选举"><a href="#3-4-Leader-election-领导者选举" class="headerlink" title="3.4 Leader election     领导者选举"></a>3.4 Leader election     领导者选举</h3><p>当节点启动时，它们都是Follower。节点只要能从Leader或Candidate收到有效的RPC，它就会一直保持Follower状态。Leader周期性的向Follower发送心跳消息来维护自己的领导地位。如果一个Follower在一段时间（称为选举超时时间）内没有收到任何消息，那该Follower就会开始选举流程。</p>
<p>Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers. A server remains in follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader.</p>
<p>开始选举时，Follower增加自己的Term，并转为Candidate状态。然后投票给自己，并向集群中的其他节点并发的发送RequestVote请求。Candidate会一直保持这种状态，直到发送下面三种情况之一：</p>
<ul>
<li>a：它赢得了选举；</li>
<li>b：另一个节点确立了自己的领导者地位；</li>
<li>c：选举超时时间内没有人赢得选举；</li>
</ul>
<p>下面分别讨论：</p>
<p>To begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: (a) it wins the election, (b) another server establishes itself as leader, or (c) another election timeout goes by with no winner. These outcomes are discussed separately in the paragraphs below.</p>
<p>a：如果Candidate在同一任期内可以从集群中获得过半节点的投票，则该Candidate就可以赢得选举。在给定的任期内，每个节点最多只能投票给一个Candidate，先到先得（注：3.6节中增加了投票的额外限制）。需要获取过半节点的投票，这条规则使得特定任期内最多只会有一个Candidate赢得选举（图3.2中的选举安全性）。一旦Candidate赢得选举，它就会转为Leader。然后向所有节点发送心跳消息，以确立起领导者地位，阻止新的选举。</p>
<p>A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server will vote for at most one candidate in a given term, on a first-come-first-served basis (note: Section 3.6 adds an additional restriction on votes). The majority rule ensures that at most one candidate can win the election for a particular term (the Election Safety Property in Figure 3.2). Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.</p>
<p>b：在等待选票期间，Candidate可能会收到另一个声称自己是Leader的节点发来的AppendEntries请求。如果该RPC中附带的Term值大于等于该Candidate的本地Term，那Candidate就会承认该Leader是合法的，并回到Follower的状态。如果RPC中的Term小于该Candidate的本地Term，则Candidate拒绝该RPC，保持Candidate状态不变。</p>
<p>While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If the leader’s term (included in its RPC) is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state.</p>
<p>c：Candidate可能在选举中既不赢，也没输：如果同一时间内有多个Follower转为Candidate开始选举，那选票可能会被这些Candidate瓜分，从而没有Candidate能够获得过半节点的投票。这种情况发生时，每个Candidate都会再次选举超时，增加Term，然后开始新的选举。但是如果不采取额外措施，瓜分选票的情况会无限重复下去。</p>
<p>The third possible outcome is that a candidate neither wins nor loses the election: if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs. However, without extra measures split votes could repeat indefinitely.</p>
<p><font color=red>Raft采用随机的选举超时策略来确保瓜分选票的情况很少发生，即使发生了也能很快解决。</font>为了避免瓜分选票，选举超时时间在固定的时间范围（比如[100ms, 300ms]）内随机选择。这种随机策略会把各个节点的选举超时时刻打散，所以多数情况下只会有一个节点会超时；并且会在其他节点超时之前赢得选举并发送心跳消息。<font color=red>出现瓜分选票的情况是也采取同样的策略，每个Candidate在开始选举时会重置其随机的超时时间，等到超时后才进行下一次选举</font>；这就降低了新的选举中再次出现瓜分选票的可能性。第9章的数据表明这种方法可以很快的选出Leader。</p>
<p>Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly. To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300 ms). This spreads out the servers so that in most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out. The same mechanism is used to handle split votes. Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse before starting the next election; this reduces the likelihood of another split vote in the new election. Chapter 9 shows that this approach elects a leader rapidly.</p>
<p>关于可理解性是如何影响我们在各种设计方案中做出选择的，领导者选举就是一个例子。最初我们计划使用优先级策略：每个Candidate被分配一个唯一的优先级，用于在Candidate出现竞争时进行选择。如果某个Candidate发现其他Candidate具有更高的优先级，那它就会转回Follower状态，所以更高优先级的Candidate就更容易赢得下一次选举。我们发现这种方法在可用性方面有些微妙的问题（当优先级较高的节点失败时，低优先级的节点还是需要超时后转为Candidate，但如果太早超时，这又可能会影响正常选举的进度），我们对算法进行了多次调整，但每次调整后都会出现新的边界情况。最终我们得出结论，随机重试方法更直观，更容易理解。</p>
<p>Elections are an example of how understandability guided our choice between design alternatives. Initially we planned to use a ranking system: each candidate was assigned a unique rank, which was used to select between competing candidates. If a candidate discovered another candidate with higher rank, it would return to follower state so that the higher ranking candidate could more easily win the next election. We found that this approach created subtle issues around availability (a lower-ranked server might need to time out and become a candidate again if a higher-ranked server fails, but if it does so too soon, it can reset progress towards electing a leader).We made adjustments to the algorithm several times, but after each adjustment new corner cases appeared. Eventually we concluded that the randomized retry approach is more obvious and understandable.</p>
<h3 id="3-5-Log-replication-复制日志"><a href="#3-5-Log-replication-复制日志" class="headerlink" title="3.5 Log replication     复制日志"></a>3.5 Log replication     复制日志</h3><p>一旦选出Leader后，它就可以响应Client的请求了。Client的请求中包含了要在复制状态机（the replicated state machine）中执行的命令。Leader将该命令作为新的条目追加到其日志中，然后向所有Follower发送AppendEntries RPC使他们复制该条目。当该条目安全的复制到大多数Follower上之后，Leader将该条目应用到状态机，并且将执行结果回复给Client。如果Follower崩溃或者运行的太慢，或者网络中丢包了，Leader会不断的重发AppendEntries（即使Leader已经回复Client了），直到所有Follower最终都保存了该条目为止。</p>
<p>Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machine. The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. If followers crash or run slowly, or if network packets are lost, the leader retries AppendEntries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries.</p>
<p>日志以下图的形式组织。日志条目中包含一条状态机命令，以及Leader创建该条目时的Term值。该Term用于检查节点间日志是否一致，并且用于保证表3.2中的一些安全属性。每个日志条目还有一个Index，标识其在日志中的位置。</p>
<img src="/img/02基本Raft算法/image-20220405111022732.png" alt="image-20220405111022732" style="zoom:67%;" />

<p><em>图3.5：日志由条目组成，条目按序排号。每个条目都包含了其创建时的任期号（条目中的数字），以及一条状态机命令。如果某个条目能够安全的应用到状态机，则可以认为该条目时已提交的。</em></p>
<p>Logs are organized as shown in Figure 3.5. Each log entry stores a state machine command along with the term number when the entry was received by the leader. The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3.2. Each log entry also has an integer index identifying its position in the log.</p>
<p>Leader决定何时可以安全的将日志条目应用到状态机；这样的条目称为已提交的。<font color=red>Raft保证已提交的条目是持久保存的，并且最终会被所有可用节点的状态机执行。</font>一旦创建日志条目的Leader在过半节点上复制了该条目，就会提交该条目（比如上图中的条目7）。这也会提交Leader日志中之前的所有条目，包括前任Leader创建的条目。3.6节讨论了Leader更换时这种日志提交规则的一些微妙之处，并且也表明了这种对提交的定义是安全的。<font color=red>Leader跟踪记录已提交日志的最大Index，即commitIndex，并且把该Index包含在AppendEntries（包括心跳消息）中，这样集群中的其他节点最终也会知道该Index。一旦Follower知道某条日志是已提交的，那它就可以将该日志应用到本地状态机中（按照日志中的顺序）。</font></p>
<p>The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is committed once the leader that created the entry has replicated it on a majority of the servers (e.g., entry 7 in Figure 3.5). This also commits all preceding entries in the leader’s log, including entries created by previous leaders. Section 3.6 discusses some subtleties when applying this rule after leader changes, and it also shows that this definition of commitment is safe. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out. Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order).</p>
<p>我们设计了Raft的日志机制，以便集群中不同节点上的日志能够保持一致。这种机制简化了系统行为，使其更好预测，更重要的是，这是确保安全性的重要组件。<font color=red>Raft保证下面的属性，这些属性共同构成图3.2中的日志匹配属性</font>：</p>
<ul>
<li>不同节点的日志中，如果两个条目的Index和Term都相同，那它们保存了相同的命令；</li>
<li>不同节点的日志中，如果两个条目的Index和Term都相同，那在它们之前的日志条目也是完全相同的；</li>
</ul>
<p>We designed the Raft log mechanism to maintain a high level of coherency between the logs on different servers. Not only does this simplify the system’s behavior and make it more predictable, but it is an important component of ensuring safety. Raft maintains the following properties, which together constitute the Log Matching Property in Figure 3.2:</p>
<ul>
<li>If two entries in different logs have the same index and term, then they store the same command.</li>
<li>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.</li>
</ul>
<p><font color=red>第一个属性是这样保证的：Leader在特定的Index和特定Term下只会创建一个条目，而且从来不会改变日志中的条目的位置；第二个属性是靠处理AppendEntries RPC时执行的一致性检查来保证的。</font>当Leader发送AppendEntries  RPC时，RPC中包含了preLogIndex和preLogTerm，它们表示Leader本地日志中，位于RPC中包含的新条目之前一个条目的Index和Term。如果Follower在其本地日志中找不到相同的Index和Term，就会拒绝该RPC。一致性检查是一个归纳步骤：初始状态下日志为空，满足日志匹配属性；当日志追加成功时，这种一致性检查会保证满足日志匹配属性。所以，<font color=red>只要AppendEntries成功返回，Leader就知道Follower的日志中，在RPC中的新条目，以及之前的条目，都跟自己是一样的。</font></p>
<p>The first property follows from the fact that a leader creates at most one entry with a given log index in a given term, and log entries never change their position in the log. The second property is guaranteed by a consistency check performed by AppendEntries. When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries. The consistency check acts as an induction step: the initial empty state of the logs satisfies the Log Matching Property, and the consistency check preserves the Log Matching Property whenever logs are extended. As a result, whenever AppendEntries returns successfully, the leader knows that the follower’s log is identical to its own log up through the new entries.</p>
<p>正常运行时，Leader和Follower的日志保持一致，所以AppendEntries的一致性检查不会失败。但是，一旦Leader崩溃就可能导致日志不一致（旧Leader可能还没有完全复制其日志中的所有条目）。这种不一致会随着一系列的Leader和Follower的崩溃而加剧。下图展示了Follower与新Leader的日志不一致时的场景。Follower可能没有Leader中存在的一些条目，可能有一些Leader没有的条目，或者两者都有。日志中的缺失条目和无关条目可能会跨越多个Term。</p>
<p><img src="/img/02%E5%9F%BA%E6%9C%ACRaft%E7%AE%97%E6%B3%95/image-20220405161430640.png" alt="image-20220405161430640"></p>
<p><em>图3.6：当顶端的Leader掌权时，Follower中的日志可能出现(a-f)的任何情况。上图条目中的数字表示Term。Follower可能缺少一些条目（a-b），可能包含额外的未提交条目（c-d），或者两者都有（e-f）。比如，<code>f</code>可能是这样发生的：该节点在Term 2时是Leader，向其本地日志中添加了若干条目，如果在提交它们之前崩溃了；然后很快重启了，在Term 3时又成了Leader，有添加了若干日志，在Term 2和Term 3的条目被提交之前，该节点又崩溃了，并且保持崩溃状态到数个任期之久。</em></p>
<p>During normal operation, the logs of the leader and followers stay consistent, so the AppendEntries consistency check never fails. However, leader crashes can leave the logs inconsistent (the old leader may not have fully replicated all of the entries in its log). These inconsistencies can compound over a series of leader and follower crashes. Figure 3.6 illustrates the ways in which followers’ logs may differ from that of a new leader. A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both. Missing and extraneous entries in a log may span multiple terms.</p>
<p><font color=red>Raft中，处理这种不一致的方式，是Leader强迫Follower复制自己的日志。这表示Follower日志中的冲突条目会被Leader中的日志条目覆盖。</font>3.6节会说明，如果加上一些选举时的限制，这种处理方式是安全的。</p>
<p>In Raft, the leader handles inconsistencies by forcing the followers’ logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log. Section 3.6 will show that this is safe when coupled with a restriction on elections.</p>
<p><font color=red>为了使Follower的日志与自己的保持一致，Leader必须找到两个节点日志中一致的Index最大的条目，删除Follower日志中该条目之后的所有日志，然后将自己日志中该条目之后的所有条目发给Follower。这些动作发生在AppendEntries的一致性检查过程中。</font>Leader为每个Follower维护一个nextIndex，表示Leader会发送给Follower的下一条日志条目的Index。Leader掌权时，会把所有nextIndex初始化为其本地日志中最后一个条目的Index加1（图3.6中的11）。如果某个Follower的日志与Leader不一致，那AppendEntries的一致性检查就会失败，Follower对该AppendEntries回复“拒绝”后，Leader减少该Follower的nextIndex的值，重发AppendEntries RPC。最终nextIndex会减少到Leader和Follower的日志匹配的点。在这之后，AppendEntries就会成功，从而删除Follower中冲突的日志条目，并把Leader的日志追加到Follower中。<font color=red>一旦AppendEntries成功后，Follower的日志就与Leader保持一致了，并在当前任期内一直保持一致。</font></p>
<p>To bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point. All of these actions happen in response to the consistency check performed by AppendEntries RPCs. The leader maintains a nextIndex for each follower, which is the index of the next log entry the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log (11 in Figure 3.6). If a follower’s log is inconsistent with the leader’s, the AppendEntries consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements the follower’s nextIndex and retries the AppendEntries RPC. Eventually the nextIndex will reach a point where the leader and follower logs match. When this happens, AppendEntries will succeed, which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any). Once AppendEntries succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term.</p>
<p>为了节省带宽，在发现与Follower的日志匹配点之前，Leader可以发送没有条目的AppendEntries RPC。然后一旦nextIndex紧跟在matchIndex之后时，说明找到了匹配点，Leader才开发发送实际的条目。</p>
<p>Until the leader has discovered where it and the follower’s logs match, the leader can send AppendEntries with no entries (like heartbeats) to save bandwidth. Then, once the matchIndex immediately precedes the nextIndex, the leader should begin to send the actual entries.</p>
<p>如果需要优化的话，实际上可以减少拒绝AppendEntries RPC的数量。比如当拒绝AppendEntries时，Follower可以在响应报文中包含冲突条目的Term值，以及该Term下的第一个条目的Index。有了这些信息，Leader可以直接减少nextIndex来跳过该Term下所有的冲突条目；这样，冲突条目的每个Term仅需要一个AppendEntries RPC，而不是每个条目一个RPC。或者，Leader可以使用二分查找法找到Follower日志与本地日志不一致的第一个条目。但是在实际场景中，我们怀疑这种优化是否是有必要的，因为很少发生失败，并且不太可能有太多的不一致条目。</p>
<p>If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs. For example, when rejecting an AppendEntries request, the follower can include the term of the conflicting entry and the first index it stores for that term. With this information, the leader can decrement nextIndex to bypass all of the conflicting entries in that term; one AppendEntries RPC will be required for each term with conflicting entries, rather than one RPC per entry. Alternatively, the leader can use a binary search approach to find the first entry where the follower’s log differs from its own; this has better worst-case behavior. In practice, we doubt these optimizations are necessary, since failures happen infrequently and it is unlikely that there will be many inconsistent entries.</p>
<p>有了这种机制，Leader在掌权时就不需要其他任何特殊措施来恢复日志的一致性。它只需要按照正常的操作，并且在一致性检查失败时，日志会自动聚合。Leader从来不会覆盖或删除自己的日志（图3.2中的Leader Append-Only属性）。</p>
<p>With this mechanism, a leader does not need to take any special actions to restore log consistency when it comes to power. It just begins normal operation, and the logs automatically converge in response to failures of the AppendEntries consistency check. A leader never overwrites or deletes entries in its own log (the Leader Append-Only Property in Figure 3.2).</p>
<p>这种日志复制机制展示了2.1节中描述的理想的一致性属性：只要大多数节点都活着，Raft就可以接受、复制和应用新的日志条目；正常情况下，一个新条目可以通过一轮RPC就复制到集群中的大多数节点；并且一个较慢的Follower不会影响集群性能。这种日志复制算法也很实用，因为AppendEntries请求的报文大小是可以控制的（Leader不用为了取得进展而在单个AppendEntries请求中发送多个条目）。其他的共识算法的描述中，一般需要发送整个日志，这给实现着带来了优化上的负担。</p>
<p>This log replication mechanism exhibits the desirable consensus properties described in Section 2.1: Raft can accept, replicate, and apply new log entries as long as a majority of the servers are up; in the normal case a new entry can be replicated with a single round of RPCs to a majority of the cluster; and a single slow follower will not impact performance. The log replication algorithm is also practical to implement, since AppendEntries requests are manageable in size (leaders never need to send more than one entry in a single AppendEntries request to make progress). Some other consensus algorithms are described as sending entire logs over the network; this places a burden on the implementer to develop optimizations required for a practical implementation.</p>
<h3 id="3-6-Safety-安全性"><a href="#3-6-Safety-安全性" class="headerlink" title="3.6 Safety     安全性"></a>3.6 Safety     安全性</h3><p>前几节描述了Raft如何选举Leader并且如何复制日志。然而到目前为止，<font color=red>上面的机制不足以保证每个节点上的状态机按照相同的顺序执行相同的命令。</font>比如当Leader提交若干日志条目时，某个Follower可能是不可用的，然后该Follower可能被选为Leader，并用新条目覆盖这些已提交的日志条目；这样就造成了不同节点上的状态机执行了不同的命令序列。</p>
<p>The previous sections described how Raft elects leaders and replicates log entries. However, the mechanisms described so far are not quite sufficient to ensure that each state machine executes exactly the same commands in the same order. For example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones; as a result, different state machines might execute different command sequences.</p>
<p>本节通过<font color=red>对领导者选举增加限制来完善Raft算法。这种限制保证了特定任期下被选为Leader的节点包含所有前任已提交的日志（图3.2中的Leader完整性）</font>。基于这种选举约束，我们更加精确的制定了日志提交的规则。最后，我们给出了Leader完整性的证明，并展示了它如何正确引导了复制状态机的行为。</p>
<p>This section completes the Raft algorithm by adding a restriction on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries committed in previous terms (the Leader Completeness Property from Figure 3.2). Given the election restriction, we then make the rules for commitment more precise. Finally, we present a proof sketch for the Leader Completeness Property and show how it leads to correct behavior of the replicated state machine.</p>
<h4 id="3-6-1-Election-restriction-选举约束"><a href="#3-6-1-Election-restriction-选举约束" class="headerlink" title="3.6.1 Election restriction     选举约束"></a>3.6.1 Election restriction     选举约束</h4><p>在任何基于Leader的共识算法中，Leader最终必须包含所有已提交的日志条目。一些共识算法中，比如Viewstamped Replication算法，即使节点没有包含所有已提交的条目，该节点依然可以被选为Leader。这些算法会有额外的机制来识别缺少的条目，并在选举过程中，或者选举后不久将其复制给新Leader。然而这带来了额外的复杂性。<font color=red>Raft采用简单的方法来保证选举时新Leader在当选时就有之前任期的所有已提交的条目，而无需将这些条目再传输给新Leader。这就意味着日志条目只会从Leader流向Follower，是单向的，而Leader永远不会覆盖日志中的条目。</font></p>
<p>In any leader-based consensus algorithm, the leader must eventually store all of the committed log entries. In some consensus algorithms, such as Viewstamped Replication [66], a leader can be elected even if it doesn’t initially contain all of the committed entries. These algorithms contain additional mechanisms to identify the missing entries and transmit them to the new leader, either during the election process or shortly afterwards. Unfortunately, this results in considerable additional mechanism and complexity. Raft uses a simpler approach where it guarantees that all the committed entries from previous terms are present on each new leader from the moment of its election, without the need to transfer those entries to the leader. This means that log entries only flow in one direction, from leaders to followers, and leaders never overwrite existing entries in their logs.</p>
<p><font color=red>Raft在投票过程中增加限制：只有Candidate的日志包含了所有已提交条目，该Candidate才会当选。</font>在选举时Candidate需要与集群中过半节点通信，这就表示其中至少有一个节点包含所有已提交的条目。如果Candidate的日志至少与过半节点的日志一样新（下面定义了“新”的概念），那就能保证该Candidate包含了所有已提交的条目。在RequestVote RPC实现了该限制：RPC中包含了Candidate的日志信息，投票者判断如果自己的日志比Candidate新，则拒绝投票。</p>
<p>Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries. A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers. If the candidate’s log is at least as up-to-date as any other log in that majority (where “up-to-date” is defined precisely below), then it will hold all the committed entries. The RequestVote RPC implements this restriction: the RPC includes information about the candidate’s log, and the voter denies its vote if its own log is more up-to-date than that of the candidate.</p>
<p><font color=red>Raft通过比较日志中最后一个条目的Index和Term来确定哪个日志更“新”。</font>如果两份日志的最后条目的Term不同，则具有更大Term的日志更新；如果Term一样，则Index更大的更新；</p>
<p>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.</p>
<p><font color=red>个人理解，基于这个“新”的定义，并不足以保证当选的Leader包含了所有已提交的条目，某些场景下还是会有问题，所以才有了3.6.2中的提交限制。</font></p>
<h4 id="3-6-2-Committing-entries-from-previous-terms-之前任期的日志条目的提交"><a href="#3-6-2-Committing-entries-from-previous-terms-之前任期的日志条目的提交" class="headerlink" title="3.6.2 Committing entries from previous terms     之前任期的日志条目的提交"></a>3.6.2 Committing entries from previous terms     之前任期的日志条目的提交</h4><p>如3.5节中所述，Leader知道如果一个当前任期创建条目在内复制到了过半节点上，那就可以提交该条目。<font color=red>如果Leader在提交条目之前崩溃了，那后续的Leader会尝试继续完成该条目的复制。然而，如果某个之前任期创建的条目已经复制到过半节点上了，Leader也不能立即将其提交。</font>下图描述了这种场景，即使某个之前任期创建的条目已经复制到过半节点上了，它依然可能被后续的Leader所覆盖。</p>
<img src="/img/02基本Raft算法/image-20220406190047133.png" alt="image-20220406190047133" style="zoom:67%;" />

<p><em>图3.7：Leader为何不能直接提交之前任期创建的条目：(a)S1是Leader，把Index2_Term2的条目复制到了部分节点上；(b)S1崩溃了，S3以Term 3赢得了S3、S4以及自己的选票，成为了Leader，并且创建了Index2_Term3的条目；(c)S5崩溃了，S1重启了，以Term 4成为新Leader，继续复制日志，并把Index2_Term2的条目复制到了过半节点上。现在分两种情况：(d1)如果S1没有复制新条目，则Index2_Term2的条目就是S1、S2、S3的最后一个条目，此时S1崩溃了，S5的日志比较新，又重新当选了，继续复制Index2_Term3的条目，它就覆盖了S1、S2和S3上的Index2_Term2条目；(d2)如果S1在崩溃之前以自己的Term复制并提交了新条目，即Index3_Term4，则S5不会当选，并且Index3_Term4之前的Index2_Term2也顺便被提交了。</em></p>
<p>As described in Section 3.5, a leader knows that an entry from its current term is committed once that entry is stored on a majority of the servers. If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry. However, a leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers. Figure 3.7 illustrates a situation where an old log entry is stored on a majority of servers, yet can still be overwritten by a future leader.</p>
<p>上图中，<font color=red>如果在(d1)场景下S1直接提交了Index2_Term2条目，则S5当选后用Index3_Term4覆盖了已提交的条目，这违反了图3.2中的状态机安全性，造成了不同节点的状态机应用了不同的日志条目。为了避免出现这种情况，Raft不能仅通过计算副本数来提交之前任期创建的条目。针对当前任期创建的条目，通过计算副本数进行提交才是安全可行的，根据日志匹配属性，一旦当前任期中的条目以这种方法（计算副本数）提交了，那之前的条目也就间接的提交了。</font>某些场景下，Leader也能安全的推导出一个旧条目是已提交的（比如该条目已经复制到所有节点上了），但是为了简单起见Raft采用了更保守的方法。</p>
<p>To eliminate problems like the one in Figure 3.7, Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity.</p>
<p>这里Raft的提交规则变复杂了，因为Leader复制之前任期创建的条目时，这些条目保留了其原始Term。其他共识算法中，如果新Leader复制以前任期中的条目时，必须使用新的Term号。Raft的方法更容易对日志条目进行推理，因为这些条目始终保留的是相同的原始Term。另外，Raft中的新Leader比其他算法发送的条目更少，因为其他算法需要发送冗余的条目，以便在提交之前对他们进行重新编号。然而实际情况中这可能不重要，因为Leader变更很少发生。</p>
<p>Raft incurs this extra complexity in the commitment rules because log entries retain their original term numbers when a leader replicates entries from previous terms. In other consensus algorithms, if a new leader re-replicates entries from prior “terms”, it must do so with its new “term number”. Raft’s approach makes it easier to reason about log entries, since they maintain the same term number over time and across logs. In addition, new leaders in Raft send fewer log entries from previous terms than in other algorithms, since other algorithms must send redundant log entries to renumber them before they can be committed; however, this may not be very important in practice, since leader changes should be rare.</p>
<h4 id="3-6-3-Safety-argument-安全性论证"><a href="#3-6-3-Safety-argument-安全性论证" class="headerlink" title="3.6.3 Safety argument     安全性论证"></a>3.6.3 Safety argument     安全性论证</h4><p><img src="/img/02%E5%9F%BA%E6%9C%ACRaft%E7%AE%97%E6%B3%95/image-20220409101718861.png" alt="image-20220409101718861"></p>
<p><em>图3.8：如果S1（LeaderT）在其任期内提交了一条新条目，S5在任期U时当选为Leader，那至少有一个节点（S3），既接受了S1创建的那个条目，又投票给了S5.</em></p>
<p>基于上面完整的Raft算法，我们现在可以更精确地论证图3.2中的Leader完整性了。我们使用反证法，假设Leader完整性不成立，然后推导出矛盾点：假设在Term T下的Leader（即LeaderT）在其任期内提交了一个日志条目，但是该条目并没有复制到某个未来Leader上。假设U是满足U&gt;T，并且LeaderU没有保存该条目的最小Term。</p>
<ol>
<li><p>那个已提交的条目在LeaderU当选时肯定就不在该节点上（因为日志只能从Leader流向Follower，Leader也从不删除或覆盖条目）；</p>
</li>
<li><p>LeaderT将该条目复制到了过半节点上，而LeaderU也获得了过半节点的投票。因此，至少有一个节点（投票者）既接受了LeaderT的条目，也投票给了LeaderU，如上图3.8所示。该节点是推导出矛盾的关键；</p>
</li>
<li><p>投票者肯定在投票给LeaderU之前，就接受了LeaderT发来的条目；否则它会拒绝LeaderT的AppendEntries 请求（如果先投票给了LeaderU，则其自己的Term也就变的比T大了）；</p>
</li>
<li><p>投票者在投票给LeaderU时肯定仍然保留着该条目，因为LeaderT和LeaderU之间的Leader都包含该条目（假设条件），Leader从不删除条目，而Follower只会在于Leader冲突时才删除条目；</p>
</li>
<li><p>投票者投票给了LeaderU，因此LeaderU的日志必须跟该投票者一样新，这就导致了下面的矛盾点：</p>
</li>
<li><p>首先，如果投票者和LeaderU最后日志条目的Term值一样，那LeaderU的日志至少要跟该投票者一样长，所以LeaderU的日志包含了投票者所有条目（假设）。这是一个矛盾点，因为投票者包含了已提交的条目，但是LeaderU并没有；</p>
</li>
<li><p>否则，那LeaderU的最后条目的Term必须必投票者的大，因此也就大于T，因为投票者最后的日志条目至少为T（因为它包含了任期T中的提交条目）。那构造LeaderU最后条目的前任Leader肯定也包含了该条目（假设条件）。所以根据日志匹配属性，LeaderU的日志中也必须包含该已提交的条目，这又是一个矛盾点。</p>
<p><font color=red>个人理解：第6条和第7条都可以这样论证：投票者既然投票给LeaderU，说明LeaderU的日志比投票者新，所以LeaderU中最后日志条目的Term大于等于投票者的最后日志条目的Term，并且一定是大于等于T的。所以，LeaderU中的最后日志条目，一定来源于LeaderT或其后的Leader，而这些Leader肯定包含了那个已提交的条目（假设条件），Leader从不删除或覆盖条目，并且通过一致性检查保证Follower的日志与自己一样，那就说明LeaderU一定包含了那个已提交条目，这就得出了矛盾。</font></p>
</li>
<li><p>这就完成了推导。所以，任期T之后的所有Leader肯定包含了所有任期T中已提交的日志。</p>
</li>
<li><p>日志匹配属性保证了未来的Leader也具有间接提交的条目，比如图3.7中(d2)中的Index2_Term2。</p>
</li>
</ol>
<p>Given the complete Raft algorithm, we can now argue more precisely that the Leader Completeness Property holds (this argument is based on the safety proof; see Chapter 8). We assume that the Leader Completeness Property does not hold, then we prove a contradiction. Suppose the leader for term T (leaderT) commits a log entry from its term, but that log entry is not stored by the leader of some future term. Consider the smallest term U &gt; T whose leader (leaderU) does not store the entry.</p>
<ol>
<li>The committed entry must have been absent from leaderU’s log at the time of its election </li>
<li>leaderT replicated the entry on a majority of the cluster, and leaderU received votes from a majority of the cluster. Thus, at least one server (“the voter”) both accepted the entry from leaderT and voted for leaderU, as shown in Figure 3.8. The voter is key to reaching a contradiction.</li>
<li>The voter must have accepted the committed entry from leaderT before voting for leaderU; otherwise it would have rejected the AppendEntries request from leaderT (its current term would have been higher than T).</li>
<li>The voter still stored the entry when it voted for leaderU, since every intervening leader contained the entry (by assumption), leaders never remove entries, and followers only remove entries if they conflict with the leader.</li>
<li>The voter granted its vote to leaderU, so leaderU’s log must have been as up-to-date as the voter’s. This leads to one of two contradictions.</li>
<li>First, if the voter and leaderU shared the same last log term, then leaderU’s log must have been at least as long as the voter’s, so its log contained every entry in the voter’s log. This is a contradiction, since the voter contained the committed entry and leaderU was assumed not to.</li>
<li>Otherwise, leaderU’s last log term must have been larger than the voter’s. Moreover, it was larger than T, since the voter’s last log term was at least T (it contains the committed entry from term T). The earlier leader that created leaderU’s last log entry must have contained the committed entry in its log (by assumption). Then, by the Log Matching Property, leaderU’s log must also contain the committed entry, which is a contradiction.</li>
<li>This completes the contradiction. Thus, the leaders of all terms greater than T must contain all entries from term T that are committed in term T.</li>
<li>The Log Matching Property guarantees that future leaders will also contain entries that are committed indirectly, such as index 2 in Figure 3.7(d2).</li>
</ol>
<p>基于Leader完整性，我们也能证明图3.2中的状态机安全性。该特性的意思就是如果某个节点已经将给定Index上的条目应用到状态机上，那其他节点在相同Index条目上不会应用不同的条目。当节点应用该条目时，那它的日志在该条目之前肯定与Leader的一样，而且该条目肯定是已提交的。现在考虑任意节点上应用给定Index时的最小Term；Leader 完整性保证拥有更⾼Term的Leader在该Index上会存储相同的⽇志条⽬，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全性是成⽴的。</p>
<p>Given the Leader Completeness Property, we can prove the State Machine Safety Property from Figure 3.2, which states that if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. At the time a server applies a log entry to its state machine, its log must be identical to the leader’s log up through that entry, and the entry must be committed. Now consider the lowest term in which any server applies a given log index; the Leader Completeness Property guarantees that the leaders for all higher terms will store that same log entry, so servers that apply the index in later terms will apply the same value. Thus, the State Machine Safety Property holds.</p>
<p>最后，Raft要求节点按日志索引的顺序应用条目。结合状态机安全性，这就意味着所有节点都会以相同的顺序将完全相同的一组日志条目应用到其状态机上。</p>
<p>Finally, Raft requires servers to apply entries in log index order. Combined with the State Machine Safety Property, this means that all servers will apply exactly the same set of log entries to their state machines, in the same order.</p>
<h3 id="3-7-Follower-and-candidate-crashes-Follower和Candidate崩溃"><a href="#3-7-Follower-and-candidate-crashes-Follower和Candidate崩溃" class="headerlink" title="3.7 Follower and candidate crashes      Follower和Candidate崩溃"></a>3.7 Follower and candidate crashes      Follower和Candidate崩溃</h3><p>目前为止我们只关注了Leader崩溃的情况。Follower 和 Candidate 崩溃后的处理方式比 Leader 崩溃要简单的多，并且两者的处理方式是相同的。如果 Follower 或 Candidate 崩溃了（或者它们与Leader断连了），那么后续发送给他们的 RequestVote和AppendEntries RPCs都会失败。Raft通过不断的重试来处理这种失败；如果崩溃的节点重启了，那么这些RPC最终总会成功。如果一个节点在处理完RPC，但在响应之前崩溃了，那么在它重启之后就会再次收到同样的请求。<font color=red>Raft的RPC都是幂等的</font>，所以这样的重试不会造成任何伤害。例如，Follower 如果收到 AppendEntries 请求但是它的日志中已经包含了这些条目，它就会直接忽略该RPC。</p>
<p>Until this point we have focused on leader failures. Follower and candidate crashes are much simpler to handle than leader crashes, and they are both handled in the same way. If a follower or candidate crashes (or the network link between it and the leader fails), then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely; if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs<br>have the same effect if repeated, so this causes no harm. For example, if a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.</p>
<h3 id="3-8-Persisted-state-and-server-restarts-持久化状态以及节点重启"><a href="#3-8-Persisted-state-and-server-restarts-持久化状态以及节点重启" class="headerlink" title="3.8 Persisted state and server restarts      持久化状态以及节点重启"></a>3.8 Persisted state and server restarts      持久化状态以及节点重启</h3><p>Raft集群中的节点必须将足够的信息保存到持久化存储中，才能保证节点安全的重启。特别是，<font color=red>每个节点必须要持久化保存当前的Term以及投票</font>，这样才能防止节点在同一Term内投票两次，或者防止来自于新Leader的日志条目被已罢免Leader的日志条目所替换。<font color=red>每个节点还必须在计算日志条目副本数以便决定是否提交之前将其持久化保存，这可以防止节点重启时已提交的条目丢失或未提交。</font></p>
<p>Raft servers must persist enough information to stable storage to survive server restarts safely. In particular, each server persists its current term and vote; this is necessary to prevent the server from voting twice in the same term or replacing log entries from a newer leader with those from a deposed leader. Each server also persists new log entries before they are counted towards the entries’ commitment; this prevents committed entries from being lost or “uncommitted” when servers restart.</p>
<p>其他状态都是可以在崩溃时丢弃的，因为他们可以重新计算得到。最有趣的是committIndex，实际上该Index在重启时是可以置为0的。即使所有节点在同一时间都重启了，committIndex也只是短时间内落后于其真实值。一旦一个Leader当选，并且能提交一个新条目之后，它的committIndex就会递增，并很快的将其传播给所有Follower。</p>
<p>Other state variables are safe to lose on a restart, as they can all be recreated. The most interesting example is the commit index, which can safely be reinitialized to zero on a restart. Even if every server restarts at the same time, the commit index will only temporarily lag behind its true value. Once a leader is elected and is able to commit a new entry, its commit index will advance, and it will quickly propagate this commit index to its followers.</p>
<p><font color=red>状态机本身可以是易失的，也可以是持久性的。易失性的状态机重启后必须重新应用保存的日志条目才能恢复；而持久性的状态机，重启时（应该是崩溃时）已经应用了大部分日志条目了，为了避免再次应用，那AppliedIndex必须持久保存；</font></p>
<p>The state machine can either be volatile or persistent. A volatile state machine must be recovered after restarts by reapplying log entries (after applying the latest snapshot; see Chapter 5). A persistent state machine, however, has already applied most entries after a restart; to avoid reapplying them, its last applied index must also be persistent.</p>
<p>如果节点丢失了任何持久化状态，则该节点就无法再安全地以之前的身份重新加入集群 。这样的节点可以通过集群成员变更（第4章）以新的身份重新回到集群。如果集群中国版节点都丢失了持久化状态，日志可能会丢失，在通过集群成员变更就不可行了，要想继续的话，系统管理员要认识到数据丢失的可能性。</p>
<p>If a server loses any of its persistent state, it cannot safely rejoin the cluster with its prior identity. Such a server can usually be added back into the cluster with a new identity by invoking a cluster membership change (see Chapter 4). If a majority of the cluster loses its persistent state, however, log entries may be lost and progress on cluster membership changes will not be possible; to proceed, a system administrator would need to admit the possibility of data loss.</p>
<h3 id="3-9-Timing-and-availability-时间和可用性"><a href="#3-9-Timing-and-availability-时间和可用性" class="headerlink" title="3.9 Timing and availability     时间和可用性"></a>3.9 Timing and availability     时间和可用性</h3><p>Raft的要求之一就是安全性不能依赖时间：<font color=red>系统不能因为某些事件运行得比预期快一点或者慢一点就产生错误的结果。</font>但是，可用性（系统能够及时响应客户端的能力）不可避免的要依赖于时间。例如，当有节点崩溃时，消息交互的时间就会比正常情况下长，Candidate 将无法等待太长时间来赢得选举；如果没有一个稳定的leader，Raft将无法工作。</p>
<p>One of our requirements for Raft is that safety must not depend on timing: the system must not produce incorrect results just because some event happens more quickly or slowly than expected. However, availability (the ability of the system to respond to clients in a timely manner) must inevitably depend on timing. For example, if message exchanges take longer than the typical time between server crashes, candidates will not stay up long enough to win an election; without a steady leader, Raft cannot make progress.</p>
<p><font color=red>Leader 选举是Raft中时间最为关键的方面。整个系统要满足下面的时间要求，Raft才可以选举出并维持一个稳定的Leader： <code>broadcastTime（消息广播时间） ＜＜ electionTimeout（选举超时时间） ＜＜ MTBF（平均故障间隔时间）</code>；</font></p>
<p>Leader election is the aspect of Raft where timing is most critical. Raft will be able to elect and maintain a steady leader when the system satisfies the following timing requirement: <code>broadcastTime ＜＜ electionTimeout ＜＜ MTBF</code></p>
<p>在这个不等式中，消息广播时间指的是一个节点并行地发送RPC给集群中所有的其他节点并接收到响应的平均时间；选举超时时间就是在3.4节中介绍的选举超时时间；平均故障间隔时间就是对于一台服务器而言，两次故障间隔时间的平均值。消息广播时间必须比选举超时时间小一个量级，这样Leader才能够可靠地发送心跳消息来阻止 Follower 开始进入选举状态；考虑到随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间需要比平均故障间隔时间小上几个数量级，这样系统才能稳步推进。<font color=red>当Leader崩溃后，整个系统会有大约选举超时时间不可用；我们希望该情况在整个时间里只占一小部分。</font></p>
<p>In this inequality broadcastTime is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses; electionTimeout is the election timeout described in Section 3.4; and MTBF is the mean (average) time between failures for a single server. The broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbeat messages required to keep followers from starting elections; given the randomized approach used for election timeouts, this inequality also makes split votes unlikely. The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress. When the leader crashes, the system will be unavailable for roughly the election timeout; we would like this to represent only a small fraction of overall time.</p>
<p>消息广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft的RPCs需要接收方将信息持久化保存，所以广播时间大约是0.5毫秒到20毫秒之间，取决于存储的技术。因此，选举超时时间可能需要在10毫秒到500毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的要求。第9章更详细地探讨了如何设置选举超时及其对可用性和Leader选举效率的影响。</p>
<p>The broadcast time and MTBF are properties of the underlying system, while the election timeout is something we must choose. Raft’s RPCs typically require the recipient to persist information to stable storage, so the broadcast time may range from 0:5–20 ms, depending on storage technology. As a result, the election timeout is likely to be somewhere between 10–500 ms. Typical server MTBFs are several months or more, which easily satisfies the timing requirement. Chapter 9 explores how to set the election timeout and its impact on availability and leader election performance in more detail.</p>
<h3 id="3-10-Leadership-transfer-extension-领导权转移"><a href="#3-10-Leadership-transfer-extension-领导权转移" class="headerlink" title="3.10 Leadership transfer extension     领导权转移"></a>3.10 Leadership transfer extension     领导权转移</h3><p>有时候需要将Leader的领导权转移到其他节点，比如下面的场景：</p>
<ol>
<li>Leader必须下线，比如需要维护，或者是从集群中删除。如果Leader直接下线，则集群至少会等一个选举超时的时间，才会有Follower选举成新的Leader，这期间集群是不可用的。这种不可用可以通过让Leader在下台之前将其领导权转移到另一个节点来避免。</li>
<li>某些情况下，可能有更适合当Leader的节点。比如为了较少Client和Leader之间的延迟，可能需要选择主数据中心的节点作为Leader。其他共识算法中，可能在选举期间就考虑这些因素，但是RAFT只需要节点的日志足够新就能成为Leader，但是选出的Leader不一定就是最合适的。所以，RAFT中的Leader可以定期检查是否有更合适作为Leader的Follower，并进行领导权的转移（如果人类领导人能如此优雅就好了）。</li>
</ol>
<p>This section describes an optional extension to Raft that allows one server to transfer its leadership to another. Leadership transfer could be useful in two types of situations:</p>
<ol>
<li>Sometimes the leader must step down. For example, it may need to reboot for maintenance, or it may be removed from the cluster (see Chapter 4). When it steps down, the cluster will be idle for an election timeout until another server times out and wins an election. This brief unavailability can be avoided by having the leader transfer its leadership to another server before it steps down.</li>
<li>In some cases, one or more servers may be more suitable to lead the cluster than others. For example, a server with high load would not make a good leader, or in a WAN deployment, servers in a primary datacenter may be preferred in order to minimize the latency between clients and the leader. Other consensus algorithms may be able to accommodate these preferences during leader election, but Raft needs a server with a sufficiently up-to-date log to become leader, which might not be the most preferred one. Instead, a leader in Raft can periodically check to see whether one of its available followers would be more suitable, and if so, transfer its leadership to that server. (If only human leaders were so graceful.)</li>
</ol>
<p>RAFT中实现领导权转移，前任Leader将日志发给目标节点，<font color=red>目标节点不等到选举超时就开始选举流程。</font>这样前任Leader保证目标节点拥有其任期开始到现在的所有日志，然后通过常规的选举流程，Majority节点的投票就保证了安全性。下面是详细过程：</p>
<ol>
<li>前任Leader停止接收Client发来的请求；</li>
<li>前任Leader同步日志给目标节点，保证其日志跟自己的一样；</li>
<li>前任Leader给目标节点发一个TimeoutNow消息，收到这个消息与选举计时器超时有相同的效果，目标节点开始选举，增加其Term并成为Candidate；</li>
</ol>
<p>To transfer leadership in Raft, the prior leader sends its log entries to the target server, then the target server runs an election without waiting for an election timeout to elapse. The prior leader thus ensures that the target server has all committed entries at the start of its term, and, as in normal elections, the majority voting guarantees the safety properties (such as the Leader Completeness Property) are maintained. The following steps describe the process in more detail:</p>
<ol>
<li>The prior leader stops accepting new client requests.</li>
<li>The prior leader fully updates the target server’s log to match its own, using the normal log replication mechanism described in Section 3.5.</li>
<li>The prior leader sends a TimeoutNow request to the target server. This request has the same effect as the target server’s election timer firing: the target server starts a new election (incrementing its term and becoming a candidate).</li>
</ol>
<p>目标节点一旦收到了TimeoutNow消息，则该节点增加Term开始选举，从而比其他节点更容易的成为新Leader。新Leader的消息中带有增加后的Term，这就会让前任Leader自动下线，这样就完成了领导权的转移。</p>
<p>Once the target server receives the TimeoutNow request, it is highly likely to start an election before any other server and become leader in the next term. Its next message to the prior leader will include its new term number, causing the prior leader to step down. At this point, leadership transfer is complete.</p>
<p>目标节点也可能失败，这种情况下，集群必须能恢复处理客户端的请求。如果领导权转移在一个选举超时内还没有完成，那前任Leader会终止转移，并恢复处理Client的请求。</p>
<p>It is also possible for the target server to fail; in this case, the cluster must resume client operations. If leadership transfer does not complete after about an election timeout, the prior leader aborts the transfer and resumes accepting client requests. If the prior leader was mistaken and the target server is actually operational, then at worst this mistake will result in an extra election, after which client operations will be restored.</p>
<p>领导权转移的方法，在RAFT集群的常规过渡中保证了安全性。比如<font color=red>Raft已经保证了，即使节点时钟以任意速度运行也可以保证安全性，当目标节点收到TimeoutNow请求时，相当于目标节点的时钟发生了跳变，所以这是安全的。</font></p>
<p>This approach preserves safety by operating within the normal transitions of a Raft cluster. For example, Raft already guarantees safety even when clocks run at arbitrary speeds; when the target server receives a TimeoutNow request, it is equivalent to the target server’s clock jumping forwards quickly, which is safe. However, we have not currently implemented or evaluated this leadership transfer approach.</p>
<h3 id="3-11-Conclusion-总结"><a href="#3-11-Conclusion-总结" class="headerlink" title="3.11 Conclusion      总结"></a>3.11 Conclusion      总结</h3><p>本章讨论了基于共识的系统的所有核心问题。Raft不仅实现了单个值的共识（就像single-decree Paxos那样），它在一系列不断增长的命令日志上都实现了共识，这是构建复制状态机所必须的。它还包括达成共识后的信息传播，以便其他节点能够知道哪些哪些日志是已提交的。Raft通过选举出Leader来单方面决策，并在其掌权时只发送必要的日志条目，从而以实用且高效的方式实现共识。我们已经在LogCabin中实现了Raft，LogCabin是一个复制状态机，在第10章会介绍。</p>
<p>This chapter addressed all the core problems for a consensus-based system. Raft goes beyond reaching consensus on a single value, as in single-decree Paxos; it achieves consensus on a growing log of commands, which is needed to build a replicated state machine. It also includes disseminates information once agreement has been reached, so that other servers learn the log entries that have been committed. Raft achieves consensus in a practical and efficient way by electing a cluster leader to unilaterally make decisions and transmitting only the necessary log entries when a new leader comes to power. We have implemented the ideas of Raft in LogCabin, a replicated state machine (described in Chapter 10).</p>
<p>Raft仅使用少量的机制来解决完全共识问题。比如，它只使用了两种RPC（RequestVote和AppendEntries）。说来奇怪，构造一个紧凑的算法&#x2F;实现并非Raft的明确目标，相反，这是我们为可理解性而设计的结果，设计中的每一个机制都必须有充分的原因和解释。我们发现冗余或迂回的机制很难解释，所以在设计过程中自然会被摒弃。</p>
<p>Raft uses only a small amount of mechanism to address the full consensus problem. For example, it uses only two RPCs (RequestVote and AppendEntries). Perhaps surprisingly, creating a compact algorithm&#x2F;implementation was not an explicit goal for Raft. Rather, it is a result of our design for understandability, where every bit of mechanism must be fully motivated and explained. We found that redundant or meandering mechanism is hard to motivate, so it naturally gets purged in the design process.</p>
<p>除非我们确信某个问题会影响Raft的大部分功能，否则我们不会在Raft上解决它。所以Raft的某些部分可能看起来比较幼稚。比如Raft中的节点是通过等待选举超时来判断选票被瓜分的情况；原则上，我们通常可以通过对所有Candidate的得票进行计数，来更快的发现甚至是解决选票瓜分问题。我们决定不对该问题进行优化，因为它增加复杂度，但可能不会带来实际的好处：在配置良好的部署中，瓜分选票是很罕见的。Raft的其他部分可能显得过于保守，比如Leader只能直接提交其当前任期内的条目，即使某些特殊场景下它可以安全的提交之前任期的条目。使用更复杂的提交规则可能不利于可理解性，而且在性能上不会有太大提升；按照目前的规则，提交可能会有短暂的延迟。在于其他人讨论Raft时，我们发现很多人忍不住想要提出这样那样的优化，但是当以可理解性为目标时，我们决定不应该多早优化。</p>
<p>Unless we felt confident that a particular problem would affect a large fraction of Raft deployments, we did not address it in Raft. As a result, parts of Raft may appear naive. For example, servers in Raft detect a split vote by waiting for an election timeout; in principle, they could often detect and even resolve split votes sooner by counting the votes granted to any candidate. We chose not to develop this optimization for Raft, since it adds complexity but probably brings no practical benefit: split votes are rare in a well-configured deployment. Other parts of Raft may appear overly conservative. For example, a leader only directly commits an entry from its current term, even though in some special cases it could safely commit entries from prior terms. Applying a more complex commitment rule would harm understandability and would not have a significant effect on performance; commitment is only delayed briefly with the current rule. In discussing Raft with others, we found that many people cannot help but think of such optimizations and propose them, but when the goal is understandability, premature optimizations should be left out.</p>
<p>本章不可避免的遗漏了一些实践中有用的特性或优化。随着开发者的Raft开发经验的增长，他们终将会了解到某些功能特性何时以及为何有用，并且需要在实际的开发中实现这些特性。本章中我们描述了一些可能不是必要的，但有助于指导开发者的扩展特性。通过聚焦可理解性，我们希望已经为开发者根据自己的经验来调整Raft提供了坚实的基础。因为Raft目前运行在我们的实验环境中，我们希望这些只是直接的扩展，而非根本性的更改。</p>
<p>Inevitably, this chapter might have left out some features or optimizations that turn out to be useful in practice. As implementers gain more experience with Raft, they will learn when and why certain additional features may be useful, and they may need to implement these for some practical deployments. Throughout the chapter, we sketched a few optional extensions that we currently think are unnecessary but that may help guide implementers should the need arise. By focusing on understandability, we hope to have provided a solid foundation for implementers to adjust Raft according to their experiences. Since Raft works in our testing environment, we expect these to be straightforward extensions rather than fundamental changes.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-06与Client的交互</title>
    <url>/2022/02/12/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/16%E4%B8%8EClient%E7%9A%84%E4%BA%A4%E4%BA%92/</url>
    <content><![CDATA[<h2 id="Chapter-6-Client-interaction-与Client的交互"><a href="#Chapter-6-Client-interaction-与Client的交互" class="headerlink" title="Chapter 6  Client interaction     与Client的交互"></a>Chapter 6  Client interaction     与Client的交互</h2><p>本章描述了几个客户端如何与基于 Raft 的复制状态机交互的问题：</p>
<ul>
<li><p>6.1节描述Client如何找到集群，即使是集群成员会随时变化的情况下；</p>
</li>
<li><p>6.2节描述Client请求如何路由到集群的Leader节点；</p>
</li>
<li><p>6.3节描述Raft如何提供线性一致性的；</p>
</li>
<li><p>6.4节描述Raft如何更高效的处理只读请求的；</p>
<span id="more"></span>
<p>This chapter describes several issues in how clients interact with a Raft-based replicated state machine:</p>
</li>
<li><p>Section 6.1 describes how clients find the cluster, even when its set of members can change over time;</p>
</li>
<li><p>Section 6.2 describes how clients’ requests are routed to the cluster leader for processing;</p>
</li>
<li><p>Section 6.3 describes how Raft provides linearizable consistency [34]; and</p>
</li>
<li><p>Section 6.4 describes how Raft can process read-only queries more efficiently.</p>
</li>
</ul>
<p>下图展示了Client与复制状态机交互使用的RPC；本章将讨论RPC中的各个元素。这些问题适用于所有基于共识的系统，并且 Raft 的解决方案与其他系统也是类似的。</p>
<p><img src="/img/16%E4%B8%8EClient%E7%9A%84%E4%BA%A4%E4%BA%92(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220502102118437.png" alt="image-20220502102118437"></p>
<p>Figure 6.1 shows the RPCs that clients use to interact with the replicated state machine; the elements of these RPCs are discussed throughout the chapter. These issues apply to all consensus-based systems, and Raft’s solutions are similar to other systems.</p>
<p>本章假设基于Raft的复制状态机对外是以网络服务的形式暴露给Client的。实际上Raft也可以直接整合到Client的应用程序中，这种情况下，与Client的交互相关问题会被提升到嵌入在应用程序中的网络客户端的层次。比如，嵌入于应用程序的网络客户端同样面临着和 Raft 作为网络服务时一样的，如何发现集群这样的问题。</p>
<p>This chapter assumes that the Raft-based replicated state machine is exposed to clients directly as a network service. Raft can alternatively be integrated directly into a client application. In this case, some issues in client interaction may be pushed up a level to network clients of the embedding application. For example, network clients of the embedding application would have a similar problem in finding the application’s cluster as clients of a Raft network service have in finding the Raft cluster.</p>
<h3 id="6-1-Finding-the-cluster-如何找到集群"><a href="#6-1-Finding-the-cluster-如何找到集群" class="headerlink" title="6.1 Finding the cluster     如何找到集群"></a>6.1 Finding the cluster     如何找到集群</h3><p>当Raft对外暴露为网络服务时，Client必须能够定位到集群才能与复制状态机交互。对于有固定成员的集群而言，该问题的解决方法很简单，比如服务器的网络地址可以在配置文件中静态配置。然而对于集群成员随时可能发生变化的场景，如何找到集群就变得有些复杂了。一般有下面两种方法：</p>
<ol>
<li>Client可以使用广播或者多播来找到集群的所有节点。但是这种方法只在支持广播或多播的特定环境中有效。</li>
<li>Client可以通过外部目录服务（如DNS）来发现集群的节点，该服务可在众所周知的位置访问。这种外部系统中记录的节点列表不需要完全一致，但是应该是其超集：Client应该始终都能在列表中找到所有集群节点，但是列表中包含一些非集群节点也是无关紧要的。因此，应该在成员变更之前更新外部目录的节点列表，以包含即将添加到集群中的新节点，然后在成员变更之后，再次更新列表，以便删除那些不属于集群的节点。</li>
</ol>
<p>When Raft is exposed as a network service, clients must locate the cluster in order to interact with the replicated state machine. For clusters with fixed membership, this is straightforward; for example, the network addresses of the servers can be stored statically in a configuration file. However, finding the cluster when its set of servers can change over time (as described in Chapter 4) is a bigger challenge. There are two general approaches:</p>
<ol>
<li>Clients can use network broadcast or multicast to find all cluster servers. However, this will only work in particular environments that support these features.</li>
<li>Clients can discover cluster servers via an external directory service, such as DNS, that is accessible at a well-known location. The list of servers in this external system need not be consistent, but it should be inclusive: clients should always be able to find all of the cluster servers, but including a few additional servers that are not currently members of the cluster is harmless. Thus, during cluster membership changes, the external directory of servers should be updated before the membership change to include any servers soon to be added to the cluster, then updated again after the membership change is complete to remove any servers that are no longer part of the cluster.</li>
</ol>
<p>LogCabin的Client目前使用DNS来发现集群。但是LogCabin不会在节点变更前后自动更新DNS中的记录（这留给管理脚本去做）。</p>
<p>LogCabin clients currently use DNS to find the cluster. LogCabin does not currently update DNS records automatically before and after membership changes (this is left to administrative scripts).</p>
<h3 id="6-2-Routing-requests-to-the-leader-将请求路由到Leader"><a href="#6-2-Routing-requests-to-the-leader-将请求路由到Leader" class="headerlink" title="6.2 Routing requests to the leader     将请求路由到Leader"></a>6.2 Routing requests to the leader     将请求路由到Leader</h3><p>Client的请求都是由Raft中的Leader处理的，因此Client需要能找到Leader。当Client启动时，它会与集群中任一节点取得联系。如果该节点不是Leader，则其拒绝Client的请求。这种情况下，一种简单的处理方式就是Client继续随机的联系下一个节点，直到找到Leader。如果Client单纯的随机选择节点，则对于含有<code>n</code>个节点集群，这种方法有望在<code>(n+1)/2</code>次尝试之后找到Leader，对于小型集群而言还是很快的。</p>
<p>Client requests in Raft are processed through the leader, so clients need a way to find the leader. When a client first starts up, it connects to a randomly chosen server. If the client’s first choice is not the leader, that server rejects the request. In this case, a very simple approach is for the client to try again with another randomly chosen server until it finds the leader. If clients choose servers randomly without replacement, this naive approach is expected to find the leader of an n-server cluster after (n+1)&#x2F;2  attempts, which may be fast enough for small clusters.</p>
<p>通过简单的优化，将请求路由到Leader也是很快的。集群中的节点一般都会知道Leader的地址，因为AppendEntries请求中包含了Leader的身份ID。当集群中的非Leader节点收到了Client的请求时，它可以有两种选择：</p>
<ol>
<li>第一种，节点拒绝Client的请求，并返回Leader的地址。我们建议选择这种方式，LogCabin也实现了这种方式。这种方式使得Client可以直接重连到Leader，所以后续的请求可以全速的发送到Leader。这种方式几乎不需要修改代码，因为在Leader失效时Client也需要重连新的Leader。</li>
<li>另一种是，节点可以将请求转发给Leader。这在某些情况下是更简单的方法。比如，如果Client向任意节点发送读请求（6.4节描述），则转发写请求有助于避免Client维护一个专门用于写请求的连接。</li>
</ol>
<p>Routing requests to the leader can also be made faster with simple optimizations. Servers usually know the address of the current cluster leader, since AppendEntries requests include the leader’s identity. When a server that is not leader receives a request from a client, it can do one of two things:</p>
<ol>
<li>The first option, which we recommend and which LogCabin implements, is for the server to reject the request and return to the client the address of the leader, if known. This allows the client to reconnect to the leader directly, so future requests can proceed at full speed. It also takes very little additional code to implement, since clients already need to reconnect to a different server in the event of a leader failure.</li>
<li>Alternatively, the server can proxy the client’s request to the leader. This may be simpler in some cases. For example, if a client connects to any server for read requests (see Section 6.4), then proxying the client’s write requests would save the client from having to manage a distinct connection to the leader used only for writes.</li>
</ol>
<p>Raft需要避免过时的Leader信息导致的Client请求被无限期的延迟处理。Leader信息可能在整个系统中，即在Leader，Follower和Client中都会过期：</p>
<ul>
<li>Leader：节点可能自认为是Leader，但是实际上它不是，这种情况就有可能造成Client请求非必要的延迟。比如，Leader可能已经被分区隔离了，但它仍可以跟特定的Client进行通信。如果没有其他机制，该隔离的Leader因无法将日志复制到其他节点上，导致该Client的请求永久的阻塞。同时期间可能会有新的Leader可以与集群中的过半节点通信，从而可以提交Client的请求。所以，<font color=red>如果Leader在选举超时时间内无法完成一轮心跳的话，Leader会自动下台。</font>这样就使得Client可以将请求发给另一个节点。</li>
<li>Follower：Follower需要持续跟踪记录Leader的身份信息，以便能将转发Client请求。它们必须在开始新的选举，或者是Term变化时丢弃该信息。否则的话，它们也可能会不必要地延迟客户端请求（比如两个节点可能会彼此重定向，导致Client请求陷入到死循环）。</li>
<li>Client：如果Client与Leader（或者其他节点）断连了，则它应该随机的选择一个节点进行重试。如果Client只认准一个节点进行不断的重试，则也会导致不必要的延迟。</li>
</ul>
<p>Raft must also prevent stale leadership information from delaying client requests indefinitely. Leadership information can become stale all across the system, in leaders, followers, and clients:</p>
<ul>
<li>Leaders: A server might be in the leader state, but if it isn’t the current leader, it could be needlessly delaying client requests. For example, suppose a leader is partitioned from the rest of the cluster, but it can still communicate with a particular client. Without additional mechanism, it could delay a request from that client forever, being unable to replicate a log entry to any other servers. Meanwhile, there might be another leader of a newer term that is able to communicate with a majority of the cluster and would be able to commit the client’s request. Thus, a leader in Raft steps down if an election timeout elapses without a successful round of heartbeats to a majority of its cluster; this allows clients to retry their requests with another server.</li>
<li>Followers: Followers keep track of the leader’s identity so that they can redirect or proxy clients. They must discard this information when starting a new election or when the term changes. Otherwise, they might needlessly delay clients (for example, it would be possible for two servers to redirect to each other, placing clients in an infinite loop).</li>
<li>Clients: If a client loses its connection to the leader (or any particular server), it should simply retry with a random server. Insisting on being able to contact the last known leader would result in unnecessary delays if that server failed.</li>
</ul>
<h3 id="6-3-Implementing-linearizable-semantics-实现线性一致性语义"><a href="#6-3-Implementing-linearizable-semantics-实现线性一致性语义" class="headerlink" title="6.3 Implementing linearizable semantics     实现线性一致性语义"></a>6.3 Implementing linearizable semantics     实现线性一致性语义</h3><p>到目前为止，Raft为Client提供的是<code>at-least-once</code>语义；复制状态机可能会多次执行同一条指令。比如，Client向Leader提交了一个指令，Leader将指令追加到集群的日志中，并且提交了该条目，但是该Leader在回复Client之前崩溃了。因Client没有得到回复，它向新Leader重新提交了该指令，这就导致该指令作为新的条目再次追加到日志中并提交。尽管Client想要执行一次该指令，但是实际上该指令执行了两次。即使Client没有重新提交该指令，该指令还是可能会执行多次，因为网络中可能会复制Client的请求报文。</p>
<p>As described so far, Raft provides at-least-once semantics for clients; the replicated state machine may apply a command multiple times. For example, suppose a client submits a command to a leader and the leader appends the command to its log and commits the log entry, but then it crashes before responding to the client. Since the client receives no acknowledgment, it resubmits the command to the new leader, which in turn appends the command as a new entry in its log and also commits this new entry. Although the client intended for the command to be executed once, it is executed twice. Commands can also be applied multiple times even without the client’s involvement if the network may duplicate the client’s requests.</p>
<p>该问题不是Raft独有的，这在大多数有状态的分布式系统中都会发生。然而，这种<code>at-least-once</code>语义特别不适合基于共识的系统，这种系统中，Client通常需要比<code>at-least-once</code>更强的保证。来自重复指令的问题可能以微妙的方式表现出来，导致Client很难从中恢复。这些问题要么导致错误的结果，要么导致错误的状态，或者同时导致两种情况。下图是一个导致错误结果的例子：一个提供锁服务的系统，Client发现它无法获取锁，因为它没有收到之前已经获取了该锁的任何确认消息。导致错误状态的一个例子是递增操作，Client希望只递增一次，但是实际上可能递增了多次。网络上的包乱序，以及多个并发的Client请求可能会导致更奇怪的结果。</p>
<p><img src="/img/16%E4%B8%8EClient%E7%9A%84%E4%BA%A4%E4%BA%92(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220503091754401.png" alt="image-20220503091754401"></p>
<p><em>图6.2：重复指令导致错误结果的例子：Client向复制状态机提交一个用于获取锁的指令。实际上Client第一个指令已经获取了锁，但是却没有收到回复消息。当Client重发该请求时，它发现锁已经是锁住的状态了。</em></p>
<p>This issue is not unique to Raft; it occurs in most stateful distributed systems. However, these at-least-once semantics are particularly unsuitable for a consensus-based system, where clients typically need stronger guarantees. Problems from duplicated commands can manifest in subtle ways that are difficult for clients to recover from. These problems cause either incorrect results, incorrect states, or both. Figure 6.2 shows an example of an incorrect result: a state machine is providing a lock, and a client finds it is unable to acquire the lock because its original request—for which it received no acknowledgment—has already acquired the lock. An example of an incorrect state would be an increment operation, where the client intends for a value to increment by one but it instead increments by two or more. Network-level reordering and concurrent clients can lead to even more surprising results.</p>
<p>Raft中，我们的目标是实现线性一致性语义，从而避免此类问题。<font color=red>在线性化系统中，每个操作似乎在调起和响应之间的某个时刻上立即执行了一次。</font>这是一种很强的一致性形式，使得Client更容易理解，并且不允许一条指令被执行多次。</p>
<p>Our goal in Raft is to implement linearizable semantics [34], which avoid these classes of problems. In linearizability, each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response. This is a strong form of consistency that is simple for clients to reason about, and it disallows commands being processed multiple times.</p>
<p>为了在Raft中实现线性一致性，节点必须过滤掉重复的请求。一个最基本的想法就是节点保存Client请求的结果，并使用该结果避免同一个请求的多次执行。为了实现这一点，每个Client都被赋予了唯一的标识，而且针对Client的每个请求指令都被赋予了唯一的序列号。状态机为每个Client维持了一个会话。通过会话跟踪记录该Client的最后的序列号，以及相应的响应。如果节点收到的指令序列号表示该指令已经执行过了，则节点直接响应Client，不用再次执行指令。</p>
<p>To achieve linearizability in Raft, servers must filter out duplicate requests. The basic idea is that servers save the results of client operations and use them to skip executing the same request multiple times. To implement this, each client is given a unique identifier, and clients assign unique serial numbers to every command. Each server’s state machine maintains a session for each client. The session tracks the latest serial number processed for the client, along with the associated response. If a server receives a command whose serial number has already been executed, it responds immediately without re-executing the request.</p>
<p>基于这种过滤重复请求的机制，Raft保证了线性一致性。针对应用到每个节点的指令，Raft的日志机制已经保证了指令执行的串行顺序。当指令在日志中第一次出现并被应用时，该指令就即刻生效了，并且只执行了一次，因为后续该指令再次出现时都被状态机过滤掉了。</p>
<p>Given this filtering of duplicate requests, Raft provides linearizability. The Raft log provides a serial order in which commands are applied on every server. Commands take effect instantaneously and exactly once according to their first appearance in the Raft log, since any subsequent appearances are filtered out by the state machines as described above.</p>
<p>该方法还可以泛化为允许单个Client的多个请求并行执行。通过Client会话不仅仅是跟踪记录Client最后的请求序列号及其响应，它可以是包含序列号和响应对的集合。Client发起的每个请求都包含了其尚未收到响应的最小的序列号，状态机将丢弃所有与之相比更小的序列号的响应。</p>
<p>This approach also generalizes to allow concurrent requests from a single client. Instead of the client’s session tracking just the client’s latest sequence number and response, it includes a set of sequence number and response pairs. With each request, the client includes the lowest sequence number for which it has not yet received a response, and the state machine then discards all responses for lower sequence numbers.</p>
<p>然而，因空间有限，会话无法永久保持。因此服务端最终会终止与Client的会话，但这就带来了两个问题：节点间如何就何时终止Client的会话达成共识，以及如何处理被过早终结的活跃Client？</p>
<p>Unfortunately, sessions cannot be kept forever, as space is limited. The servers must eventually decide to expire a client’s session, but this creates two problems: how can servers agree on when to expire a client’s session, and how can they deal with an active client whose session was unfortunately expired too soon?</p>
<p>节点间必须就何时终止Client的会话达成一致；否则的话，节点的状态机会出现不一致。比如，如果某个节点将某个特定客户端的会话终止了，然后重新应用了该客户端的多个重复指令；同时其他节点依然保持着该Client的会话，从而不会重复执行指令，这就导致节点间复制状态机的不一致。为了避免该问题，会话何时终止必须是确定的，就像正常的状态机操作一样。一种处理方式是设置会话数的上限，使用LRU（最近最少使用）策略删除条目；另一种处理方式是基于统一的时间戳来将会话过期。在LogCabin中，Leader会对日志中每个指令增加一个时间戳的属性。在提交日志条目时，所有节点在该时间戳上达成一致；然后，状态机使用确定的时间来过期不活跃的会话。Client会在不活跃期间发出keepalive请求，该请求会增加Leader的时间戳并提交到Raft日志中，以保持会话状态。</p>
<p>Servers must agree on when to expire a client’s session; otherwise, servers’ state machines could diverge from each other. For example, suppose one server expired the session for a particular client, then re-applied many of that client’s duplicated commands; meanwhile, the other servers kept the session alive and did not apply the duplicates. The replicated state machine would become inconsistent. To avoid such problems, session expiry must be deterministic, just as normal state machine operations must be. One option is to set an upper bound on the number of sessions and remove entries using an LRU (least recently used) policy. Another option is to expire sessions based on an agreed upon time source. In LogCabin, the leader augments each command that it appends to the Raft log with its current time. Servers reach agreement on this time as part of committing the log entry; then, the state machines deterministically use this time input to expire inactive sessions. Live clients issue keep-alive requests during periods of inactivity, which are also augmented with the leader’s timestamp and committed to the Raft log, in order to maintain their sessions.</p>
<p>第二个问题是如何处理会话期满后继续运行的Client。我们希望这是一种异常情况；但是总会有一些风险，因为通常没有办法知道Client何时退出。一种选择是在没有会话记录时为Client分配一个新会话，但这会有重复执行在Client上一个会话过期之前执行过的指令的风险。为了提供更严格的保证，服务端需要将新Client与会话已过期的Client区分开。Client首次启动时，可以使用 RegisterClient RPC 在集群中注册自己。这将分配新Client的会话，并向Client返回其标识符，Client随后的所有指令都要带上这个标识符。如果状态机遇到一个没有会话记录的指令，则它不处理该指令而是向Client返回错误。在这种情况下，目前LogCabin 的实现是导致Client崩溃（大多数Client可能无法正确优雅地处理会话过期错误，但系统通常能够处理Client崩溃）。</p>
<p>The second issue is how to deal with a client that continues to operate after its session was expired. We expect this to be an exceptional situation; there is always some risk of it, however, since there is generally no way to know when clients have exited. One option would be to allocate a new session for a client any time there is no record of it, but this would risk duplicate execution of commands that were executed before the client’s previous session was expired. To provide stricter guarantees, servers need to distinguish a new client from a client whose session was expired. When a client first starts up, it can register itself with the cluster using the RegisterClient RPC. This allocates the new client’s session and returns the client its identifier, which the client includes with all subsequent commands. If a state machine encounters a command with no record of the session, it does not process the command and instead returns an error to the client. LogCabin currently crashes the client in this case (most clients probably wouldn’t handle session expiration errors gracefully and correctly, but systems must typically already handle clients crashing).</p>
<h3 id="6-4-Processing-read-only-queries-more-efficiently"><a href="#6-4-Processing-read-only-queries-more-efficiently" class="headerlink" title="6.4 Processing read-only queries more efficiently"></a>6.4 Processing read-only queries more efficiently</h3><p>客户端的只读命令不会修改复制状态机的状态，所以很自然的就能想到只读请求是否可以绕过RAFT的日志机制，因为RAFT的日志就是用于保证多节点以相同顺序修改状态机的状态的。绕过日志可以有很大的性能提升：只读请求在许多应用程序中很常见，将日志条目附加到日志中所需的同步磁盘写入非常耗时。</p>
<p>Read-only client commands only query the replicated state machine; they do not change it. Thus, it is natural to ask whether these queries can bypass the Raft log, whose purpose is to replicate changes to the servers’ state machines in the same order. Bypassing the log offers an attractive performance advantage: read-only queries are common in many applications, and the synchronous disk writes needed to append entries to the log are time-consuming.</p>
<p>然而，如果没有额外的预防措施，<font color=red>绕过日志可能会导致读请求得到过时的结果。</font>比如，收到只读请求的Leader可能已经被分区隔离了，当前集群可能已经选出了新的Leader并且有新的日志提交了，所以如果被隔离的Leader没有与集群中的其他节点沟通的情况下直接回复只读请求，CLIENT就会得到一个过时的结果，这就不符合<font color=red>线性一致性（Linearizability ）</font>的约束了。线性一致性要求读请求得到的结果，必须能反映出在读请求开始进行后的系统状态，所以<font color=red>读请求的结果至少应该能返回最近一次已提交的写操作的结果</font>。一个允许读取过期数据的系统只提供了<font color=red>顺序&#x2F;串行一致性（serializability）</font>，这是一种较弱的一致性形式。</p>
<p>However, without additional precautions, bypassing the log could lead to stale results for readonly queries. For example, a leader might be partitioned from the rest of the cluster, and the rest of the cluster might have elected a new leader and committed new entries to the Raft log. If the partitioned leader responded to a read-only query without consulting the other servers, it would return stale results, which are not linearizable. Linearizability requires the results of a read to reflect a state of the system sometime after the read was initiated; each read must at least return the results of the latest committed write. (A system that allowed stale reads would only provide serializability, which is a weaker form of consistency.) Problems due to stale reads have already been discovered in two third-party Raft implementations [45], so this issue deserves careful attention. </p>
<p>好在，还是可以绕过日志机制处理只读请求，并且仍然保证线性一致性的能力。Leader采取以下步骤即可：</p>
<ol>
<li>如果Leader还没有以其当前Term提交一个日志条目，则该Leader必须要等，直到这个动作完成后（即当前任期提交一个日志条目），才能处理读请求。“Leader完备属性”保证了Leader具有当前集群中所有已提交的条目，但是<font color=red>在其当前任期开始时，它不知道哪些条目是已提交的。</font>要知道这一点，它需要以其当前Term提交一个日志。可以在Leader选举成功后，以自己当前Term提交一个空条目来满足这个要求。只要这个空日志条目被提交了，那就可以保证Leader的committed Index是当前集群所有节点中最大的。 <em>—- 找到Leader刚当选时集群中最大的committed Index；</em></li>
<li>Leader将其当前的committed Index保存为节点内部的变量readIndex，该readIndex会作为读请求返回的系统状态的版本的<font color=red>下限</font>；*—- 读请求返回的系统状态<font color=red>至少</font>能反映出应用readIndex日志修改后系统状态；*</li>
<li>Leader需要确保自己没有在未感知的情况下被其他节点所替代了。所以它会发起新一轮的心跳消息，等到收到集群中Majority节点的响应消息后，Leader就知道，<font color=red>在它发出心跳消息的那一刻，就不可能存在一个具有更大Term的新Leader，</font>所以就能保证，readIndex就是当时集群中所有节点中最大的committed Index。*—- <font color=red>保证readIndex确实是发出心跳消息之前（或者可以说，是收到只读请求时），集群中所有节点最大的committed Index；*<ul>
<li>问：某个Follower先收到了其他Candidate的Vote消息并回复成功了，然后收到了该Leader的心跳消息，怎么办？答：先收到Vote消息并回复成功，说明该Follower的Term增加了，收到心跳消息后就直接忽略了；</li>
<li>问：先收到心跳消息，再收到Vote消息呢？答：收到心跳消息，因为有CheckQuorum机制，所以直接忽略Vote消息；</li>
</ul>
</li>
<li>Leader等到状态机运行到readIndex后，就能保证线性一致性了；*—- 让状态机运行到readIndex日志条目。状态机应用到 ReadIndex 之后的状态都能使这个读请求满足线性一致，不管过了多久，也不管 Leader 有没有飘走。因为线性一致性的要求就是读请求返回最近一次写操作的结果；*</li>
<li>此时，Leader向状态机发起读请求，并将结果返回给CLIENT；*—- 此时向状态机发起读请求，得到系统状态，返回给CLIENT*</li>
</ol>
<p>Fortunately, it is possible to bypass the Raft log for read-only queries and still preserve linearizability. To do so, the leader takes the following steps:</p>
<ol>
<li>If the leader has not yet marked an entry from its current term committed, it waits until it has done so. The Leader Completeness Property guarantees that a leader has all committed entries, but at the start of its term, it may not know which those are. To find out, it needs to commit an entry from its term. Raft handles this by having each leader commit a blank no-op entry into the log at the start of its term. As soon as this no-op entry is committed, the leader’s commit index will be at least as large as any other servers’ during its term. </li>
<li>The leader saves its current commit index in a local variable readIndex. This will be used as a lower bound for the version of the state that the query operates against.</li>
<li>The leader needs to make sure it hasn’t been superseded by a newer leader of which it is unaware. It issues a new round of heartbeats and waits for their acknowledgments from a majority of the cluster. Once these acknowledgments are received, the leader knows that there could not have existed a leader for a greater term at the moment it sent the heartbeats. Thus, the readIndex was, at the time, the largest commit index ever seen by any server in the cluster.</li>
<li>The leader waits for its state machine to advance at least as far as the readIndex; this is current enough to satisfy linearizability.</li>
<li>Finally, the leader issues the query against its state machine and replies to the client with the results.</li>
</ol>
<p>跟将读请求作为普通日志条目进行提交的方法相比，这种方法效率更高，因为它避免了磁盘的同步写操作。为了进一步提高效率，Leader可以分摊其确认领导地位的成本：它可以对累积的只读请求只使用一轮心跳流程。</p>
<p>This approach is more efficient than committing read-only queries as new entries in the log, since it avoids synchronous disk writes. To improve efficiency further, the leader can amortize the cost of confirming its leadership: it can use a single round of heartbeats for any number of read-only queries that it has accumulated.</p>
<hr>
<p><font color=red>Follower也可以帮忙处理读请求</font>，这可以提升提供的读吞吐量，也能分担Leader的负载。然而，如果没有额外的预防措施，Follower处理读操作也有返回过期数据的风险。比如Follower被隔离分区了，或者虽然Follower能收到Leader的心跳消息，但是这个Leader自己可能已经被隔离了，只是它不知道而已。所以，为了保证读请求的安全性，Follower需要询问Leader当前的readIndex（Leader需要执行上面的1-3步），然后Follower执行剩下的4和5步，向自己的状态机发送累积的读请求。</p>
<p>Followers could also help offload the processing of read-only queries. This would improve the system’s read throughput, and it would also divert load away from the leader, allowing the leader to process more read-write requests. However, these reads would also run the risk of returning stale data without additional precautions. For example, a partitioned follower might not receive any new log entries from the leader for long periods of time, or even if a follower received a heartbeat from a leader, that leader might itself be deposed and not yet know it. To serve reads safely, the follower could issue a request to the leader that just asked for a current readIndex (the leader would execute steps 1–3 above); the follower could then execute steps 4 and 5 on its own state machine for any number of accumulated read-only queries.</p>
<p>LogCabin implements the above algorithm on leaders, and it amortizes the cost of the heartbeats across multiple read-only queries under high load. Followers in LogCabin do not currently serve read-only requests.</p>
<h4 id="6-4-1-Using-clocks-to-reduce-messaging-for-read-only-queries"><a href="#6-4-1-Using-clocks-to-reduce-messaging-for-read-only-queries" class="headerlink" title="6.4.1 Using clocks to reduce messaging for read-only queries"></a>6.4.1 Using clocks to reduce messaging for read-only queries</h4><p>上面处理读请求的方法在异步模型中保证了线性一致性，所谓异步模型，就是时钟，CPU，消息的传递都可以以任意的速度进行。这种级别的安全性就需要通信来保证：每一批读请求都需要发送一轮到过半节点的心跳消息，这就增加了处理请求的延迟。下面我们会探索一种替代方法，这种方法中，只读查询可以通过依赖时钟来完全避免发送消息。我们不建议使用这种方法，除非对性能有极高的要求。</p>
<p>Up until now, the approach to read-only queries presented has provided linearizability in an asynchronous model (where clocks, processors, and messages can all operate at arbitrary speeds). This level of safety requires communication to achieve: it requires a round of heartbeats to half the cluster for each batch of read-only queries, which adds latency to the queries. The remainder of this section explores an alternative in which read-only queries would avoid sending messages altogether by relying on clocks. LogCabin does not currently implement this alternative, and we do not recommend using it unless necessary to meet performance requirements.</p>
<p>为了在处理只读请求时使用时钟，而非消息，那就需要常规的心跳机制提供一种租约（lease）。一旦Leader的心跳消息被集群中Majority个节点确认了，那Leader就可以认为在一个选举超时时间内没有节点可以成为新的Leader，从而相应的延长其租约（如下图）。在租约到期的这段时间内，Leader就可以无需任何额外的消息直接就可以处理读请求。（第三章介绍的领导权转移机制中，可以提前更换Leader，所以Leader在转移领导权之前需要使其租约超期）</p>
<p>To use clocks instead of messages for read-only queries, the normal heartbeat mechanism would provide a form of lease [33]. Once the leader’s heartbeats were acknowledged by a majority of the cluster, the leader would assume that no other server will become leader for about an election timeout, and it could extend its lease accordingly (see Figure 6.3). The leader would then reply to read-only queries during that period without any additional communication. (The leadership transfer mechanism presented in Chapter 3 allows the leader to be replaced early; a leader would need to expire its lease before transferring leadership.)</p>
<p><img src="/img/05RAFT%E5%A4%84%E7%90%86%E8%AF%BB%E8%AF%B7%E6%B1%82(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220219173541330.png" alt="image-20220219173541330"></p>
<p><em>图6.3：为了在处理只读请求时时钟而不是消息，Leader使用常规的心跳机制来维护租约。<font color=red>一旦Leader的心跳得到集群中Majority节点的回应，Leader就可以将租约延长到 start + election_timeout &#x2F; clock_drift_bound(时钟漂移范围)。因为Follower在租约超期之前，肯定不会因选举超时而发起新的选举。</font>Leader在租约内可以无需通信，直接处理读请求。</em></p>
<p>租约方法假设节点之间的时钟差异有一个界限（假设各个服务器的 CPU clock 的时间是准的，即使有误差，也会在一个非常小的 bound 范围里面，如果各个服务器之间 clock 走的频率不一样，有些太快，有些太慢，这套 lease 机制就可能出问题）。发现和维护这种时钟漂移界限可能会有额外的工作，但是如果这个假设被打破的话，则Leader就可能会回复CLIENT一个过期数据。</p>
<p>The lease approach assumes a bound on clock drift across servers (over a given time period, no server’s clock increases more than this bound times any other). Discovering and maintaining this bound might present operational challenges (e.g., due to scheduling and garbage collection pauses, virtual machine migrations, or clock rate adjustments for time synchronization). If the assumptions are violated, the system could return arbitrarily stale information.</p>
<p>幸运的是，可以通过简单的一个扩展，来保证即使是在异步场景下（时钟出现错误），CLIENT也可以看到状态机是单调递增的前进（顺序一致性）。比如，CLIENT如果看到了某个节点回复的系统状态对应的是Index为n的日志，那它换一个节点请求时，肯定不会看到n-1日志对应系统状态。要实现这一点，节点每次回复CLIENT时，包含当时状态机状态对应的日志Index。CLIENT可以跟踪记录它看到的结果的最新Index，并在每次请求时向节点提供该Index信息。如果节点收到一个请求，如果请求中的Index大于节点本身Applied的日志索引，则它不会处理该请求。</p>
<p>Fortunately, a simple extension can improve the guarantee provided to clients, so that even under asynchronous assumptions (even if clocks were to misbehave), each client would see the replicated state machine progress monotonically (sequential consistency). For example, a client would not see the state as of log index n, then change to a different server and see only the state as of log index n - 1. To implement this guarantee, servers would include the index corresponding to the state machine state with each reply to clients. Clients would track the latest index corresponding to results they had seen, and they would provide this information to servers on each request. If a server received a request for a client that had seen an index greater than the server’s last applied log index, it would not service the request (yet).</p>
<h3 id="6-5-Conclusion-总结"><a href="#6-5-Conclusion-总结" class="headerlink" title="6.5 Conclusion      总结"></a>6.5 Conclusion      总结</h3><p>本章讨论了Client如何与Raft互动的几个问题。在正确性方面，提供线性一致性和优化只读查询的问题尤其微妙。不幸的是，当前的共识相关文献都是讲集群节点间的通信问题，没有涉及这些重要的问题。我们认为这是一个错误。一个完整的系统必须正确地与Client交互，否则核心一致算法提供的一致性水平将被浪费。正如我们在真实的基于Raft的系统中已经看到的，Client与Raft的交互可能是bug的主要来源，但我们希望更好地理解这些问题从而可以防止未来出现问题。</p>
<p>This chapter discussed several issues in how clients interact with Raft. The issues of providing linearizability and optimizing read-only queries are particularly subtle in terms of correctness. Unfortunately, when the consensus literature only addresses the communication between cluster servers, it leaves these important issues out. We think this is a mistake. A complete system must interact with clients correctly, or the level of consistency provided by the core consensus algorithm will go to waste. As we’ve already seen in real Raft-based systems, client interaction can be a major source of bugs, but we hope a better understanding of these issues can help prevent future problems.</p>
<hr>
<h3 id="附注：关于Lease-Read的补充"><a href="#附注：关于Lease-Read的补充" class="headerlink" title="附注：关于Lease Read的补充"></a>附注：关于Lease Read的补充</h3><p>Lease Read就是Leader在发送心跳前记一个时间点start，一旦心跳消息得到集群中Majority节点的回应，Leader就可以将租约延长到 start + election_timeout &#x2F; clock_drift_bound(时钟漂移范围)。Leader在租约内可以直接处理读请求，无需与其他节点进行沟通。</p>
<p>先抛开节点间的clock drift不谈，start是发送心跳消息的起始时间，Follower收到心跳消息的时间点记为<code>t_recvHeart</code>，那显然<code>start &lt; t_recvHeart</code>，收到Majority节点的心跳响应，就说明Follower至少在<code>t_recvHeart + election_timeout</code>内不会选举超时，也不会投票给其他节点，因为<code>start + election_timeout</code>肯定小于<code>t_recvHeart + election_timeout</code>，所以Leader将租约定为<code>start + election_timeout</code>肯定是安全的。</p>
<p>考虑clock drift的话，由于不同节点的<code>CPU</code>时钟可能有不同程度的漂移，这会导致在一个很小的时间窗口内，即使Leader认为其持有租约，但集群已经选举出了新的Leader。因此，一些系统在实现Lease Read时缩小了Leader持有租约的时间，选择了一个略小于 <code>election_timeout</code> 的时间，以减小时钟漂移带来的影响。</p>
<p>下面是关于clock drift的资料：</p>
<ul>
<li>clocks on different computers may give different times;</li>
<li>computer clocks drift from perfect time and their drift rates differ from one another.</li>
<li>clock drift rate: the relative amount that a computer clock differs from a perfect clock;</li>
<li>Computer clocks are not generally in perfect agreement.<ul>
<li>Clock Skew: the difference between the times on two clocks (at any instant);</li>
<li>Computer clocks are subject to clock drift (they count time at different rates);</li>
<li>Clock drift rate: the difference per unit of time from some ideal reference;</li>
<li>Ordinary quartz clocks（普通石英钟） drift by about 1 sec in 11-12 days. (10^-6 secs&#x2F;sec). High precision quartz clocks drift rate is about 10^-7 or 10^-8 secs&#x2F;sec;</li>
</ul>
</li>
</ul>
<p>参考：</p>
<ul>
<li><a href="https://blog.csdn.net/weixin_43705457/article/details/120572291">https://blog.csdn.net/weixin_43705457/article/details/120572291</a></li>
<li><a href="https://mrcroxx.github.io/posts/code-reading/etcdraft-made-simple/6-readonly/#13-lease-read">https://mrcroxx.github.io/posts/code-reading/etcdraft-made-simple/6-readonly/#13-lease-read</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25367435">https://zhuanlan.zhihu.com/p/25367435</a></li>
<li>《Lecture10-TimeClock.pdf》</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>raft大论文翻译-05日志压缩</title>
    <url>/2022/01/08/%E5%88%86%E5%B8%83%E5%BC%8F/raft/raft%E8%AE%BA%E6%96%87/15%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9/</url>
    <content><![CDATA[<h2 id="Chapter-5-Log-compaction"><a href="#Chapter-5-Log-compaction" class="headerlink" title="Chapter 5 Log compaction"></a>Chapter 5 Log compaction</h2><p>在正常运行期间，随着接收的客户端请求的增多，Raft的日志大小也会增长。随着日志增多，它会占用更多的空间，需要更多的时间来replay。如果没有压缩日志的方法，最终将导致可用性问题：<font color=red>服务器要么空间不足，要么启动时间过长。</font>因此，任何实际系统都需要某种形式的日志压缩方法。</p>
<p>Raft’s log grows during normal operation as it incorporates more client requests. As it grows larger, it occupies more space and takes more time to replay. Without some way to compact the log, this will eventually cause availability problems: servers will either run out of space, or they will take too long to start. Thus, some form of log compaction is necessary for any practical system.</p>
<span id="more"></span>
<p>日志压缩的一般思路是，随着时间的推移，日志中的许多信息就会变得过时，从而可以丢弃。例如，如果稍后的操作将x设置为3，则之前将x设置为2的操作日志就是过时日志。一旦日志条目被提交并应用到状态机，就不再需要用于到达当前状态的中间状态和操作，从而可以将它们压缩掉。</p>
<p>The general idea of log compaction is that much of the information in the log becomes obsolete over time and can be discarded. For example, an operation that sets x to 2 is obsolete if a later operation sets x to 3. Once log entries have been committed and applied to the state machine, the intermediate states and operations used to arrive at the current state are no longer needed, and they can be compacted away.</p>
<p>与Raft的核心算法和成员变更算法不同，<font color=red>不同的系统在日志压缩方面会有不同的需求</font>。由于以下两个原因，没有一种万能的日志压缩解决方案。首先，不同的系统可能会选择在不同程度上权衡简单性和性能。其次，状态机与日志压缩密切相关，状态机的大小各有不同，而且状态机是基于磁盘还是基于易失性内存也有很大的不同。</p>
<p>Unlike the core Raft algorithm and membership changes, different systems will have different needs when it comes to log compaction. There is no one-size-fits-all solution to log compaction for a couple of reasons. First, different systems may choose to trade off simplicity and performance to varying degrees. Second, the state machine must be intimately involved in log compaction, and state machines differ substantially in size and in whether they are based on disk or volatile memory.</p>
<p>本章会讨论各种日志压缩方法。在各种方法中，<font color=red>日志压缩的大部分责任都落在状态机上，状态机负责将状态写入磁盘并压缩状态。</font>状态机可以通过不同的方式实现这一点，本章对其进行了描述，并在图5.1中进行了总结：</p>
<ul>
<li>对于基于内存的状态机而言，快照（Snapshotting ）是概念上是最简单的方法。在该方法中，整个当前系统状态会写入稳定存储上的快照文件中，然后丢弃该点之前的整个日志。在Chubby[11,15]和ZooKeeper[38]中使用了快照技术，我们在Logcard中也实现了快照。5.1节中会深入介绍快照方法。</li>
<li>对于基于磁盘的状态机，在磁盘上维护系统状态的最新副本是正常操作的一部分。因此，只要状态机将写操作反映到磁盘上，Raft日志就可以被丢弃，并且快照文件仅在向其他服务器发送一致的磁盘映像时使用（5.2节描述）。</li>
<li>5.3节介绍了日志压缩的增量方法，如日志清理和日志结构合并树（ log-structured merge trees）。这些方法可以高效地写入磁盘，并且随着时间的推移，它们可以更加均匀地利用资源。</li>
<li>最后，5.4节讨论了一种将快照直接存储在日志中的压缩方法。虽然该方法容易实现，但其只适用于非常小的状态机。</li>
</ul>
<p>The goal of this chapter is to discuss a variety of approaches to log compaction. In each approach, most of the responsibility of log compaction falls on the state machine, which is in charge of writing the state to disk and compacting the state. State machines can achieve this in different ways, which are described throughout the chapter and summarized in Figure 5.1:</p>
<ul>
<li>Snapshotting for memory-based state machines is conceptually the simplest approach. In snapshotting, the entire current system state is written to a snapshot on stable storage, then the entire log up to that point is discarded. Snapshotting is used in Chubby [11, 15] and ZooKeeper [38], and we have implemented snapshotting in LogCabin. Snapshotting is the approach presented in the most depth in this chapter, in Section 5.1.</li>
<li>With disk-based state machines, a recent copy of the system state is maintained on disk as part of normal operation. Thus, the Raft log can be discarded as soon as the state machine reflects writes to disk, and snapshotting is used only when sending consistent disk images to other servers (Section 5.2).</li>
<li>Incremental approaches to log compaction, such as log cleaning and log-structured merge trees, are presented in Section 5.3. These approaches write to disk efficiently, and they utilize resources evenly over time.</li>
<li>Finally, Section 5.4 discusses an approach to log compaction that minimizes the mechanism required by storing snapshots directly in the log. Though easier to implement, this approach is only suitable for very small state machines.</li>
</ul>
<p><img src="/img/15RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220415170837048.png" alt="image-20220415170837048"></p>
<p>LogCabin系统目前只实现了基于内存的快照方法（它使用基于内存的状态机）。</p>
<p>LogCabin currently only implements the memory-based snapshotting approach (it embeds a memory based state machine).</p>
<p>日志压缩的各种方法有几个共同的核心概念。</p>
<p>首先，<font color=red>每个服务器独立压缩已提交的日志，而不是将压缩功能都集中在Leader身上。</font>这避免了Leader将快照数据传输给本身已有日志数据的Follower。这也有助于模块化：日志压缩的大部分复杂性都落在状态机中，而与Raft本身没有太多交互。这有助于将系统总体复杂性保持在最低水平：这样，整体复杂度是Raft的复杂度加上日志压缩复杂度，而不是与之相乘。5.4节会讨论一种将压缩责任集中在Leader身上的方法（对于非常小的状态机，基于Leader的日志压缩方法可能更好）。</p>
<p>The various approaches to compaction share several core concepts. First, instead of centralizing compaction decisions on the leader, each server compacts the committed prefix of its log independently. This avoids having the leader transmit data to followers that already have the data in their logs. It also helps modularity: most of the complexity of log compaction is contained within the state machine and does not interact much with Raft itself. This helps keep overall system complexity to a minimum: the complexity of Raft adds to, rather than multiplies with, the complexity of log compaction. Alternative approaches that centralize compaction responsibilities on a leader are discussed further in Section 5.4 (and for very small state machines, a leader-based approach may be better).</p>
<p>其次，状态机和Raft之间的基本交互涉及将已提交的日志从Raft传输给状态机。在应用日志条目之后，状态机迟早会以一种可以恢复到当前系统状态的方式将这些条目反映到磁盘上。一旦这样做了，它会告诉Raft丢弃日志中相应的条目。<font color=red>在Raft丢弃日志条目之前，Raft自身必须保存一些描述这些日志条目的信息。具体而言，Raft必须保留其丢弃的最后一个条目的Index和Term；</font>这俩信息可用于将位于状态机的压缩的状态之后的日志部分保持在正确的OFFSET上，并保证AppendEntries的一致性检查可以继续工作（该RPC需要包含其中的第一个条目之前的条目的Index和Term信息）。<font color=red>Raft还需要保留丢弃日志中的成员配置信息，以支持集群成员配置变更。</font></p>
<p>Second, the basic interaction between the state machine and Raft involves transferring responsibility for a prefix of the log from Raft to the state machine. Sooner or later after applying entries, the state machine reflects those entries to disk in a way that can recover the current system state. Once it has done so, it tells Raft to discard the corresponding prefix of the log. Before Raft can give up responsibility for the log prefix, it must save some of its own state describing the log prefix. Specifically, Raft retains the index and term of the last entry it discarded; this anchors the rest of the log in place after the state machine’s state and allows the AppendEntries consistency check to continue to work (it needs the index and term for the entry preceding the first entry in the log). Raft also retains the latest configuration from the discarded log prefix in order to support cluster membership changes.</p>
<p>第三，一旦Raft丢弃了日志的部分条目，状态机将承担两个新的职责：<font color=red>如果服务器重新启动，状态机需要从磁盘加载与丢弃的日志条目对应的状态，然后才能应用Raft日志中的任何条目；此外，状态机需要生成一致的状态镜像文件，以便将其发送给慢Follower（其日志远远落后于领导者的日志）。</font>将日志压缩延迟到日志条目“完全复制”到集群中的每个成员后是不可行的，否则少数慢速Follower可能会使得集群不可用，而且实际场景中可能会随时向集群中添加新的服务器。因此，慢速Follower或新服务器偶尔需要通过网络接收初始状态。Raft通过在AppendEntries中包含已丢弃日志的Index和Term信息来检测到这一点。在这种情况下，状态机必须提供一致的状态映像文件，然后由Leader发送给Follower。</p>
<p>Third, once Raft has discarded a prefix of the log, the state machine takes on two new responsibilities. If the server restarts, the state machine will need to load the state corresponding to the discarded log entries from disk before it can apply any entries from the Raft log. In addition, the state machine may need to produce a consistent image of the state so that it can be sent to a slow follower (one whose log is far behind the leader’s). It is not feasible to defer compaction until log entries have been “fully replicated” to every member in the cluster, since a minority of slow followers must not keep the cluster from being fully available, and new servers can be added to the cluster at any time. Thus, slow followers or new servers will occasionally need to receive their initial states over the network. Raft detects this when the next entry needed in AppendEntries has already been discarded in the leader’s log. In this case, the state machine must provide a consistent image of the state, which the leader then sends to the follower.</p>
<h3 id="5-1-Snapshotting-memory-based-state-machines-基于内存状态机的快照方法"><a href="#5-1-Snapshotting-memory-based-state-machines-基于内存状态机的快照方法" class="headerlink" title="5.1 Snapshotting memory-based state machines     基于内存状态机的快照方法"></a>5.1 Snapshotting memory-based state machines     基于内存状态机的快照方法</h3><p><img src="/img/04RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20210924082907442.png" alt="image-20210924082907442"></p>
<p><font color=red>快照方法适用于状态机的数据结构保存在内存中的场景。</font>对于数据集为千兆字节或数十千兆字节的状态机来说，这是一个合理的选择。它使操作能够快速完成，因为它们不必从磁盘获取数据；编程也很容易，因为可以使用丰富的数据结构，并且每个操作都可以运行到完成（无需阻塞I&#x2F;O）。</p>
<p>The first approach to snapshotting applies when the state machine’s data structures are kept in memory. This is a reasonable choice for state machines with datasets in the gigabytes or tens of gigabytes. It enables operations to complete quickly, since they never have to fetch data from disk; it is also easy to program, since rich data structures can be used and every operation can run to completion (without blocking for I&#x2F;O).</p>
<p><img src="/img/04RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20210927082637519.png" alt="image-20210927082637519"></p>
<p><em>图5.2：服务器将日志中已提交的条目（Index从1到5）替换为新快照，该快照只包含当前状态（示例中的变量x和y）。在丢弃1-5的日志条目之前，Raft需要保存快照中的最后一条的Index（即5）和Term（即3），从而将快照定位到日志中Index为6之前的部分。</em></p>
<p>上图展示了内存状态机中在Raft中进行快照的基本思想。<font color=red>每个服务器都独立地进行快照，快照只覆盖日志中已提交且应用的条目。</font>快照方法的大部分工作就是将状态机的当前状态进行序列化，而这是特定于状态机来实现的。例如，<code>LogCabin</code>的状态机使用树作为其主要数据结构，它使用前序遍历深度优先的算法来序列化此树（因此在应用快照时，父节点在子节点之前被创建）。状态机还必须序列化它们保存的信息，以便为客户端提供线性化能力（参见第6章）。</p>
<p>Figure 5.2 shows the basic idea of snapshotting in Raft when the state machine is kept in memory. Each server takes snapshots independently, covering just the committed entries in its log. Most of the work in snapshotting involves serializing the state machine’s current state, and this is specific to a particular state machine implementation. For example, LogCabin’s state machine uses a tree as its primary data structure; it serializes this tree using a pre-order depth-first traversal (so that when applying the snapshot, parent nodes are created before their children). State machines must also serialize the information they keep for providing linearizability to clients (see Chapter 6).</p>
<p><font color=red>一旦状态机写完快照，就可以将日志截断。Raft首先保存重启所需的状态：快照中最后一个条目的Index和Term，以及快照中的最新成员配置，然后将Index之前的日志丢弃，之前保存的快照也可以丢弃，它们也没用了。</font></p>
<p>Once the state machine completes writing a snapshot, the log can be truncated. Raft first stores the state it needs for a restart: the index and term of the last entry included in the snapshot and the latest configuration as of that index. Then it discards the prefix of its log up through that index. Any previous snapshots can also be discarded, as they are no longer useful.</p>
<p>如上所述，Leader可能偶尔需要将其状态发送给慢Follower和新加入集群的服务器。在快照方法中，这个状态就是最新的快照文件，Leader使用名为<code>InstallSnapshot</code>的RPC发送该快照，如下图5.3所示。<font color=red>当Follower收到此RPC中包含的快照文件时，它必须决定如何处理其现有的日志项。</font>通常，快照文件中会包含Follower日志中没有的新信息，在这种情况下，Follower丢弃其整个日志，将其直接用快照取代，日志中可能有与快照冲突的未提交条目，也直接被快照覆盖。相反，如果Follower接收到的快照，仅包含其日志条目的头部部分条目（由于重传或某些错误），则快照所能覆盖的那些日志条目将被删除，但快照之后的条目仍然有效，必须保留。</p>
<p><img src="/img/04RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20210927083723980.png"></p>
<p><em>图5.3：Leader使用InstallSnapshot RPC将快照发送给慢Follower。Leader使用AppendEntries RPC将条目同步到Follower，如果发现Follower需要的日志条目在Leader中已经被丢弃时，Leader才会发送快照。<font color=red>Leader将快照文件分割成若干块（chunk）进行发送</font>。通过分块传输，相当于给Follower一个心跳，可以使其重置选举定时器。快照块按照顺序发送，从而可以按序写入磁盘文件。<font color=red>RPC中包含了重启时RAFT加载快照所需的状态：快照中覆盖的最后日志条目的Index和Term，以及最新的成员配置信息。</font></em></p>
<p><em>InstallSnapshot RPC：</em></p>
<ul>
<li><em>参数：</em><ul>
<li><em>term：Leader的Term；</em></li>
<li><em>leaderId：Leader的Id，从而Follower可以重定向客户端请求；</em></li>
<li><em>lastIndex：快照所能覆盖的日志中最后一个条目的的Index；</em></li>
<li><em>lastTerm：快照所能覆盖的日志中最后一个条目的的Term；</em></li>
<li><em>lastConfig：快照中最后的成员配置；</em></li>
<li><em>offset：快照块在快照文件中的偏移；</em></li>
<li><em>data[]：快照块的数据；</em></li>
<li><em>done：为true表示最后一个快照块；</em></li>
</ul>
</li>
<li><em>结果：</em><ul>
<li><em>term：Follower的currentTerm，可用于Leader更新自己的Term；</em></li>
</ul>
</li>
<li><em>接收者的逻辑：</em><ol>
<li><em>如果参数term小于自己的currentTerm，则立即回复；</em></li>
<li><em>如果offset为0，即受到的是第一个快照块，则创建快照文件；</em></li>
<li><em>根据offset将快照块写入快照文件；</em></li>
<li><em>如果done为false，则回复Leader并等待更多的快照块；</em></li>
<li><em>如果快照中的lastIndex大于之前快照中的lastIndex，则保存快照文件以及RAFT的状态（即快照中的lastIndex, lastTerm和lastConfig）。丢弃其他快照文件；</em></li>
<li><em>如果当前的日志中有日志条目与快照中的lastIndex和lastTerm相同，则丢弃日志中lastIndex及之前的日志条目，然后回复；</em></li>
<li><em>其他情况下，丢弃所有日志；</em></li>
<li><em>使用快照的内容重置状态机，并且加载快照中的lastConfig作为集群成员配置；</em></li>
</ol>
</li>
</ul>
<p>As introduced above, the leader may occasionally need to send its state to slow followers and to new servers that are joining the cluster. In snapshotting, this state is just the latest snapshot, which the leader transfers using a new RPC called InstallSnapshot, as shown in Figure 5.3. When a follower receives a snapshot with this RPC, it must decide what to do with its existing log entries. Usually the snapshot will contain new information not already in the follower’s log. In this case, the follower discards its entire log; it is all superseded by the snapshot and may possibly have uncommitted entries that conflict with the snapshot. If, instead, the follower receives a snapshot that describes a prefix of its log (due to retransmission or by mistake), then log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.</p>
<p>本节的剩余部分将讨论基于内存的状态机的快照方法中的次要问题：</p>
<ul>
<li>5.1.1节讨论了如何在正常操作的同时生成快照，以尽量减少快照对客户端的影响；</li>
<li>5.1.2节讨论了何时保存快照，平衡空间使用和快照开销；</li>
<li>5.1.3节讨论了实施快照方法时出现的问题；</li>
</ul>
<p>The remainder of this section discusses secondary issues for snapshotting memory-based state machines：</p>
<ul>
<li>Section 5.1.1 discusses how to produce snapshots in parallel with normal operations, to minimize their effects on clients;</li>
<li>Section 5.1.2 discusses when to take a snapshot, balancing the space usage and the overhead of snapshotting; and</li>
<li>Section 5.1.3 discusses the issues that arise in implementing snapshotting.</li>
</ul>
<h4 id="5-1-1-Snapshotting-concurrently-并发快照（保证服务的同时进行写入快照）"><a href="#5-1-1-Snapshotting-concurrently-并发快照（保证服务的同时进行写入快照）" class="headerlink" title="5.1.1 Snapshotting concurrently 并发快照（保证服务的同时进行写入快照）"></a>5.1.1 Snapshotting concurrently 并发快照（保证服务的同时进行写入快照）</h4><p>创建快照可能需要很长时间，包括状态序列化和将其写入磁盘。例如，在当今的服务器上复制10GB内存大约需要1秒，而将其序列化通常需要更长的时间。即使是固态磁盘也只能在1秒内写入大约500MB的内存。因此，<font color=red>序列化和写入快照必须与正常操作同时进行，以避免出现可用性问题。</font></p>
<p>Creating a snapshot can take a long time, both in serializing the state and in writing it to disk. For example, copying 10GB of memory takes about one second on today’s servers, and serializing it will usually take much longer: even a solid state disk can only write about 500MB in one second. Thus, both serializing and writing snapshots must be concurrent with normal operations to avoid availability gaps.</p>
<p>幸运的是，写时复制技术允许在不影响当前写入快照的情况下apply新的更新。有两种方法：</p>
<ul>
<li>使用不可变（功能性的）数据结构构建状态机来支持写时复制技术。由于状态机命令不会原地修改状态，快照任务可以维护一个对先前状态的引用，并将其一致地写入快照中；<font color=red>没看懂，估计是类似于在应用层实现操作系统层面的copy-on-write</font>.</li>
<li>或者，可以使用操作系统支持的写时拷贝技术（在编程环境允许的情况下）。例如，在Linux上，内存状态机可以使用fork来复制服务进程的整个地址空间。然后，在父进程继续为请求提供服务的同时，子进程可以将状态写入文件并退出。Logcard实现目前使用这种方法。</li>
</ul>
<p>Fortunately, copy-on-write techniques allow new updates to be applied without impacting the snapshot being written. There are two approaches to this:</p>
<ul>
<li>State machines can be built with immutable (functional) data structures to support this. Because state machine commands would not modify the state in place, a snapshotting task could keep a reference to some prior state and write it consistently into a snapshot.</li>
<li>Alternatively, the operating system’s copy-on-write support can be used (where the programming environment allows it). On Linux for example, in-memory state machines can use fork to make a copy of the server’s entire address space. Then, the child process can write out the state machine’s state and exit, all while the parent process continues servicing requests. The LogCabin implementation currently uses this approach.</li>
</ul>
<p>服务器需要额外的内存来进行并行快照，这些内存需要进行规划和管理。状态机必须具有针对快照文件的流式接口，这样在快照创建时就不必完全暂存在内存中。尽管如此，写时复制仍需要与快照过程中更改的状态部分成比例的额外内存。此外，由于错误共享（false sharing），使用操作系统提供的写时复制时通常会使用更多内存（例如，如果两个不相关的数据项恰好位于同一内存页上，则即使只有第一个数据项发生了更改，第二个数据项也会被复制）。在运气不那么好的情况下，<font color=red>快照期间内存容量可能会耗尽，此时服务器应停止接受新日志条目，直到完成快照；</font>这将暂时牺牲该服务器的可用性（集群可能仍然可用），但至少允许服务器恢复。这种情况下最好不要中止快照，稍后重试，因为下一次尝试也可能会遇到相同的问题。（LogCabin 使用磁盘流接口，但目前仍无法很好的处理内存耗尽问题。）</p>
<p>Servers require additional memory for snapshotting concurrently, which should be planned for and managed. It is essential for state machines to have a streaming interface to the snapshot file, so that the snapshot does not have to be staged entirely in memory while it is created. Still, copy-on-write requires extra memory proportional to the fraction of the state machine state that is changed during the snapshotting process. Moreover, relying on the operating system for copy-on-write will typically use even more memory due to false sharing (for example, if two unrelated data items happen to be on the same page of memory, the second item will be duplicated even when only the first has changed). In the unfortunate event that memory capacity is exhausted during snapshotting, a server should stop accepting new log entries until it completes its snapshot; this would temporarily sacrifice the server’s availability (the cluster might still remain available), but at least it would allow the server to recover. It is better not to abort the snapshot and retry later, since the next attempts might also face the same problem. (LogCabin uses a streaming interface to disk, but it does not currently handle memory exhaustion gracefully.)</p>
<h4 id="5-1-2-When-to-snapshot-何时进行快照"><a href="#5-1-2-When-to-snapshot-何时进行快照" class="headerlink" title="5.1.2 When to snapshot 何时进行快照"></a>5.1.2 When to snapshot 何时进行快照</h4><p>服务器必须决定何时快照。如果服务器快照太频繁，则会浪费磁盘带宽和其他资源；如果快照频率太低，则可能会耗尽其存储容量，并会增加重启时重放（replay）日志所需的时间。</p>
<p>Servers must decide when to snapshot. If a server snapshots too often, it wastes disk bandwidth and other resources; if it snapshots too infrequently, it risks exhausting its storage capacity, and it increases the time required to replay the log during restarts.</p>
<p>一个简单的策略是在日志达到固定大小（以字节为单位）时进行快照。如果此大小设置为大大超过快照的预期大小，则快照的磁盘带宽开销将很小。然而，对于小型状态机，这可能会导致不必要的大量日志堆积。</p>
<p>One simple strategy is to take a snapshot when the log reaches a fixed size in bytes. If this size is set to be significantly larger than the expected size of a snapshot, then the disk bandwidth overhead for snapshotting will be small. However, this can result in needlessly large logs for small state machines.</p>
<p>更好的方法是比较快照的大小和日志的大小。如果快照比日志小很多倍，则进行快照可能是值得的。但是，在拍摄快照之前预估其大小可能是非常困难和繁重的，会给状态机带来很大的簿记负担，或者需要与实际拍摄快照一样多的工作来动态计算大小。压缩快照文件还可以更加节省空间和带宽，但很难预测压缩输出的大小。</p>
<p>A better approach involves comparing the snapshot’s size with the log’s size. If the snapshot will be many times smaller than the log, it is probably worthwhile to take a snapshot. However, calculating the size of a snapshot before it is taken can be difficult and burdensome, imposing a significant bookkeeping burden for the state machine, or requiring almost as much work as actually taking a snapshot to compute the size dynamically. Compressing snapshot files also results in space and bandwidth savings, but it is hard to predict how large the compressed output will be.</p>
<p>幸运的是，<font color=red>使用上一个快照的大小而不是下一个快照的大小作为参照，可能会更加合理。一旦日志的大小超过前一个快照的大小乘以可配置的扩展因子，服务器就可以进行快照</font>。扩展因子需要权衡磁盘带宽和空间利用率。例如，扩展因子为4会导致磁盘带宽的20%用于快照（快照的1个字节对应4字节的日志条目），并且需要大约6倍于存储单个状态副本所需的磁盘容量（其中包括：旧快照，比旧快照大4倍的日志，以及正在写入新快照）。</p>
<p>Fortunately, using the size of the previous snapshot rather than the size of the next one results in reasonable behavior. Servers take a snapshot once the size of the log exceeds the size of the previous snapshot times a configurable expansion factor. The expansion factor trades off disk bandwidth for space utilization. For example, an expansion factor of 4 results in about 20% of the disk’s bandwidth being used towards snapshotting (for every 1 byte of snapshot, 4 bytes of log entries will be written), and requires about 6 times the disk capacity as that needed to store a single copy of the state (the old snapshot, a log 4 times bigger than that, and the new snapshot being written).</p>
<p>快照也会造成CPU和磁盘带宽的大量使用，这可能会影响客户端性能。这可以通过额外的硬件来缓解；例如，可以使用第二个磁盘驱动器来提供额外的磁盘带宽。</p>
<p>Snapshotting still creates a burst of CPU and disk bandwidth usage that might impact client performance. This can be mitigated with additional hardware; for example, a second disk drive can be used to provide the additional disk bandwidth.</p>
<p>还可以以客户端请求从不在正在快照的服务器上等待的方式来安排快照。在这种方法中，对服务器将进行协调，以便在任何时候（如果可能的话）集群中只有少数服务器进行快照。因为Raft只需要过半服务器即可提交日志条目，所以少数服务器进行快照通常不会对客户端产生不利影响。当Leader想要进行快照时，它会先退出，同时允许另一台服务器管理集群。如果这一方法足够可靠，那么可以不需要并发快照；服务器仅在拍摄快照时不可用（尽管这会影响集群屏蔽故障的能力）。这对于未来的工作来说是一个激动人心的机会，因为它有可能提高系统的整体性能并减少系统的复杂度。</p>
<p>It may also be possible to schedule snapshots in a way that client requests never wait on a server that is snapshotting. In this approach, servers would coordinate so that only up to a minority of the servers in the cluster would snapshot at any one time (when possible). Because Raft only requires a majority of servers to commit log entries, the minority of snapshotting servers would normally have no adverse effect on clients. When a leader wished to snapshot, it would step down first, allowing another server to manage the cluster in the meantime. If this approach was sufficiently reliable, it could also eliminate the need to snapshot concurrently; servers could just be unavailable while they took their snapshots (though they would count against the cluster’s ability to mask failures). This is an exciting opportunity for future work because of its potential to both improve overall system performance and reduce mechanism.</p>
<h4 id="5-1-3-Implementation-concerns-实现细节"><a href="#5-1-3-Implementation-concerns-实现细节" class="headerlink" title="5.1.3 Implementation concerns 实现细节"></a>5.1.3 Implementation concerns 实现细节</h4><p>本节主要描述实现快照方法所涉及的主要组件，并讨论了实现这些组件的困难：</p>
<p>This section reviews the major components needed for a snapshotting implementation and discusses the difficulties with implementing them:</p>
<p><strong>保存和加载快照</strong>：保存快照涉及状态序列化以及将其写入文件，而加载则是相反的过程。这相当简单，尽管序列化各种类型的数据对象可能有些繁琐。从状态机到磁盘上文件的流式接口有助于避免在内存中保存整个状态机状态；压缩流并对其应用校验和也是有益的。LogCabin 首先将每个快照写入一个临时文件，然后在写入完成并已刷新到磁盘时重命名该文件；这样可以确保服务器在启动时不会加载未完全写入的快照。</p>
<p><strong>Saving and loading snapshots</strong>: Saving a snapshot involves serializing the state machine’s state and writing that data out to a file, while loading is the reverse process. We found this to be fairly straightforward, although it was somewhat tedious to serialize the various types of data objects from their native representations. A streaming interface from the state machine to a file on disk is useful to avoid buffering the entire state machine state in memory; it may also be beneficial to compress the stream and apply a checksum to it. LogCabin writes each snapshot to a temporary file first, then renames the file when writing is complete and has been flushed to disk; this ensures that no server loads a partially written snapshot on startup.</p>
<p><strong>传输快照</strong>：传输快照涉及分别在Leader端和Follower端实现InstallSnapshot RPC。这也很简单，并且可以共享一些将快照保存到磁盘和从磁盘加载快照的代码。快照的传输性能通常不是很重要（需要此状态的Follower尚未参与条目的提交，因此不会很快需要它；另一方面，如果集群遇到其他故障，它可能需要赶上其他Follower以恢复可用性）。</p>
<p><strong>Transferring snapshots</strong>: Transferring snapshots involves implementing the leader and follower sides of the InstallSnapshot RPC. This is fairly straightforward and may be able to share some code with saving snapshots to and loading snapshots from disk. The performance of this transfer is usually not very important (a follower that needs this state has not been participating in the commitment of entries, so it is probably not needed soon; on the other hand, if the cluster suffers additional failures, it may need to catch up the follower to restore availability).</p>
<p><strong>消除不安全的日志访问并丢弃日志条目</strong>：我们最初设计LogCabin 时没有考虑日志压缩，因此代码中假设如果日志中存在条目<code>i</code>，则条目1到<code>i-1</code>也会存在。引入日志压缩之后，这种假设就不再成立了。例如，在确定AppendEntries RPC中前一个条目的Term时，该条目可能已被丢弃。在整个代码中删除这些假设需要仔细的推理和测试。如果编译器能够强制对日志的每次访问都能处理索引超出范围的情况，那么在更强大的类型系统的帮助下，这将更容易实现。一旦我们保证了所有日志访问的安全性，那么丢弃日志的头部条目就很简单了。到目前为止，我们只能单独测试快照的保存、加载和传输，但是当日志条目可以安全地丢弃时，就可以开始在系统范围的测试中这些操作了。</p>
<p><strong>Eliminating unsafe log accesses and discarding log entries</strong>: We originally designed LogCabin without worrying about log compaction, so the code assumed that if entry <code>i</code> was present in the log, entries <code>1</code> through <code>i-1</code> would also be present. This is no longer true with log compaction; for example, when determining the term for the previous entry in the AppendEntries RPC, that entry might have been discarded. Removing these assumptions throughout the code required careful reasoning and testing. This would have been easier with help from a more powerful type system, if the compiler could enforce that every access to the log also handled the case that the index was out of bounds. Once we had made all the log accesses safe, discarding the prefix of the log was straightforward. Until this point, we could only test the saving, loading, and transferring snapshots in isolation, but when log entries can be safely discarded, these can all start to be exercised in system-wide tests.</p>
<p><strong>使用写时复制进行并发快照</strong>：<font color=red>并发快照可能需要重新设计状态机，或是利用操作系统的fork操作。</font>LogCabin目前使用的是fork，但是<font color=red>fork在涉及多线程，以及C++析构函数时需要慎重设计，要使其正确工作存在一些困难。但是，使用fork只需要少量代码，而且完全不需要修改状态机的数据结构，因此我们认为使用fork是正确的方法。</font></p>
<p>Snapshotting concurrently with copy-on-write**: Snapshotting concurrently may require reworking the state machine or leveraging the operating system’s fork operation. LogCabin currently uses fork, which interacts poorly with threads and C++ destructors; getting this to work correctly presented some difficulty. However, it is a small amount of code and completely eliminates the need to modify the state machine’s data structures, so we think it was the right approach.</p>
<p><strong>决定何时快照</strong>：我们建议在开发阶段，每次应用日志条目后就进行快照，因为这有助于快速捕捉bug。实现完成后，应使用更加有用的快照时间策略（例如，使用有关Raft日志大小和最后一次快照大小的统计信息）。</p>
<p><strong>Deciding when to snapshot</strong>: We recommend taking snapshots after applying every log entry during development, since that can help catch bugs quickly. Once the implementation is complete, a more useful policy of when to snapshot should be added (e.g., using statistics about the size of Raft log and the size of the last snapshot).</p>
<p>我们发现快照的分段开发和测试很有挑战性。在可以丢弃日志条目之前，上述这些组件大多数都必须就绪，但只有到丢弃日志条目时，许多新的代码路径才能在系统测试中被测到。因此，应该仔细考虑实现和测试这些组件的顺序。</p>
<p>We found piecewise development and testing of snapshotting to be challenging. Most of these components must be in place before it is possible to discard log entries, but only then will many of the new code paths be exercised in system-wide tests. Thus, implementers should consider the order in which to implement and test these components carefully.</p>
<hr>
<h3 id="5-2-Snapshotting-disk-based-state-machines-基于磁盘的状态机的快照"><a href="#5-2-Snapshotting-disk-based-state-machines-基于磁盘的状态机的快照" class="headerlink" title="5.2 Snapshotting disk-based state machines  基于磁盘的状态机的快照"></a>5.2 Snapshotting disk-based state machines  基于磁盘的状态机的快照</h3><p><img src="/img/04RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20210928183432186.png" alt="image-20210928183432186"></p>
<p>本节讨论基于磁盘的大型状态机（大约几十或数百GB）的快照方法。这种状态机与基于内存的状态机在行为上略有不同，<font color=red>在磁盘上状态机总是会有一份状态副本以防崩溃。每次应用Raft日志中的条目都会改变磁盘上的状态，从而形成一个新的状态快照。因此，一旦应用了条目，就可以将其从RAFT日志中删除。</font>（状态机可以在内存中缓冲写操作，以期实现更好的磁盘效率；这种情况下一旦将缓存写入磁盘，相应的条目可以从Raft日志中丢弃）</p>
<p>This section discusses a snapshotting approach for large state machines (on the order of tens or hundreds of gigabytes) that use disk as their primary location of record. These state machines behave differently in that they always have a copy of the state ready on disk in case of a crash. Applying each entry from the Raft log mutates the on-disk state and effectively arrives at a new snapshot. Thus, once an entry is applied, it can be discarded from the Raft log. (State machines can also buffer writes in memory in hopes of achieving better disk efficiency; once they are written to disk, the corresponding entries can be discarded from the Raft log.)</p>
<p>基于磁盘的状态机的主要问题，是修改磁盘上的状态可能性能较差。如果没有写缓存，则每次应用一个命令都需要一个或多个随机磁盘写入，这会降低系统总体的写吞吐量（而写缓冲可能没有多大帮助）。第5.3节讨论了日志压缩的增量方法，该方法通过大的顺序写入来更有效地写入磁盘。</p>
<p>The main problem with disk-based state machines is that mutating state on disk can lead to poor performance. Without write buffering, it requires one or more random disk writes for every command applied, which can limit the system’s overall write throughput (and write buffering might not help much). Section 5.3 discusses incremental approaches to log compaction which write to disk more efficiently with large, sequential writes.</p>
<p>基于磁盘的状态机必须能够提供一致的磁盘快照，以便将其传输给慢Follower。尽管在磁盘上总会有一个状态快照，但状态机仍在不断地修改它。因此，<font color=red>仍然需要写时复制技术</font>，以便在足够长的时间内保持一致的快照以进行传输。幸运的是，磁盘格式几乎总是总可以划分为逻辑块，因此在状态机中实现写时拷贝应该很简单。基于磁盘的状态机也可以依赖操作系统对其快照的支持。例如，Linux上的LVM（逻辑卷管理）可用于创建整个磁盘分区的快照[70]，最近的一些文件系统则允许对单个目录进行快照[19]。</p>
<p>Disk-based state machines must be able to provide a consistent snapshot of the disk for the purpose of transmitting it to slow followers. Although they always have a snapshot on disk, they are continuously modifying it. Thus, they still require copy-on-write techniques to retain a consistent snapshot for a long enough period to transmit it. Fortunately, disk formats are almost always divided into logical blocks, so implementing copy-on-write in the state machine should be straightforward. Disk-based state machines can also rely on operating system support for their snapshots. For example, LVM (logical volume management) on Linux can be used to create snapshots of entire disk partitions [70], and some recent file systems allow snapshotting individual directories [19].</p>
<p>随着磁盘修改的累积，复制磁盘映像的快照可能需要很长时间，保留快照所需的额外磁盘使用量也会增加。虽然我们还没有实现过基于磁盘的快照，但我们推测基于磁盘的状态机可以通过<font color=red>以下方法</font>传输其磁盘内容来避免大部分开销：</p>
<ol>
<li>对于每个磁盘块，记录它的上次修改时间。</li>
<li>在继续正常操作的同时，将整个磁盘内容逐块传输到Follower。在此过程中，Leader上无需额外的磁盘空间。由于传输的同时可能会修改块，这可能导致Follower上的磁盘映像与Leader上的不一致。在Leader发送磁盘块时，记录其上次修改时间。</li>
<li>状态机修改状态时，在磁盘上进行写时复制。一旦执行完此操作，Leader将拥有其磁盘内容的一致性副本，但由于客户端操作的持续进行，需要使用额外的磁盘空间来修改磁盘。</li>
<li>对第2步首次传输磁盘块，到第3步拍摄快照之间修改的磁盘块重新传输。</li>
</ol>
<p>Copying a snapshot of a disk image can take a long time, and as modifications to the disk accumulate, so does the extra disk usage required to retain the snapshot. Although we haven’t implemented disk-based snapshotting, we speculate that disk-based state machines could avoid most of this overhead by transmitting their disk contents with the following algorithm:</p>
<ol>
<li>For each disk block, track the time it was last modified.</li>
<li>While continuing normal operation, transmit the entire disk contents to a follower block by block. During this process, no extra disk space is used on the leader. Since blocks are being modified concurrently, this is likely to result in an inconsistent disk image on the follower. As each block is transferred from the leader, note its last modification time.</li>
<li>Take a copy-on-write snapshot of the disk contents. Once this is taken, the leader has a consistent copy of its disk contents, but additional disk space is used as modifications to the disk occur due to continued client operations.</li>
<li>Retransmit only the disk blocks that were modified between when they were first transmitted in Step 2 and when the snapshot was taken in Step 3.</li>
</ol>
<p>可以预期的是，在第3步创建一致性快照时，该快照的大部分块都已被传输。如果是这种情况，步骤4中的传输将能够很快完成，在步骤4中用于保存快照的额外磁盘容量将较低，并且重新传输修改后的块的额外网络带宽也将较低。</p>
<p>Hopefully, most of the blocks of the consistent snapshot will have already been transmitted by the time it is created in Step 3. If that is the case, the transfer in Step 4 will proceed quickly: the additional disk capacity used to retain the snapshot on the leader during Step 4 will be low, and the additional network bandwidth used during Step 4 to retransmit modified blocks will also be low.</p>
<hr>
<h3 id="5-3-Incremental-cleaning-approaches-增量清洗方法（略）"><a href="#5-3-Incremental-cleaning-approaches-增量清洗方法（略）" class="headerlink" title="5.3 Incremental cleaning approaches  增量清洗方法（略）"></a>5.3 Incremental cleaning approaches  增量清洗方法（略）</h3><p>Incremental approaches to compaction, such as log cleaning [97, 98] and log-structured merge trees [84, 17] (LSM trees), are also possible. Although they are more complex than snapshotting, incremental approaches have several desirable features:</p>
<ul>
<li>They operate on only a fraction of the data at once, so they spread the load of compaction evenly over time.</li>
<li>They write to disk efficiently, both in normal operation and while compacting. They use large, sequential writes in both cases. Incremental approaches also selectively compact parts of the disk with the most reclaimable space, so they write less data to disk than snapshotting for memory-based state machines (which rewrites all of disk on every snapshot).</li>
<li>They can transfer consistent state snapshots fairly easily because they do not modify regions of disk in place.</li>
</ul>
<p>Section 5.3.1 and Section 5.3.2 first describe the basics of log cleaning and LSM trees in general. Then, Section 5.3.3 discusses how they could be applied to Raft.</p>
<h4 id="5-3-1-Basics-of-log-cleaning"><a href="#5-3-1-Basics-of-log-cleaning" class="headerlink" title="5.3.1 Basics of log cleaning"></a>5.3.1 Basics of log cleaning</h4><p>Log cleaning was introduced in the context of log-structured file systems [97] and has recently been proposed for in-memory storage systems such as RAMCloud [98]. In principle, log cleaning can be used for any type of data structure, though some would be harder to implement efficiently than others.</p>
<p>Log cleaning maintains the log as the place of record for the system’s state. The layout is optimized for sequential writing, and it makes read operations effectively random access. Thus, indexing structures are needed to locate data items to read.</p>
<p>In log cleaning, the log is split into consecutive regions called segments. Each pass of the log cleaner compacts the log using a three-step algorithm:</p>
<ol>
<li>It first selects segments to clean that have accumulated a large fraction of obsolete entries.</li>
<li>It then copies the live entries (those that contribute to the current system state) from those segments to the head of the log.</li>
<li>Finally, it frees the storage space for the segments, making that space available for new segments.</li>
</ol>
<p>To minimize the effect on normal operation, this process can be done concurrently [98].</p>
<p>As a result of copying the live entries forwards to the head of the log, the entries get to be out of order for replay. The entries can include additional information (e.g., version numbers) to recreate the correct ordering when the log is applied.</p>
<p>The policy of which segments are selected for cleaning has a big impact on performance; prior work proposes a cost-benefit policy that factors in not only the amount of space utilized by live<br>entries but also how long those entries are likely to remain live [97, 98].</p>
<p>Determining whether entries are live is the state machine’s responsibility. For example, in a keyvalue store, a log entry to set a key to a particular value is live if the key exists and is currently set to the given value. Determining whether a log entry that deletes a key is live is more subtle: it is live as long as any prior entries setting that key are present in the log. RAMCloud preserves deletion commands (called tombstones) as necessary [98], but another approach is to periodically write out a summary of the keys that are present in the current state, then all log entries regarding keys not listed are not live. Key-value stores are a fairly simple example; other state machines are possible, but unfortunately, determining liveness will be different for each.</p>
<h4 id="5-3-2-Basics-of-log-structured-merge-trees"><a href="#5-3-2-Basics-of-log-structured-merge-trees" class="headerlink" title="5.3.2 Basics of log-structured merge trees"></a>5.3.2 Basics of log-structured merge trees</h4><p>Log-structured merge trees (LSM trees) were first described by O’Neil [84] and were later popularized in distributed systems by BigTable [17]. They are used in systems such as Apache Cassandra [1] and HyperDex [27] and are available as libraries such as LevelDB [62] and its forks (e.g., RocksDB [96] and HyperLevelDB [39]).</p>
<p>LSM trees are tree-like data structures that store ordered key-value pairs. At a high level, they use disk similarly to log cleaning approaches: they write in large sequential strides and do not modify data on disk in place. However, instead of maintaining all state in the log, LSM trees reorganize the state for better random access.</p>
<p>A typical LSM tree keeps recently written keys in a small log on disk. When the log reaches a fixed size, it is sorted by key and written to a file called a run in sorted order. Runs are never modified in place, but a compaction process periodically merges multiple runs together, producing new runs and discarding the old ones. The merge is reminiscent of merge sort; when a key is in multiple input runs, only the latest version is kept, so the produced runs are more compact. The compaction strategy used in LevelDB is summarized in Figure 5.1; it segregates runs by age for efficiency (similar to log cleaning).</p>
<p>During normal operation, the state machine can operate on this data directly. To read a key, it first checks to see if that key was modified recently in its log, then checks each run. To avoid checking every run for a key on every lookup, some systems create a bloom filter for each run (a compact data structure which can say with certainty in some cases that a key does not appear in a run, though it may sometimes require searching a run even when a key is not present).</p>
<h4 id="5-3-3-Log-cleaning-and-log-structured-merge-trees-in-Raft"><a href="#5-3-3-Log-cleaning-and-log-structured-merge-trees-in-Raft" class="headerlink" title="5.3.3 Log cleaning and log-structured merge trees in Raft"></a>5.3.3 Log cleaning and log-structured merge trees in Raft</h4><p>We have not attempted to implement log cleaning or LSM trees in Raft, but we speculate that both would work well. Applying LSM trees to Raft appears to be fairly straightforward. Because the Raft log already stores recent entries durably on disk, the LSM tree can keep recent data in a more convenient tree format in memory. This would be fast for servicing lookups, and when the Raft log reached a fixed size, the tree would already be in sorted order to write to disk as a new run. Transferring the state from the leader to a slow follower requires sending all the runs to the follower (but not the in-memory tree); fortunately, runs are immutable, so there is no concern of the runs being modified during the transfer.</p>
<p><img src="/img/15RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20220416091413088.png" alt="image-20220416091413088"></p>
<p>Applying log cleaning to Raft is less obvious.We first considered an approach in which the Raft log was divided into segments and cleaned (see Figure 5.4(a)). Unfortunately, cleaning would place a lot of holes in the log where segments were freed, which would require a modified approach to log replication.We think this approach could be made to work, but it adds significant complexity to Raft and its interaction with the state machine. Moreover, since only the leader can append to the Raft log, cleaning would need to be leader-based, which would waste the leader’s network bandwidth (this is discussed further in Section 5.4).</p>
<p>A better approach would be to handle log cleaning similarly to LSM trees: Raft would keep a contiguous log for recent changes, and the state machine would keep its own state as a log, but these logs would be logically distinct (see Figure 5.4(b)). When the Raft log grew to a fixed size, its new entries would be written as a new segment in the state machine’s log, and the corresponding prefix of the Raft log would be discarded. Segments in the state machine would be cleaned independently on each server, and the Raft log would remain entirely unaffected by this. We prefer this approach over cleaning the Raft log directly, since the complexity of log cleaning is encapsulated entirely in the state machine (the interface between the state machine and Raft remains simple), and servers can clean independently.</p>
<p>As described, this approach would require the state machine to write all of Raft’s log entries into its own log (though it could do so in large batches). This additional copy could be optimized away by directly moving a file consisting of log entries from Raft’s log and incorporating that file into the state machine’s data structures. This could be a helpful optimization for performance-critical systems, but unfortunately, it would more tightly couple the state machine and the Raft module, since the state machine would need to understand the on-disk representation of the Raft log.</p>
<hr>
<h3 id="5-4-Alternative-leader-based-approaches-备选方案：基于领导者的方法"><a href="#5-4-Alternative-leader-based-approaches-备选方案：基于领导者的方法" class="headerlink" title="5.4 Alternative: leader-based approaches 备选方案：基于领导者的方法"></a>5.4 Alternative: leader-based approaches 备选方案：基于领导者的方法</h3><p><img src="/img/04RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20210928183359525.png" alt="image-20210928183359525"></p>
<p><em>快照步骤：</em></p>
<ol>
<li><em>当Raft日志大小超过1MB时，停止接收新的客户端请求；</em></li>
<li><em>等待日志中最后一个条目被apply；</em></li>
<li><em>序列化状态数据，将序列化的内容添加到日志末尾；</em></li>
<li><em>恢复处理客户端请求；</em></li>
<li><em>当服务器认识到快照条目被提交后，它丢弃之前的日志条目。</em></li>
</ol>
<p>本章之前介绍的日志压缩方法与Raft的强Leader原则不同，因为服务器在Leader不知情的情况下各自压缩日志。然而，我们认为这种背离是合理的。<font color=red>Leader的作用是避免在达成共识时出现决策冲突，但在进行快照时已经达成了共识，因此没有决策冲突。数据仍然只从Leader流向Follower，只不过Follower现在可以独立组织自己的数据。</font></p>
<p>The log compaction approaches presented in this chapter depart from Raft’s strong leader principle, since servers compact their logs without the knowledge of the leader. However, we think this departure is justified. While having a leader helps avoid conflicting decisions in reaching consensus, consensus has already been reached when snapshotting, so no decisions conflict. Data still only flows from leaders to followers, but followers can now reorganize their data independently.</p>
<p>我们也考虑了基于Leader的日志压缩方法，但这种方法对性能的影响都远大于其带来的好处。Leader压缩自己的日志，然后将其发送给Follower，而Follower本来也可以独立压缩自己的日志，这实际上是一种浪费。将冗余状态发送给每个Follower将浪费网络带宽并减慢压缩过程。每个Follower已经拥有压缩自己状态所需的信息，而Leader的出口网络带宽通常是Raft最宝贵的（瓶颈）资源。<font color=red>对于基于内存的快照，服务器从其本地状态直接生成快照通常比通过网络发送和接收快照成本低得多。</font>对于增量压缩方法，这更多地取决于硬件配置，但我们依然认为独立压缩成本更低。</p>
<p>We also considered leader-based approaches to log compaction, but any benefits are usually outweighed by performance considerations. It would be wasteful for the leader to compact its log, then send the result to the followers, when they could just as well compact their own logs independently. Sending the redundant state to each follower would waste network bandwidth and slow the compaction process. Each follower already has the information needed to compact its own state, and the leader’s outbound network bandwidth is usually Raft’s most precious (bottleneck) resource. For memory-based snapshots, it is typically much cheaper for a server to produce a snapshot from its local state than it is to send and receive one over the network. For incremental compaction approaches, this depends a bit more on the hardware configuration, but we also expect independent compaction to be cheaper.</p>
<h4 id="5-4-1-Storing-snapshots-in-the-log-在日志中存储快照"><a href="#5-4-1-Storing-snapshots-in-the-log-在日志中存储快照" class="headerlink" title="5.4.1 Storing snapshots in the log 在日志中存储快照"></a>5.4.1 Storing snapshots in the log 在日志中存储快照</h4><p>基于领导者的方法的一个可能的好处是，如果所有系统状态都可以存储在日志中，那么就不需要复制和持久化状态的新机制。因此，我们考虑了一种基于领导者的快照方法，Leader创建快照并将快照作为条目存储在Raft日志中，如下图5.5所示。然后，Leader使用AppendEntries RPC将此快照发送给其每个Follower。为了不中断正常请求，每个快照将被拆分为多个条目，并与日志中的正常客户端命令交叉存储在日志中。</p>
<p>One possible benefit to leader-based approaches is that, if all the system state could be stored in the log, then new mechanisms to replicate and persist the state would not be needed. Thus, we considered a leader-based approach to snapshotting in which the leader would create a snapshot and store the snapshot as entries in the Raft log, as shown in Figure 5.5. The leader would then send this snapshot to each of its followers using the AppendEntries RPC. To reduce any disruption on normal operation, each snapshot would be split into many entries and interleaved with normal client commands in the log.</p>
<p><img src="/img/04RAFT%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9(%E5%A4%A7%E8%AE%BA%E6%96%87)/image-20210928181911418.png" alt="image-20210928181911418"></p>
<p><em>图5.5：一种基于领导者的方法，将快照分块存储在日志中，并与客户端命令交错存储。快照内容从start条目开始，到end条目结束。快照存储在start和end之间的多个日志条目中。因此，客户端请求可以与快照并行进行，每个条目的大小都是有限的，并且条目附加到日志中的速率也是有限的：只有当Leader知道上一个快照块已提交时，下一个快照块才会附加到日志中。一旦每个服务器了解到end条目已提交，它就可以丢弃其日志中，start条目之前的内容。日志的重放需要两阶段算法：首先应用最后一个完整的快照，然后应用快照的start条目后的客户端请求。</em></p>
<p>与快照存储在日志之外相比，这种方法无需引入新的机制，因为服务器不需要额外的机制来传输或持久化快照（快照将像其他日志条目一样被复制和持久化）。然而，除了要为Follower浪费一些网络带宽（他们本可以轻松地制作自己的快照），这还有一个严重的问题。如果一个Leader在创建快照的过程中失效，它将在日志中留下不完整的快照。原则上，这种情况可能会反复发生，经过多次失败的快照尝试后，积累的垃圾将会耗尽服务器的存储容量。因此，我们认为这种机制在实践中不可行。</p>
<p>This would achieve better economy of mechanism than storing the snapshot outside the log, since servers would not need separate mechanisms to transfer snapshots or persist them (they would be replicated and persisted just like other log entries). However, in addition to wasting network bandwidth for followers that could just as easily produce their own snapshots, this has a serious problem. If a leader fails in the middle of creating a snapshot, it leaves a partial snapshot in the servers’ logs. In principle this could happen repeatedly and exhaust servers’ storage capacity with garbage accumulated from numerous failed snapshotting attempts. Thus, we don’t think this mechanism is viable in practice.</p>
<h4 id="5-4-2-Leader-based-approach-for-very-small-state-machines-小状态机上的基于Leader的压缩方法"><a href="#5-4-2-Leader-based-approach-for-very-small-state-machines-小状态机上的基于Leader的压缩方法" class="headerlink" title="5.4.2 Leader-based approach for very small state machines 小状态机上的基于Leader的压缩方法"></a>5.4.2 Leader-based approach for very small state machines 小状态机上的基于Leader的压缩方法</h4><p>然而对于非常小的状态机，在日志中存储快照不仅变得可行，而且还可以大大简化。如果快照足够小（最多大约1兆字节），它可以轻松地放入单个日志条目中，而不会中断正常操作太长时间。要以这种方式压缩服务器日志，Leader的操作步骤是：</p>
<ol>
<li>停止接受新的客户请求；</li>
<li>等待日志中的所有条目都提交，并等待其状态机应用其日志中的所有条目；</li>
<li>拍摄快照（同步）；</li>
<li>将快照附加到日志末尾的单个日志条目中；</li>
<li>继续接受新的客户端请求。</li>
</ol>
<p>For very small state machines, storing the snapshot in the log not only becomes viable but can also be simplified significantly. If the snapshot is small enough (up to about one megabyte), it can fit comfortably in a single log entry without interrupting normal operation for too long. To compact the servers’ logs in this way, the leader would:</p>
<ol>
<li>Stop accepting new client requests;</li>
<li>Wait for all entries in its log to be committed and its state machine to have applied all entries in its log;</li>
<li>Take a snapshot (synchronously);</li>
<li>Append the snapshot into a single log entry at the end of its log; and</li>
<li>Resume accepting new client requests.</li>
</ol>
<p>一旦每个服务器了解到快照条目已提交，它就可以在日志中丢弃快照之前的所有条目。在停止客户端请求和传输快照条目之前，这种方法会导致较小段时间的可用性问题，但对于非常小的状态机，其影响将是有限的。</p>
<p>Once each server learned that the snapshot entry was committed, it could discard every entry before the snapshot in its log. This approach would cause a small availability gap while client requests were stopped and the snapshot entry was transferred, but its impact would be limited for very small state machines.</p>
<p>这种更简单的方法避免了在日志之外持久化快照、避免了使用新的RPC传输快照以及并发快照的实现。然而，这种方法不适合较大的状态机。</p>
<p>This simpler approach avoids the implementation effort of persisting snapshots outside the log, transferring them using a new RPC, and snapshotting concurrently. However, successful systems tend to be used more than their original designers intended, and this approach would not work well for larger state machines.</p>
<hr>
<h3 id="5-5-Conclusion-总结"><a href="#5-5-Conclusion-总结" class="headerlink" title="5.5 Conclusion 总结"></a>5.5 Conclusion 总结</h3><p>本章讨论了RAFT中日志压缩的几种方法。不同系统使用不同的方法，具体取决于状态机的大小、所需的性能级别以及预算的复杂性。Raft支持多种方法，这些方法有一些共通的概念框架：</p>
<ul>
<li>每个服务器独立压缩其日志中的已提交条目。</li>
<li>状态机和Raft之间的基本交互涉及将已提交日志从Raft转移到状态机。一旦状态机将命令应用到磁盘中后，它就可以让Raft丢弃相应的日志条目。Raft保留它丢弃的最后一个条目的Index和Term，以及该索引之前的最新配置。</li>
<li>一旦Raft丢弃了日志的头部条目，状态机就承担了两个新的职责：在重新启动时加载状态，并提供一致的映像用以传输给慢Follower。</li>
</ul>
<p>This chapter discussed several approaches to log compaction in Raft, which are summarized in Figure 5.1. Different approaches are suitable for different systems, depending on the size of the state machine, the level of performance required, and the amount of complexity budgeted. Raft supports a wide variety of approaches that share a common conceptual framework:</p>
<ul>
<li>Each server compacts the committed prefix of its log independently.</li>
<li>The basic interaction between the state machine and Raft involves transferring responsibility for a prefix of the log from Raft to the state machine. Once the state machine has applied commands to disk, it instructs Raft to discard the corresponding prefix of the log. Raft retains the index and term of the last entry it discarded, along with the latest configuration as of that index.</li>
<li>Once Raft has discarded a prefix of the log, the state machine takes on two new responsibilities: loading the state on a restart and providing a consistent image to transfer to a slow follower.</li>
</ul>
<p>基于内存的状态机的快照方法已成功地应用于多个生产系统，包括Chubby和ZooKeeper，我们也已经在Logcard中实现了这种方法。尽管操作内存中的数据结构通常很快，但快照过程中的性能也可能会受到显著影响。并发快照有助于隐藏资源使用情况，将来，可以通过安排集群中的服务器在不同的时间进行快照，从而使快照根本不影响客户端。</p>
<p>Snapshotting for memory-based state machines is used successfully in several production systems, including Chubby and ZooKeeper, and we have implemented this approach in LogCabin. Although operating on an in-memory data structure is fast for most operations, performance during the snapshotting process may be significantly impacted. Snapshotting concurrently helps to hide the resource usage, and in the future, scheduling servers across the cluster to snapshot at different times might keep snapshotting from affecting clients at all.</p>
<p>基于磁盘的状态机在概念上很简单，因为可以原地改变它们的状态。它们仍然需要写时拷贝，以便将一致的磁盘映像传输到其他服务器，但这对于磁盘来说负担不大，因为磁盘会自然地分成块。但是，正常操作期间的随机磁盘写入速度会变慢，因此这种方法将限制系统的写入吞吐量。</p>
<p>Disk-based state machines that mutate their state in place are conceptually simple. They still require copy-on-write for transferring a consistent disk image to other servers, but this may be a small burden with disks, which naturally split into blocks. However, random disk writes during normal operation tend to be slow, so this approach will limit the system’s write throughput.</p>
<p>最后，增量方法可能是最有效的压缩方法。通过每次操作一小块状态，它们可以限制资源使用的爆发式增长（这种方法也可以并发压缩）。这种方法还可以避免将相同的数据重复写入磁盘；稳定的数据存储在磁盘上不经常被压缩的区域。虽然实现增量压缩可能很复杂，但这种复杂性可以转移到类似LevelDB这样的库中。此外，通过将数据结构保留在内存中并在内存中缓存更多磁盘的内容，使用增量压缩的客户端操作的性能可以接近基于内存的状态机。</p>
<p>Ultimately, incremental approaches can be the most efficient form of compaction. By operating on small pieces of the state at a time, they can limit bursts in resource usage (and they can also compact concurrently). They can also avoid writing the same data out to disk repeatedly; stable data should make its way to a region of disk that does not get compacted often. While implementing incremental compaction can be complex, this complexity can be offloaded to a library such as LevelDB. Moreover, by keeping data structures in memory and caching more of the disk in memory, the performance for client operations with incremental compaction can approach that of memory based state machines.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>raft论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>Time,Clocks,and the Ordering of Events in a Distributed System</title>
    <url>/2022/06/24/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/%E8%AE%BA%E6%96%87/Time,Clocks,and%20the%20Ordering%20of%20Events%20in%20a%20Distributed%20System/</url>
    <content><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>时间概念是我们思维方式的基础。而它源于事件发生的顺序这个更基本的概念。事件的时间顺序的概念渗透到我们对系统的思考中。然而，在考虑分布式系统中的事件时，必须仔细重新审视这个概念。</p>
<p>The concept of time is fundamental to our way of thinking. It is derived from the more basic concept of the order in which events occur. We say that something happened at 3:15 if it occurred after our clock read 3:15 and before it read 3:16. The concept of the temporal ordering of events pervades our thinking about systems. For example, in an airline reservation system we specify that a request for a reservation should be granted if it is made before the flight is filled. However, we will see that this concept must be carefully reexamined when considering events in a distributed system.</p>
<span id="more"></span>
<p>分布式系统由一组在空间上分离的进程组成，这些进程通过交换消息相互通信。</p>
<p>A distributed system consists of a collection of distinct processes which are spatially separated, and which communicate with one another by exchanging messages. A network of interconnected computers, such as the ARPA net, is a distributed system. A single computer can also be viewed as a distributed system in which the central control unit, the memory units, and the input-output channels are separate processes. A system is distributed if the message transmission delay is not negligible compared to the time between events in a single process.</p>
<p>We will concern ourselves primarily with systems of spatially separated computers. However, many of our remarks will apply more generally. In particular, a multiprocessing system on a single computer involves problems similar to those of a distributed system because of the unpredictable order in which certain events can occur.</p>
<p><font color=red>在分布式系统中，有时说不清两个事件中的哪一个先发生。因此，“happened before”的关系只是系统中偏序关系。</font>我们发现这里经常会出问题，因为人们没有充分意识到这一事实及其影响。</p>
<p>In a distributed system, it is sometimes impossible to say that one of two events occurred first. The relation “happened before” is therefore only a partial ordering of the events in the system. We have found that problems often arise because people are not fully aware of this fact and its implications.</p>
<p>这篇论文中，我们讨论了由”happened before”关系定义的偏序关系，并给出了将其扩展到所有事件上的一致的全序关系上的分布式算法。该算法为实现一个分布式系统提供了一种有用的机制。我们用一个简单的用于解决同步问题的方法来距离说明该算法。如果通过该算法获得的顺序与用户感知的顺序不同，则可能会发生意外的异常行为。这可以通过引入真实的物理时钟来避免。我们描述了一种同步这些时钟的简单方法，并推导了它们偏离同步的上限。</p>
<p>In this paper, we discuss the partial ordering defined by the “happened before” relation, and give a distributed algorithm for extending it to a consistent total ordering of all the events. This algorithm can provide a useful mechanism for  implementing a distributed system. We illustrate its use with a simple method for solving synchronization problems. Unexpected, anomalous behavior can occur if the ordering obtained by this algorithm differs from that perceived by the user. This can be avoided by introducing real, physical clocks. We describe a simple method for synchronizing these clocks, and derive an upper bound on how far out of synchrony they can drift.</p>
<hr>
<h3 id="The-Partial-Ordering"><a href="#The-Partial-Ordering" class="headerlink" title="The Partial Ordering"></a>The Partial Ordering</h3><p>如果事件<code>a</code>发生的时间早于<code>b</code>，大多数人可能会说<code>a</code>发生在<code>b</code>之前（happened before）。他们可能会用时间的物理理论来证明这个定义。然而，如果<font color=red>想要系统满足某种规范，那必须根据系统内可观察到的事件来给出该规范。</font>如果该规范与物理时间相关，那系统必须包含实际的物理时钟。即使该系统确实包含真实的物理时钟，仍然存在这样的问题，因为这种时钟不可能是完全精确的，不能保持精确的物理时间。因此，我们将在不使用物理时钟的情况下定义“happened before”关系。</p>
<p>Most people would probably say that an event a happened before an event b if a happened at an earlier time than b. They might justify this definition in terms of physical theories of time. However, if a system is to meet a specification correctly, then that specification must be given in terms of events observable within the system. If the specification is in terms of physical time, then the system must contain real clocks. Even if it does contain real clocks, there is still the problem that such clocks are not perfectly accurate and do not keep precise physical time. We will therefore define the “happened before” relation without using physical clocks.</p>
<p>现在我们更加精确的定义系统。我们假设系统由一些进程组成。每个进程则包含一系列的事件，如果事件<code>a</code>在事件<code>b</code>之前发生，则这个序列中事件<code>a</code>会出现在事件<code>b</code>之前。因此单个进程可以定义为满足偏序关系的一组事件。</p>
<p>We begin by defining our system more precisely. We assume that the system is composed of a collection of processes. Each process consists of a sequence of events. Depending upon the application, the execution of a subprogram on a computer could be one event, or the execution of a single machine instruction could be one a sequence, where a occurs before b in this sequence if a happens before b. In other words, a single process is defined to be a set of events with an a priori total ordering. This seems to be what is generally meant by a process. It would be trivial to extend our definition to allow a process to split into distinct subprocesses, but we will not bother to do so. </p>
<p>我们假设进程中发送或者接收消息就是一个事件。因此我们将”happened before”关系表示为”–&gt;”，<font color=red>定义如下</font>：”–&gt;”是满足以下三个条件的最小关系：</p>
<ol>
<li>如果事件<code>a</code>和<code>b</code>是同一个进程中的事件，并且<code>a</code>出现在<code>b</code>之前，则<code>a--&gt;b</code>;</li>
<li>如果<code>a</code>是某个进程中发送消息的事件，而<code>b</code>是其他进程中接收该消息的事件，则<code>a--&gt;b</code>;</li>
<li>如果<code>a--&gt;b</code>并且<code>b--&gt;c</code>，则<code>a--&gt;c</code>。如果<code>a-/-&gt;b</code>，并且<code>b-/-&gt;a</code>，则两个事件<code>a</code>和<code>b</code>被认为是并发执行的；</li>
</ol>
<p>We assume that sending or receiving a message is an event in a process. We can then define the “happened before” relation, denoted by “–&gt;”, as follows. </p>
<p>Definition. The relation “–&gt;” on the set of events of a system is the smallest relation satisfying the following three conditions: (1) If a and b are events in the same process, and a comes before b, then a –&gt; b. (2) If a is the sending of a message by one process and b is the receipt of the same message by another process, then a –&gt; b. (3) If a –&gt; b and b –&gt; c then a –&gt; c. Two distinct events a and b are said to be concurrent if a -&#x2F;-&gt; b and b -&#x2F;-&gt; a. </p>
<p>我们假设任何事件<code>a</code>都满足<code>a-/-&gt;a</code>（在系统中的事件可以先于自身发生似乎没有物理意义）。这意味着<code>--&gt;</code>是系统中所有事件集上的<font color=red>偏序关系</font>。</p>
<p>We assume that a -&#x2F;-&gt; a for any event a. (Systems in which an event can happen before itself do not seem to be physically meaningful.) This implies that –&gt; is an irreflexive partial ordering on the set of all events in the system.</p>
<p>下面的时空图图1是个例子。横向表示空间，竖向表示时间，向上表示时间的递增。点表示事件，垂线表示进程，波浪线表示消息。容易看出，<code>a--&gt;b</code>意味着可以在消息线上从<code>a</code>转到<code>b</code>。比如图1中有<code>p1--&gt;p4</code>。</p>
<p>![image-20220614082038586](&#x2F;img&#x2F;Time,Clocks,and the Ordering of Events in a Distributed System&#x2F;image-20220614082038586.png)</p>
<p>It is helpful to view this definition in terms of a “space-time diagram” such as Figure 1. The horizontal direction represents space, and the vertical direction represents time–later times being higher than earlier ones. The dots denote events, the vertical lines denote processes, and the wavy lines denote messages. It is easy to see that a –&gt; b means that one can go from a to b in and message lines. For example, we have p1 –&gt; p4 in Figure 1.</p>
<p>该定义的另一种解释方式是，<code>a--&gt;b</code>表示事件<code>a</code>会对事件<code>b</code>产生因果影响。如果两个事件都不能对另一个产生因果影响，则两个事件是并发的。例如，图1中的事件<code>p3</code>和<code>q3</code>是并发的。尽管我们绘制的图中暗示<code>q3</code>的发生在物理时间上早于<code>p3</code>，但进程<code>P</code>在<code>p4</code>收到消息之前无法知道进程<code>Q</code>在<code>q3</code>做了什么。（在事件<code>p4</code>之前，<code>P</code>最多只能知道<code>Q</code>在<code>q3</code>计划做什么）</p>
<p>Another way of viewing the definition is to say that a –&gt; b means that it is possible for event a to causally affect event b. Two events are concurrent if neither can causally affect the other. For example, events p3 and q3 of Figure 1 are concurrent. Even though we have drawn the diagram to imply that q3 occurs at an earlier physical time than p3, process P cannot know what process Q did at q3 until it receives the message at p4. (Before event p4, P could at most know what Q was planning to do at q3)</p>
<p>对于熟悉狭义相对论不变时空公式的读者来说，这一定义将显得非常自然。</p>
<p>This definition will appear quite natural to the reader familiar with the invariant space-time formulation of special relativity, as described for example in [1] or the first chapter of [2]. In relativity, the ordering of events is defined in terms of messages that could be sent. However, we have taken the more pragmatic approach of only considering messages that actually are sent. We should be able to determine if a system performed correctly by knowing only those events which did occur, without knowing which events could have occurred.</p>
<hr>
<h3 id="Logical-Clocks"><a href="#Logical-Clocks" class="headerlink" title="Logical Clocks"></a>Logical Clocks</h3><p>现在把时钟引入到系统中。从一个抽象的观点开始，这个观点中，<font color=red>时钟就是为每个事件分配一个数字</font>，该数字表示事件发生的时间。更详细的说法是：为每个进程<code>Pi</code>定义一个时钟函数<code>Ci</code>，该函数可以为该进程中的每个事件<code>a</code>分配一个数字<code>Ci(a)</code>。而整个系统的时钟由函数<code>C</code>表示，它会为任一事件b分配一个<code>C(b)</code>，并且如果b是<code>Pj</code>进程中的时间的话，则<code>C(b) = Cj(b)</code>。目前我们没有对<code>Ci(a)</code>和物理时间的关系做任何的假设，所以可以认为<code>Ci</code>实际上就是一个逻辑时钟，而非物理时钟。</p>
<p>We now introduce clocks into the system. We begin with an abstract point of view in which a clock is just a way of assigning a number to an event, where the number is thought of as the time at which the event occurred. More precisely, we define a clock Ci for each process Pi to be a function which assigns a number Ci(a) to any event a in that process. The entire system of clocks is represented by the function C which assigns to any event b the number C(b), where C(b) &#x3D; Cj(b) if b is an event in process Pj. For now, we make no assumption about the relation of the numbers Ci(a) to physical time, so we can think of the clocks Ci as logical rather than physical clocks. They may be implemented by counters with no actual timing mechanism.</p>
<p>现在考虑这样一个时钟系统的<font color=red>正确性</font>。我们肯定不能基于物理时间来定义正确性，因为这需要进入物理时钟。因此正确性的定义必须是基于事件发生的顺序的。最合理的条件就是：如果一个事件<code>a</code>发生在事件<code>b</code>之前，那<code>a</code>应该发生在比<code>b</code>更早的时间。该条件语句正式描述是：</p>
<p><font color=red>Clock Condition</font>: 对于任意的事件<code>a</code>和<code>b</code>：如果<code>a--&gt;b</code>，那么<code>C(a) &lt; C(b)</code>.</p>
<p>We now consider what it means for such a system of clocks to be correct. We cannot base our definition of correctness on physical time, since that would require introducing clocks which keep physical time. Our definition must be based on the order in which events occur. The strongest reasonable condition is that if an event a occurs before another event b, then a should happen at an earlier time than b. We state this condition more formally as follows. </p>
<p>Clock Condition. For any events a, b: if a–&gt; b then C(a) &lt; C(b).</p>
<p>注意该命题的反命题不成立，否则的话就表示两个并发执行的事件必须发生在相同的时间，这显然是不成立的。（证明：如果反命题成立，即 <code>if C(a) &lt; C(b) then a--&gt;b</code>，则其逆反命题，即 <code>if a-/-&gt;b, then C(a) &gt;= C(b)</code> 也成立。<code>a</code>和<code>b</code>事件并发表示<code>a-/-&gt;b</code>，同时<code>b-/-&gt;a</code>，所以得到结论<code>C(a) &gt;= C(b)</code>，且<code>C(b) &gt;= C(a)</code>，所以<code>C(b) == C(a)</code>. 证毕）</p>
<p>Note that we cannot expect the converse condition to hold as well, since that would imply that any two concurrent events must occur at the same time. In Figure 1, p2 and p3 are both concurrent with q3, so this would mean that they both must occur at the same time as q3, which would contradict the Clock Condition because p2 –&gt; p3. </p>
<p>从我们对<code>&quot;--&gt;&quot;</code>关系的定义可以看出，如果满足下面两个条件的话，则Clock Condition也会得到满足：</p>
<ul>
<li><code>C1</code>. 如果<code>a</code>和<code>b</code>是进程<code>Pi</code>中的事件，并且<code>a</code>在<code>b</code>之前，则<code>Ci(a) &lt; Ci(b)</code>. </li>
<li><code>C2</code>. 如果<code>a</code>是<code>Pi</code>进程中发送消息的事件，而<code>b</code>是<code>Pj</code>进程中接收该消息的事件，则<code>Ci(a) &lt; Cj(b)</code>.</li>
</ul>
<p>It is easy to see from our definition of the relation “–&gt;” that the Clock Condition is satisfied if the following two conditions hold. </p>
<ul>
<li>C1. If a and b are events in process Pi, and a comes before b, then Ci(a) &lt; Ci(b). </li>
<li>C2. If a is the sending of a message by process Pi and b is the receipt of that message by process Pj, then Ci(a) &lt; Cj(b).</li>
</ul>
<p>现在从时空图的角度来考虑时钟。设想一个进程的时钟每次tick都会递增数字，tick发生在进程的事件之间。比如，假设<code>a</code>和<code>b</code>是进程<code>Pi</code>中相继出现的两个事件，<code>Ci(a)</code>为<code>4</code>，<code>Ci(b)</code>为<code>7</code>，时钟tick <code>5,6,7</code>发生在两个事件期间。我们画了一条虚线的tick线，穿过所有不同进程相同编号的tick。从而图1的时空图变成了图2那样。<code>C1</code>命题表示一个进程中两个事件之间必然有一条tick线，而<code>C2</code>命题则意味着每条消息线必然穿过一条tick线。从–&gt;的图示含义就很容易看出为什么这两个命题就能意味着Clock Condition的成立。</p>
<p>![image-20220616083214152](&#x2F;img&#x2F;Time,Clocks,and the Ordering of Events in a Distributed System&#x2F;image-20220616083214152.png)</p>
<p>Let us consider the clocks in terms of a space-time diagram. We imagine that a process’ clock “ticks” through every number, with the ticks occurring between the process’ events. For example, if a and b are consecutive events in process Pi with Ci(a) &#x3D; 4 and Ci(b) &#x3D; 7, then clock ticks 5, 6, and 7 occur between the two events. We draw a dashed “tick line” through all the like numbered ticks of the different processes. The spacetime diagram of Figure 1 might then yield the picture in Figure 2. Condition C1 means that there must be a tick line between any two events on a process line, and condition C2 means that every message line must cross a tick line. From the pictorial meaning of –&gt;, it is easy to see why these two conditions imply the Clock Condition.</p>
<p>我们可以将tick线视为时空上某个笛卡尔坐标系的时间坐标线。从而可以重新绘制图2来拉直这些坐标线，从而得到图3。图3是图2表示的相同的事件系统的有效替代方法。</p>
<p>![image-20220616083613225](&#x2F;img&#x2F;Time,Clocks,and the Ordering of Events in a Distributed System&#x2F;image-20220616083613225.png)</p>
<p>We can consider the tick lines to be the time coordinate lines of some Cartesian coordinate system on spacetime. We can redraw Figure 2 to straighten these coordinate lines, thus obtaining Figure 3. Figure 3 is a valid alternate way of representing the same system of events as Figure 2. Without introducing the concept of physical time into the system (which requires introducing physical clocks), there is no way to decide which of these pictures is a better representation.</p>
<p>The reader may find it helpful to visualize a two dimensional spatial network of processes, which yields a three-dimensional space-time diagram. Processes and messages are still represented by lines, but tick lines become two-dimensional surfaces.</p>
<p>现在假设进程是一种算法，事件表示算法执行过程中的某些操作。我们将展示在进程中引入时钟以满足Clock Condition。进程<code>Pi</code>的时钟由<code>Ci</code>表示，所以<code>Ci(a)</code>就表示事件a发生时从<code>Ci</code>获得的时钟值。<code>Ci</code>的值在事件发生期间进行递增，所以改变<code>Ci</code>的值本身并不是一个事件。</p>
<p>Let us now assume that the processes are algorithms, and the events represent certain actions during their execution. We will show how to introduce clocks into the processes which satisfy the Clock Condition. Process Pi’s clock is represented by a register Ci, so that Ci(a) is the value contained by Ci during the event a. The value of Ci  will change between events, so changing Ci does not itself constitute an event.</p>
<p><font color=red>为了保证系统时钟满足Clock Condition，我们需要满足<code>C1</code>和<code>C2</code>。</font>满足<code>C1</code>很简单，只要进程满足下面的规则：</p>
<p><font color=red><code>IR1</code></font>：每个进程<code>Pi</code>在两个相继的事件中间会增加<code>Ci</code>的值；</p>
<p>为了满足<code>C2</code>，需要每个消息<code>m</code>包含时间戳<code>Tm</code>，<code>Tm</code>就是该消息发送的时间戳。其他进程收到包含时间戳<code>Tm</code>的消息后，需要将其进程时钟调整到<code>Tm</code>后的时间，更精确的说法是：</p>
<p><font color=red><code>IR2</code></font>：(a)如果进程<code>Pi</code>的事件<code>a</code>发送消息<code>m</code>，那<code>m</code>中包含了时间戳<code>Tm</code>，<code>Tm</code>等于<code>Ci(a)</code>；(b)进程<code>Pj</code>收到消息<code>m</code>后，将其进程时钟<code>Cj</code>的值设置为大于<code>Tm</code>，且大于等于当前<code>Cj</code>的值；</p>
<p>在IR2(b)中，我们认为表示收到消息m的事件发生在设置Cj之后。显然，IR2能确保C2得到满足。因此，实现了规则IR1和IR2就意味着满足Clock Condition，因此它们保证了正确的逻辑时钟系统。</p>
<p>To guarantee that the system of clocks satisfies the Clock Condition, we will insure that it satisfies conditions C1 and C2. Condition C1 is simple; the processes need only obey the following implementation rule:</p>
<p>IR1. Each process Pi increments Ci between any two successive events.</p>
<p>To meet condition C2, we require that each message m contain a timestamp Tm which equals the time at which the message was sent. Upon receiving a message timestamped Tm, a process must advance its clock to be later than Tm. More precisely, we have the following rule.</p>
<p>IR2. (a) If event a is the sending of a message m by process Pi, then the message m contains a timestamp Tm &#x3D; Ci(a). (b) Upon receiving a message m, process Pj sets Cj greater than or equal to its present value and greater than Tm.</p>
<p>In IR2(b) we consider the event which represents the receipt of the message m to occur after the setting of Cj. (This is just a notational nuisance, and is irrelevant in any actual implementation.) Obviously, IR2 insures that C2 is satisfied. Hence, the simple implementation rules IR l and IR2 imply that the Clock Condition is satisfied, so they guarantee a correct system of logical clocks.</p>
<hr>
<h3 id="Ordering-the-Events-Totally-对事件全排序"><a href="#Ordering-the-Events-Totally-对事件全排序" class="headerlink" title="Ordering the Events Totally    对事件全排序"></a>Ordering the Events Totally    对事件全排序</h3><p>我们可以使用满足Clock Condition的时钟系统对所有系统事件集进行总排序。我们只是按照事件发生的时间来排序。为了打破联系，我们使用任意的进程全序关系<code>≺</code>（注意<code>≺</code>与<code>&lt;</code>不一样） 。更准确地说，我们<font color=red>定义一个关系<code>==&gt;</code>如下</font>：如果<code>a</code>是进程<code>Pi</code>中的事件，<code>b</code>是进程<code>Pj</code>中的事件，那么<code>a==&gt;b</code>当且仅当(i)<code>Ci(a)&lt;Cj(b)</code> 或(ii)<code>Ci(a)==Cj(b)</code>且<code>Pi≺Pj</code>。很容易看出，这是一种全序关系，Clock Condition意味着如果<code>a--&gt;b</code>，则<code>a==&gt;b</code>。换句话说，关系<code>==&gt;</code>是将“happened before”这种偏序<font color=red>扩展到全序</font>的一种方式。</p>
<p>We can use a system of clocks satisfying the Clock Condition to place a total ordering on the set of all system events. We simply order the events by the times at which they occur. To break ties, we use any arbitrary total ordering ≺ of the processes. More precisely, we define a relation &#x3D;&#x3D;&gt; as follows: if a is an event in process Pi and b is an event in process Pj, then a &#x3D;&#x3D;&gt; b if and only if either (i) Ci{a) &lt; Cj(b) or (ii) Ci(a) &#x3D;&#x3D;  Cj(b) and Pi ≺ Pj. It is easy to see that this defines a total ordering, and that the Clock Condition implies that if a –&gt; b then a &#x3D;&#x3D;&gt; b. In other words, the relation &#x3D;&#x3D;&gt; is a way of completing the “happened before” partial ordering to a total ordering.</p>
<p>排序<code>==&gt;</code>取决于时钟系统<code>Ci</code>，并且不是唯一的。选择满足Clock Condition的不同时钟会产生不同的<code>==&gt;</code>关系（个人理解：每个进程的逻辑时钟可以自行决定tick的频率，只要满足IR1和IR2即可。比如Ci和Cj，如果Ci每秒递增10个tick，而Cj每秒递增100个tick，则Pi和Pj不相关的事件，一般是Pi的事件排在前面，一旦二者的事件之间有交互，只要按照IR2的规则调整Ci就可以）。给定任何扩展<code>--&gt;</code>的全序关系<code>==&gt;</code>，总有一个满足Clock Condition的时钟系统能产生该关系。只有偏序才是由系统中的事件唯一确定的。</p>
<p>The ordering &#x3D;&#x3D;&gt; depends upon the system of clocks Ci, and is not unique. Different choices of clocks which satisfy the Clock Condition yield different relations &#x3D;&#x3D;&gt;. Given any total ordering relation &#x3D;&#x3D;&gt; which extends –&gt;, there is a system of clocks satisfying the Clock Condition which yields that relation. It is only the partial ordering which is uniquely determined by the system of events.</p>
<p>能够定义事件的全序关系对于实现分布式系统非常有用。事实上，实现正确的逻辑时钟系统的原因就是为了获得这样的全序关系。我们将通过解决下面的互斥问题来说明事件全序关系的使用。考虑一个由若干进程组成的系统，这些进程共享单个资源。一次只能有一个进程使用该资源，因此这些进程必须同步以避免冲突。我们希望找到一种用于将资源授予进程的算法，它满足以下三个条件：</p>
<ul>
<li>(I) 已授予资源的进程必须先释放资源，然后该资源才能授予另一个进程。</li>
<li>(II) 对资源的请求必须按照请求的顺序授予资源。</li>
<li>(III) 如果每个被授予资源的进程最终都会释放它，那么每个请求最终都会被授予。</li>
</ul>
<p>Being able to totally order the events can be very useful in implementing a distributed system. In fact, the reason for implementing a correct system of logical clocks is to obtain such a total ordering. We will illustrate the use of this total ordering of events by solving the following version of the mutual exclusion problem. Consider a system composed of a fixed collection of processes which share a single resource. Only one process can use the resource at a time, so the processes must synchronize themselves to avoid conflict. We wish to find an algorithm for granting the resource to a process which satisfies the following three conditions: (I) A process which has been granted the resource must release it before it can be granted to another process. (II) Different requests for the resource must be granted in the order in which they are made. (III) If every process which is granted the resource eventually releases it, then every request is eventually granted.</p>
<p>假设初始情况下，资源被授予了单个进程。</p>
<p>这些都是非常自然的要求。它们精确地指定了解决方案为何正确的含义。考虑上面涉及事件顺序的条件，条件II没有对并发产生的请求，哪个应该先被授予请求有任何要求。</p>
<p>必须认识到这是一个非常重要的问题。除非做出额外的假设，否则使用中央调度进程来将资源按照收到请求的顺序进行授予是无法满足条件的。假设<code>P0</code>就是中央调度进程，<code>P1</code>向<code>P0</code>发送请求，然后向<code>P2</code>发了一个消息，收到消息后，<code>P2</code>向<code>P0</code>发送请求。<code>P2</code>的请求可能在<code>P1</code>的请求到达<code>P0</code>之前先到达。如果<code>P2</code>的请求首先被批准，则违反条件II。</p>
<p>We assume that the resource is initially granted to exactly one process.</p>
<p>These are perfectly natural requirements. They precisely specify what it means for a solution to be correct. Observe how the conditions involve the ordering of events. Condition II says nothing about which of two concurrently issued requests should be granted first.</p>
<p>It is important to realize that this is a nontrivial problem. Using a central scheduling process which grants requests in the order they are received will not work, unless additional assumptions are made. To see this, let P0 be the scheduling process. Suppose P1 sends a request to P0 and then sends a message to P2. Upon receiving the latter message, P2 sends a request to P0. It is possible for P2’s request to reach P0 before Pl’s request does. Condition II is then violated if P2’s request is granted first.</p>
<p>为了解决这个问题，我们使用规则<code>IR1</code>和<code>IR2</code>实现了一个时钟系统，并使用它们定义所有事件的全序关系<code>==&gt;</code>。这提供了所有请求和释放操作的全序关系。通过这种排序，解决方法变得很简单。它只需确保每个进程能了解所有其他进程的操作。</p>
<p>To solve the problem, we implement a system of clocks with rules IR1 and IR2, and use them to define a total ordering &#x3D;&#x3D;&gt; of all events. This provides a total ordering of all request and release operations. With this ordering, finding a solution becomes a straightforward exercise. It just involves making sure that each process learns about all other processes’ operations.</p>
<p>为了简化问题，我们做了如下的假设，它们不是必需的，但引入它们是为了避免分散注意力到实现细节上：</p>
<ul>
<li>假设对于任何两个进程<code>Pi</code>和<code>Pj</code>，从<code>Pi</code>发送到<code>Pj</code>的消息的接收顺序与发送顺序相同。</li>
<li>假设每个消息最终都会被接收到。</li>
<li>假设一个进程可以直接向其他每个进程发送消息。</li>
</ul>
<p>To simplify the problem, we make some assumptions. They are not essential, but they are introduced to avoid distracting implementation details. We assume first of all that for any two processes P&#x2F;and Pj, the messages sent from Pi to Pi are received in the same order as they are sent. Moreover, we assume that every message is eventually received. (These assumptions can be avoided by introducing message numbers and message acknowledgment protocols.) We also assume that a process can send messages directly to every other process.</p>
<p>每个进程都维护自己内部的请求队列，该队列外部不可见。假设请求队列最初包含一个消息<code>T0:P0</code>，其中<code>P0</code>是最初被授予资源的进程，<code>T0</code>小于任何时钟的初始值。</p>
<p>Each process maintains its own request queue which is never seen by any other process. We assume that the request queues initially contain the single message T0:P0 requests resource, where P0 is the process initially granted the resource and T0 is less than the initial value of any clock.</p>
<p>该算法由下面的5条规则定义：</p>
<ol>
<li>为了请求资源，进程<code>Pi</code>向其他每个进程发送消息<code>Tm:Pi</code> 请求资源，并将该消息放在其请求队列中，其中<code>Tm</code>是消息的时间戳。</li>
<li>当进程<code>Pj</code>接收到消息<code>Tm:Pi</code> 时，将其放在其请求队列中，并向<code>Pi</code>发送（附带时间戳）确认消息。</li>
<li>为了释放资源，进程<code>Pi</code>从其请求队列中删除所有<code>Tm:Pi</code> 消息，并向其他每个进程发送一条（附带时间戳）<code>Pi</code> 释放资源消息。</li>
<li>当进程<code>Pj</code> 收到 <code>Pi</code> 释放资源消息时，它会从其请求队列中删除所有<code>Tm:Pi</code> 消息。</li>
<li>当满足以下两个条件时，进程<code>Pi</code>被授予资源：(i) 其请求队列中有一条<code>Tm:Pi</code> 消息，该消息按<code>==&gt;</code>关系排序，排在队列中任何其他请求资源消息之前（为了定义消息的<code>“==&gt;”</code>关系，我们用发送事件来标识消息）。(ii) <code>Pi</code>已从其他每个进程接收到时间戳晚于<code>Tm</code>的消息。</li>
</ol>
<p>上述规则5中的(i)和(ii)规则都由进程<code>Pi</code>自行检测。</p>
<p>The algorithm is then defined by the following five rules. For convenience, the actions defined by each rule are assumed to form a single event.</p>
<ol>
<li><p>To request the resource, process Pi sends the message Tm:Pi requests resource to every other process, and puts that message on its request queue, where Tm is the timestamp of the message.</p>
</li>
<li><p>When process Pj receives the message Tm:Pi requests resource, it places it on its request queue and sends a (timestamped) acknowledgment message to Pi.</p>
</li>
<li><p>To release the resource, process Pi removes any Tm:Pi requests resource message from its request queue and sends a (timestamped) Pi releases resource message to every other process.</p>
</li>
<li><p>When process Pj receives a Pi releases resource message, it removes any Tm:Pi requests resource message from its request queue.</p>
</li>
<li><p>Process Pi is granted the resource when the following two conditions are satisfied: (i) There is a Tm:Pi requests resource message in its request queue which is ordered before any other request in its queue by the relation &#x3D;&#x3D;&gt;. (To define the relation “&#x3D;&#x3D;&gt;” for messages, we identify a message with the event of sending it.) (ii) Pi has received a message from every other process timestamped later than Tm.</p>
</li>
</ol>
<p>Note that conditions (i) and (ii) of rule 5 are tested locally by Pi.</p>
<p>很容易验证这些规则定义的算法满足条件I-III。首先，考虑规则5的条件(ii)以及消息按顺序接收的假设，可以确保<code>Pi</code>已经了解了当前请求之前的所有请求。由于规则3和4是唯一从请求队列中删除消息的规则，因此很容易得到条件I被满足了。条件II源自这样一个事实，即全序关系<code>==&gt;</code>扩展了偏序关系<code>--&gt;</code>。规则2保证在<code>Pi</code>请求资源后，规则5的条件(ii)最终将得到满足。规则3和4意味着，如果每个被授予资源的进程最终释放了资源，那么规则5的条件(i)最终将被满足，从而证明条件III。</p>
<p>It is easy to verify that the algorithm defined by these rules satisfies conditions I-III. First of all, observe that condition (ii) of rule 5, together with the assumption that messages are received in order, guarantees that Pi has learned about all requests which preceded its current request. Since rules 3 and 4 are the only ones which delete messages from the request queue, it is then easy to see that condition I holds. Condition II follows from the fact that the total ordering &#x3D;&#x3D;&gt; extends the partial ordering –&gt;. Rule 2 guarantees that after Pi requests the resource, condition (ii) of rule 5 will eventually hold. Rules 3 and 4 imply that if each process which is granted the resource eventually releases it, then condition (i) of rule 5 will eventually hold, thus proving condition III.</p>
<p>这是一个分布式的算法。每个进程都独立地遵循这些规则，并且没有中央同步进程或中央存储。这种方法可以泛化为实现分布式多进程系统所需的任何同步。同步以<font color=red>状态机</font>的形式指定，状态机由一组可能的命令<code>C</code>、一组可能的状态<code>S</code>和一个函数<code>e</code>: <code>C×S-&gt;S</code>组成。 <code>e(C, S) = S&#39;</code>表示状态机处于<code>S</code>状态时执行命令<code>C</code>会导致状态机状态变为<code>S’</code>。在我们的示例中，集合<code>C</code>由所有<code>Pi</code> requests resource命令和<code>Pi</code> releases resource命令组成，状态由等待requests命令的队列组成，其中队列头部的请求是当前被授予的请求。执行requests 命令会将requests 添加到队列的尾部，执行releases 命令会从“队列”中删除命令。</p>
<p>This is a distributed algorithm. Each process independently follows these rules, and there is no central synchronizing process or central storage. This approach can be generalized to implement any desired synchronization for such a distributed multiprocess system. The synchronization is specified in terms of a State Machine, consisting of a set C of possible commands, a set S of possible states, and a function e: C×S-&gt;S. The relation e(C, S) – S’ means that executing the command C with the machine in state S causes the machine state to change to S’. In our example, the set C consists of all the commands Pi requests resource and Pi releases resource, and the state consists of a queue of waiting request commands, where the request at the head of the queue is the currently granted one. Executing a request command adds the request to the tail of the queue, and executing a release command removes a command from the queue.</p>
<p>每个进程使用所有进程发出的命令，独立地驱动状态机的执行。能够实现同步是因为所有进程都根据其时间戳（使用关系<code>==&gt;</code>）对命令进行排序，因此每个进程使用的是相同的命令序列。当一个进程收到所有进发来的时间戳小于或等于<code>T</code>的所有命令时，它就可以执行时间戳为<code>T</code>的命令。算法精确描述很简单，我们不必费心描述它。</p>
<p>Each process independently simulates the execution of the State Machine, using the commands issued by all the processes. Synchronization is achieved because all processes order the commands according to their timestamps (using the relation &#x3D;&#x3D;&gt;), so each process uses the same sequence of commands. A process can execute a command timestamped T when it has learned of all commands issued by all other processes with timestamps less than or equal to T. The precise algorithm is straightforward, and we will not bother to describe it.</p>
<p>这种方法允许在分布式系统中实现任何所需形式的多进程同步。然而，该算法需要所有进程的积极参与。一个进程必须知道其他进程发出的所有命令，因此单个进程的故障将使其他任何进程都无法执行状态机命令，从而系统停止。</p>
<p>This method allows one to implement any desired form of multiprocess synchronization in a distributed system. However, the resulting algorithm requires the active participation of all the processes. A process must know all the commands issued by other processes, so that the failure of a single process will make it impossible for any other process to execute State Machine commands, thereby halting the system.</p>
<p>故障问题是一个困难的问题，详细讨论它超出了本文的范围。我们注意到，故障的概念只有在物理时间的背景下才有意义。没有物理时间，就无法区分一个进程是故障了，还是只是在事件间暂停了。用户可以判断系统“崩溃”，仅仅是因为他等待响应的时间太长。论文《The implementation of reliable distributed multiprocess systems》中描述了一种即使单个进程或通信线路出现故障也能正常工作的方法。</p>
<p>The problem of failure is a difficult one, and it is beyond the scope of this paper to discuss it in any detail. We will just observe that the entire concept of failure is only meaningful in the context of physical time. Without physical time, there is no way to distinguish a failed process from one which is just pausing between events. A user can tell that a system has “crashed” only because he has been waiting too long for a response. A method which works despite the failure of individual processes or communication lines is described in [3].</p>
<hr>
<h3 id="Anomalous-Behavior-反常行为"><a href="#Anomalous-Behavior-反常行为" class="headerlink" title="Anomalous Behavior     反常行为"></a>Anomalous Behavior     反常行为</h3><p>我们的资源调度算法根据全序关系<code>==&gt;</code>，对请求进行排序。这允许出现下面这种“异常行为”：考虑一个全国范围的互联计算机系统。假设一个人在计算机A上发出请求<code>a</code>，然后打电话给另一个城市的朋友，让他在另一台计算机B上发出请求<code>b</code>。请求<code>b</code>很可能附带较小的时间戳，并排在在请求<code>a</code>之前。这可能发生，因为系统无法知道<code>a</code>实际上在<code>b</code>之前，因为<code>a</code>和<code>b</code>谁先谁后这个信息是基于系统外部的电话消息。</p>
<p>Our resource scheduling algorithm ordered the requests according to the total ordering &#x3D;&#x3D;&gt;. This permits the following type of “anomalous behavior.” Consider a nationwide system of interconnected computers. Suppose a person issues a request A on a computer A, and then telephones a friend in another city to have him issue a request B on a different computer B. It is quite possible for request B to receive a lower timestamp and be ordered before request A. This can happen because the system has no way of knowing that A actually preceded B, since that precedence information is based on messages external to the system.</p>
<p>让我们更仔细地检查问题的根源。设 F 为所有系统事件的集合。让我们引入事件集 <u>F</u> ，其中包含 F 中的事件以及所有其他相关的外部事件，比如上面示例中的电话呼叫。用 <strong>–&gt;</strong> 表示 <u>F</u> 中的“happened before”关系。在我们的示例中，我们有 <code>a</code><strong>–&gt;</strong><code>b</code>，但<code>a-/-&gt;b</code>。很明显，没有任何算法可以完全基于 F 中的事件，且不以任何方式将这些事件与 <u>F</u> 中的其他事件关联起来，来保证请求<code>a</code>排在请求<code>b</code>之前。</p>
<p>Let us examine the source of the problem more closely. Let F be the set of all system events. Let us introduce a set of events which contains the events in F together with all other relevant external events, such as the phone calls in our example. Let <strong>–&gt;</strong> denote the “happened before” relation for <u>F</u>. In our example, we had A <strong>–&gt;</strong> B, but A -&#x2F;-&gt; B. It is obvious that no algorithm based entirely upon events in F, and which does not relate those events in any way with the other events in <u>F</u>, can guarantee that request A is ordered before request B.</p>
<p>有两种可能的方法可以避免这种异常行为。第一种方法是在系统中明确引入有关 <strong>–&gt;</strong> 关系的必要信息。在我们的示例中，发出请求<code>a</code>的人可以从系统中得到接收该请求的时间戳<code>Ta</code>。当发出请求<code>b</code>时，他的朋友可以指定<code>b</code>的时间戳晚于<code>Ta</code>。这就是让用户来负责避免异常行为。</p>
<p>There are two possible ways to avoid such anomalous behavior. The first way is to explicitly introduce into the system the necessary information about the ordering <strong>–&gt;</strong>. In our example, the person issuing request A could receive the timestamp Ta of that request from the system. When issuing request B, his friend could specify that B be given a timestamp later than Ta. This gives the user the responsibility for avoiding anomalous behavior.</p>
<p>第二种方法是构造一个满足以下条件的时钟系统。<font color=red>Strong Clock Condition</font>：对于 F 中的任何事件<code>a、b</code>：如果<code>a</code><strong>–&gt;</strong><code>b</code>，则<code>C(a) &lt; C(b)</code>。</p>
<p>The second approach is to construct a system of clocks which satisfies the following condition.</p>
<p>Strong Clock Condition. For any events a, b in F: if a <strong>–&gt;</strong> b then C(a} &lt; C(b).</p>
<p>这比Clock Condition强，因为 <strong>–&gt;</strong> 的关系比–&gt;强。一般来说，逻辑时钟无法满足该条件。</p>
<p>This is stronger than the ordinary Clock Condition because <strong>–&gt;</strong> is a stronger relation than –&gt;. It is not in general satisfied by our logical clocks.</p>
<p>让我们用物理时空中的一组“真实”事件来标识 <u>F</u> ，并让 <strong>–&gt;</strong> 是狭义相对论定义的事件的偏序。宇宙的奥秘之一，是有可能构建一个物理时钟系统，该系统彼此独立运行，并能满足Strong Clock Condition。因此，我们可以使用物理时钟来消除异常行为。我们现在把注意力转向物理时钟。</p>
<p>Let us identify <u>F</u> with some set of “real” events in physical space-time, and let <strong>–&gt;</strong> be the partial ordering of events defined by special relativity. One of the mysteries of the universe is that it is possible to construct a system of physical clocks which, running quite independently of one another, will satisfy the Strong Clock Condition. We can therefore use physical clocks to eliminate anomalous behavior. We now turn our attention to such clocks.</p>
<hr>
<h3 id="Physical-Clocks"><a href="#Physical-Clocks" class="headerlink" title="Physical Clocks"></a>Physical Clocks</h3><p>现在在时空图中引入一个物理时间坐标，用<code>Ci(t)</code>表示在物理时间<code>t</code>时对时钟<code>Ci</code>的读数。出于数学上的方便, 我们假设时钟是连续运转的，而不是离散的”ticks”。更准确地说，我们假定除了时钟重置时的孤立的跳跃不连续点外, <code>Ci(t)</code>是一个连续的、对<code>t</code>可微的函数。从而 <code>dCi(t)/dt</code> 表示时钟在<code>t</code>时刻的运行速率.</p>
<p>Let us introduce a physical time coordinate into our space-time picture, and let Ci(t) denote the reading of the clock Ci at physical time t. For mathematical convenience, we assume that the clocks run continuously rather than in discrete “ticks.” (A discrete clock can be thought of as a continuous one in which there is an error of up to ½ “tick” in reading it.) More precisely, we assume that Ci(t) is a continuous, differentiable function of t except for isolated jump discontinuities where the clock is reset. Then dCi(t)&#x2F;dt represents the rate at which the clock is running at time t.</p>
<p>为了使时钟<code>Ci(t)</code>成为一个真正的物理时钟，它必须以大概正确的速度运行。即，对于所有<code>t</code>，必须有<code>dCi(t)/dt ≈ 1</code>。更准确地说，我们假设满足以下条件:</p>
<p><strong>PC1</strong>. 存在一个常量<code>x &lt;&lt; 1</code>, 对于所有的 <code>i</code>满足: <code>| dCi(t)/dt - 1 | &lt; x</code>. 在典型的晶体控制的时钟中, <code>x &lt;=</code> 10<sup>-6</sup>.</p>
<p>In order for the clock Ci to be a true physical clock, it must run at approximately the correct rate. That is, we must have dCi(t)&#x2F;dt ≈ 1 for all t. More precisely, we will assume that the following condition is satisfied:</p>
<p>PCI. There exists a constant x &lt;&lt; 1, such that for all i: | dCi(t)&#x2F;dt - 1 | &lt; x. For typical crystal controlled clocks, x &lt;&#x3D; 10<sup>-6</sup>.</p>
<p>仅使各时钟以大约正确的速度运行是不够的。它们还必须尽可能的同步，使得对于所有的<code>i,j</code>和<code>t</code>, <code>Ci(t) ≈ Cj(t)</code>。更准确地说，必须有一个足够小的常数<code>e</code> ，使下列条件成立:</p>
<p><strong>PC2</strong>.对于所有<code>i, j</code> : <code>| Ci(t) - Cj(t) | &lt; e</code></p>
<p>It is not enough for the clocks individually to run at approximately the correct rate. They must be synchronized so that Ci(t) ≈ Cj(t) for all i,j, and t. More precisely, there must be a sufficiently small constant e so that the following condition holds:</p>
<p>PC2. For all i, j: | Ci(t) - Cj(t) | &lt; e.</p>
<p>如果我们考虑图2中的垂直距离来表示物理时间，那么PC2表示单个刻度线的高度变化小于e。</p>
<p>If we consider vertical distance in Figure 2 to represent physical time, then PC2 states that the variation in height of a single tick line is less than e.</p>
<p>由于两个不同的时钟永远不会以完全相同的速度运行，它们会越走越远。因此，我们必须设计一种算法来确保PC2始终有效。然而, 首先，让我们检查<code>x</code>和<code>e</code>必须要多小才能阻止异常行为。我们必须确保相关物理事件的系统 <u>F</u> 满足Strong Clock Condition。我们假设我们的时钟满足普通Clock Condition，因此我们只需要保证对于 <u>F</u> 中满足 <code>a</code><strong>–&gt;</strong><code>b</code> 的事件<code>a, b</code>, Strong Clock Condition成立. 因此，我们只需要考虑发生在不同进程中的事件。</p>
<p>Since two different clocks will never run at exactly the same rate, they will tend to drift further and further apart. We must therefore devise an algorithm to insure that PC2 always holds. First, however, let us examine how small x and e must be to prevent anomalous behavior. We must insure that the system <u>F</u> of relevant physical events satisfies the Strong Clock Condition. We assume that our clocks satisfy the ordinary Clock Condition, so we need only require that the Strong Clock Condition holds when a and b are events in F with a <strong>-&#x2F;-&gt;</strong> b. Hence, we need only consider events occurring in different processes.</p>
<p>假设<code>u</code>是一个数字，如果事件<code>a</code>发生在物理时间<code>t</code>，而另一个进程中的事件<code>b</code>满足 a**–&gt;**b，则<code>b</code>发生在物理时间<code>t+u</code>之后。换句话说，<code>u</code>小于进程间消息的最短传输时间。我们总是可以选择<code>u</code>等于过程之间的最短距离除以光速。但是，根据 <u>F</u> 中的消息的传输方式，<code>u</code>可能要大得多。</p>
<p>Let u be a number such that if event a occurs at physical time t and event b in another process satisfies a ~ b, then b occurs later than physical time t + u. In other words, u is less than the shortest transmission time for interprocess messages. We can always choose # equal to the shortest distance between processes divided by the speed of light. However, depending upon how messages in <u>F</u> are transmitted, u could be significantly larger.</p>
<p>为了避免异常行为, 我们必须保证对任意<code>i, j, t</code>: <code>Ci(t + u) - Cj(t) &gt; 0</code>. 将这个条件与PC1和PC2结合起来，我们可以将<code>x</code>和<code>e</code>的最小值与<code>u</code>的值联系起来，如下所示。我们假定当时钟被重置时，它总是向前拨，而不会后退。(允许往后拨会导致违反C1). PC1暗示<code>Ci(t + u) - Ci(t) &gt; (1 - x)u</code>. 使用PC2, 很容易推导出当<code>e/(1-x) &lt;= u</code>时 <code>Ci(t + u) - Cj(t) &gt; 0</code>。</p>
<p>To avoid anomalous behavior, we must make sure that for any i, j, and t: Ci(t + u) - Cj(t) &gt; 0. Combining this with PCI and 2 allows us to relate the required smallness of x and e to the value of # as follows. We assume that when a clock is reset, it is always set forward and never back. (Setting it back could cause C1 to be violated.) PCI then implies that Ci(t + u) - Ci(t) &gt; (1 - x)u. Using PC2, it is then easy to deduce that Ci(t + u) - Cj(t) &gt; 0 if the following inequality holds: e&#x2F;(1-x) &lt;&#x3D; u.</p>
<p>这个不等式以及PC1和PC2表明反常行为是不可能的。</p>
<p>This inequality together with PC 1 and PC2 implies that anomalous behavior is impossible.</p>
<p>现在我们描述确保PC2成立的算法。设<code>m</code>是在物理时间t发送并在时间<code>t&#39;</code>接收的消息. 我们定义<code>vm=t&#39;-t</code>是消息<code>m</code>的total delay (总延迟). 当然，这个延迟接收<code>m</code>的进程所不知道的。然而, 我们假设接收进程知道一些minimum delay (最小延迟)  <code>um &gt;= 0</code> 使得 <code>um &lt;= vm</code> . 我们称 <code>zm = vm-um</code> 是消息的unpredictable delay (不可预测延迟)</p>
<p>我们现在指定针对物理时钟的IRI和IR2规则如下:</p>
<p><strong>IR1’</strong>. 对于任意<code>i</code>, 如果<code>Pi</code>没有在物理时间<code>t</code>收到消息, 那么<code>Ci</code>是在<code>t</code>点可微, 且<code>dCi(t)/dt &gt; 0</code></p>
<p><strong>IR2’</strong>. (a) 如果<code>Pi</code>在物理时间<code>t</code>发送消息<code>m</code>, 那么<code>m</code>包含时间戳<code>Tm= Ci(t)</code>. (b) 当在时间<code>t&#39;</code>收到消息<code>m</code>, 进程<code>Pj</code>设置<code>Cj(t&#39;)</code>为 <code>max(Cj(t&#39; - 0), Tm + um)</code>. (其中<img src="https://www.zhihu.com/equation?tex=C_j(t%27-0)=lim_%7B%5Cdelta%5Crightarrow+0%7DC_j(t%27-%7C%5Cdelta%7C)" alt="[公式]"> )</p>
<p>We now describe our algorithm for insuring that PC2 holds. Let m be a message which is sent at physical time t and received at time t’. We define vm &#x3D; t’ - t to be the total delay of the message m. This delay will, of course, not be known to the process which receives m. However, we assume that the receiving process knows some minimum delay um &gt;&#x3D; 0 such that um &lt;&#x3D; vm. We call zm &#x3D; vm-um the unpredictable delay of the message.</p>
<p>We now specialize rules IRI and 2 for our physical clocks as follows:</p>
<p>IR 1’. For each i, if Pi does not receive a message at physical time t, then Ci is differentiable at t and dCi(t)&#x2F;dt &gt; 0.</p>
<p>IR2’. (a) If Pi sends a message m at physical time t, then m contains a timestamp Tm&#x3D; Ci(t). (b) Upon receiving a message m at time t’, process Pj sets Cj(t’) equal to maximum (Cj(t’ - 0), Tm + um). </p>
<p>虽然规则是用物理时间参数正式指定的，但进程只需要知道自己的时钟读数和它接收到的消息的时间戳。为了数学上的方便，我们假设每个事件都发生在物理时间的精确瞬间，而同一进程中的不同事件发生在不同的时间。这些规则是规则IR1和规则IR2的特殊化，因此我们的时钟系统满足Clock Condition。真实事件的持续时间是有限的，因此实现该算法方面没有任何困难。在实现中唯一真正需要关注的是确保离散时钟滴答声足够频繁，以便C1得到维护。</p>
<p>Although the rules are formally specified in terms of the physical time parameter, a process only needs to know its own clock reading and the timestamps of messages it receives. For mathematical convenience, we are assuming that each event occurs at a precise instant of physical time, and different events in the same process occur at different times. These rules are then specializations of rules IR1 and IR2, so our system of clocks satisfies the Clock Condition. The fact that real events have a finite duration causes no difficulty in implementing the algorithm. The only real concern in the implementation is making sure that the discrete clock ticks are frequent enough so C 1 is maintained.</p>
<p>我们现在证明这个时钟同步算法可以满足条件PC2。我们假设进程系统是用一个有向图来描述的，其中从进程<code>Pi</code>到进程<code>Pj</code>的弧表示一条通信线，消息从<code>Pi</code>直接发送到<code>Pj</code>。我们说每<code>π</code>秒在这个弧上发送一条消息, 当对于任意<code>t</code>, <code>Pi</code>在物理时间<code>t</code>和<code>t+π</code>之间至少发送一条消息给<code>Pj</code>。有向图的直径diameter是一个最小值<code>d</code>, 使得任意一对不同的过程<code>Pj</code>，<code>Pk</code>，从<code>Pj</code>到<code>Pk</code> 有一条路径，该路径最多有<code>d</code>条弧。</p>
<p>We now show that this clock synchronizing algorithm can be used to satisfy condition PC2. We assume that the system of processes is described by a directed graph in which an arc from process Pi to process Pj represents a communication line over which messages are sent directly from Pi to Pj. We say that a message is sent over this arc every T seconds if for any t, Pi sends at least one message to Pj between physical times t and t + -r. The diameter of the directed graph is the smallest number d such that for any pair of distinct processes Pj, Pk, there is a path from Pj to Pk having at most d arcs.</p>
<p>除了建立PC2之外，下面的定理限制了系统第一次启动时时钟可以同步的时间长度。</p>
<p>In addition to establishing PC2, the following theorem bounds the length of time it can take the clocks to become synchronized when the system is first started.</p>
<p><strong>THEOREM</strong>. 假设一个直径为<code>d</code>的进程的强连通图，它始终保证规则<code>IR1&#39;</code>和<code>IR2&#39;</code>的成立。假设对任意消息<code>m</code>, 存在某个参数<code>u</code>使得<code>um &lt;= u</code> , 并且让所有<code>t &gt;= t0</code> : (a) PC1成立 (b) 存在参数<code>π, z</code> , 使得每<code>π</code>秒, 一个unpredictable delay小于<code>z</code>的消息在每个弧中发送. 从而PC2满足条件, 且对于任意<code>t&gt;=t0+πd</code>，有<code>e</code>约等于<code>d(2kπ + z)</code>， 其中约等于条件假定了<code>u+z &lt;&lt; π</code>.</p>
<p>THEOREM. Assume a strongly connected graph of processes with diameter d which always obeys rules IR1’ and IR2’. Assume that for any message m, um &lt;&#x3D; u for some constant u, and that for all t &gt; t0: (a) PC 1 holds. (b) There are constants π and z such that every π seconds a message with an unpredictable delay less than z is sent over every arc. Then PC2 is satisfied with e ≈d(2kπ + z) for all t&gt;&#x3D;t0+πd, where the approximations assume u+z &lt;&lt; π.</p>
<p>这个定理的证明是非常困难的，在附录中给出。在同步物理时钟的问题上已经做了大量的工作。我们请读者参考[4]来了解这个主题。文献中描述的方法对于估计消息延迟<code>um</code>和调整时钟频率<code>dCi/dt</code> (对于允许这样调整的时钟)是有用的。然而，时钟永远不能倒拨的要求似乎将我们的情况与之前研究过的情况区分开来，我们相信这个定理是一个新的结果。</p>
<p>The proof of this theorem is surprisingly difficult, and is given in the Appendix. There has been a great deal of work done on the problem of synchronizing physical clocks. We refer the reader to [4] for an intro-duction to the subject. The methods described in the literature are useful for estimating the message delays ktm and for adjusting the clock frequencies dCi&#x2F;dt (for clocks which permit such an adjustment). However, the requirement that clocks are never set backwards seems to distinguish our situation from ones previously studied, and we believe this theorem to be a new result.</p>
<hr>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>我们已经看到，“happening before”的概念定义了分布式多进程系统中事件的偏序关系。我们描述了一种将偏序扩展为全序的算法，并展示了如何使用该全序关系来解决一个简单的同步问题。未来的一篇论文将展示如何扩展此方法以解决任何同步问题。</p>
<p>We have seen that the concept of “happening before” defines an invariant partial ordering of the events in a distributed multiprocess system. We described an algorithm for extending that partial ordering to a somewhat arbitrary total ordering, and showed how this total ordering can be used to solve a simple synchronization problem. A future paper will show how this approach can be extended to solve any synchronization problem.</p>
<p>该算法定义的全序关系是任意的。如果它与系统用户感知到的顺序不一致，可能会产生异常行为。该问题可以通过使用正确同步的物理时钟来防止。我们的定理表明了时钟可以同步到多近。</p>
<p>The total ordering defined by the algorithm is somewhat arbitrary. It can produce anomalous behavior if it disagrees with the ordering perceived by the system’s users. This can be prevented by the use of properly synchronized physical clocks. Our theorem showed how closely the clocks can be synchronized.</p>
<p>在分布式系统中，重要的是要认识到事件发生的顺序只是偏序。我们相信这个想法对于理解任何多进程系统都是有用的。它应该有助于人们在不依赖于解决这些问题的机制的情况下，理解多进程的根本问题。</p>
<p>In a distributed system, it is important to realize that the order in which events occur is only a partial ordering. We believe that this idea is useful in understanding any multiprocess system. It should help one to understand the basic problems of multiprocessing independently of the mechanisms used to solve them.</p>
<hr>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/484914733">https://zhuanlan.zhihu.com/p/484914733</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/484914733#ref_1">https://zhuanlan.zhihu.com/p/484914733#ref_1</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/87944336">https://zhuanlan.zhihu.com/p/87944336</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/34057588">https://zhuanlan.zhihu.com/p/34057588</a></p>
</li>
<li><p><a href="https://lamport.azurewebsites.net/pubs/pubs.html#time-clocks">https://lamport.azurewebsites.net/pubs/pubs.html#time-clocks</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>The many faces of consistency</title>
    <url>/2022/05/28/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/%E8%AE%BA%E6%96%87/02The%20many%20faces%20of%20consistency#####/</url>
    <content><![CDATA[<p><font color=red>一致性的概念存在</font>于计算机科学中的各种学科中，比如<font color=red>分布式系统、数据库系统，计算机体系结构</font>等。然而在这些学科中，一致性可能表示的是完全不同的事情。本文中我们识别出一致性的两大类型：状态一致性和操作一致性，它们在含义和应用范围上有根本的不同。本文中我们会通过每个学科中的多个示例来解释这两种一致性类型。</p>
<span id="more"></span>
<p>The notion of consistency is used across different computer science disciplines from distributed systems to database systems to computer architecture. It turns out that consistency can mean quite different things across these disciplines, depending on who uses it and in what context it appears. We identify two broad types of consistency, state consistency and operation consistency, which differ fundamentally in meaning and scope. We explain how these types map to the many examples of consistency in each discipline.</p>
<h3 id="1-Introduction-引言"><a href="#1-Introduction-引言" class="headerlink" title="1 Introduction     引言"></a>1 Introduction     引言</h3><p>在涉及<font color=red>共享和多数据副本</font>的计算机系统中，一致性是个重要的考虑因素。早期的计算系统只有私有数据，而随着计算机从计算机器发展到信息交换工具，共享数据变得越来越普遍。从分布式系统到数据库系统再到多处理器系统，共享数据出现在许多类型的系统中。比如在分布式系统中，跨网络的文件共享、DNS条目、Key-value系统中的数据块、系统元数据等都需要共享数据；在数据库系统中，用户需要共享包含账户信息、产品描述、航班预订和座位分配的表；在计算机系统内部，多CPU核心需要共享cache line和物理内存。</p>
<p>Consistency is an important consideration in computer systems that share and replicate data. Whereas early computing systems had private data exclusively, shared data has become increasingly common as computers have evolved from calculating machines to tools of information exchange. Shared data occurs in many types of systems, from distributed systems to database systems to multiprocessor systems. For example, in distributed systems, users across the network share files (e.g., source code), network names (e.g., DNS entries), data blobs (e.g., images in a key-value store), or system metadata (e.g., configuration information). In database systems, users share tables containing account information, product descriptions, flight bookings, and seat assignments. Within a computer, processor cores share cache lines and physical memory.</p>
<p>除了共享，计算机系统越来越多地需要在组件间复制数据。分布式系统中，每个节点可能都持有共享文件、DNS条目、数据块或系统元数据的本地副本。这些副本称为缓存，可以提高系统的性能；数据库系统中，会复制行或表以提高速度或进行容错；在计算机系统内，内存的部分内容在缓存层次结构（L1、L2、L3缓存）中的不同点进行复制，这也是为了提高速度。我们使用“副本”一词广义的表示系统维护的数据的任何副本。</p>
<p>In addition to sharing, computer systems increasingly replicate data within and across components. In distributed systems, each site may hold a local replica of files, network names, blobs, or system metadata—these replicas, called caches, increase performance of the system. Database systems also replicate rows or tables for speed or to tolerate disasters. Within a computer, parts of memory are replicated at various points in the cache hierarchy (L1、L2、L3 caches), again for speed. We use the term replica broadly to mean any copies of the data maintained by the system.</p>
<p>在所有这些系统中，数据共享和复制都涉及到一个<font color=red>基本问题：如果一个Client修改了一些数据项，同一时间，另一个Client在不同的副本上，读取或者修改了相同的数据项，会发生什么？</font></p>
<p>In all these systems, data sharing and replication raise a fundamental question: what should happen if a client modifies some data items and simultaneously, or within a short time, another client reads or modifies the same items, possibly at a different replica?</p>
<p>这个问题没有适用所有场景的标准答案。一致性属性通过限制数据的变化方式或Client在每种情况下可以观察到的内容来控制可能的结果。比如，在DNS系统中，对域名的修改可能数小时内都是不可见的，而唯一能保证的是，最终总会看到更新—-这是最终一致性的例子。但是对于航班订座系统来说，更新必须是即时的，互斥的，这样才能确保两个乘客不会订到相同的座位—-这是序列化保证的强一致性的例子。还有其他一些一致性属性，包括因果一致性（causal consistency）、读我所写（read-my-writes）、有界延迟一致性（bounded staleness）、持续一致性（continuous consistency）、释放一致性（release consistency）、fork一致性（fork consistency）、epsilon序列化性（epsilon serializability）等等。</p>
<p>This question does not have a single answer that is right in every context. A consistency property governs the possible outcomes by limiting how data can change or what clients can observe in each case. For example, with DNS, a change to a domain may not be visible for hours; the only guarantee is that updates will be seen eventually—an example of a property called eventual consistency [23]. But with flight seat assignments, updates must be immediate and mutually exclusive, to ensure that no two passengers receive the same seat—an example of a strong type of consistency provided by serializability [5]. Other consistency properties include causal consistency [13], read-my-writes [21], bounded staleness [1], continuous consistency [1, 25], release consistency [10], fork consistency [16], epsilon serializability [18], and more.</p>
<p>一致性很重要，因为开发者必须能理解上面那个基本问题的答案。特别是当与系统交互的Client不是人类，而是其他计算机程序时，开发者就必须能编码应对所有可能的状况。</p>
<p>Consistency is important because developers must understand the answer to the above fundamental question. This is especially true when the clients interacting with the system are not humans but other computer programs that must be coded to deal with all possible outcomes.</p>
<p>本文我们研究了分布式系统、数据库系统和计算机体系结构这三种计算机学科如何使用一致性的诸多例子。我们发现这些学科对于一致性的使用存在很大差异。为了更清晰的解释一致性，我们识别出了两种根本不同的一致性类型：<font color=red>状态一致性和操作一致性。状态一致性关心的是系统状态，建立了不同数据项或是相同数据项的不同副本之间的正确关系的约束。</font>比如，状态一致性可能要求两个副本在更新完成时保存的是相同的值；<font color=red>操作一致性关注的是针对系统的操作，并对它们可能返回的结果建立约束</font>。比如，操作一致性可能要求对文件的读操作能反映出该文件上最近一次写入的内容。状态一致性更容易一些，并且依赖于应用程序，而操作一致性更复杂且不依赖于应用程序。这两种类型的一致性都很重要，在我们看来，社区应该更清楚地将它们区分开来。</p>
<p>In this article, we examine many examples of how consistency is used across three computer science disciplines: distributed systems, database systems, and computer architecture. We find that the use of consistency varies significantly across these disciplines. To bring some clarity, we identify two fundamentally different types of consistency: state consistency and operation consistency. State consistency concerns the state of the system and establishes constraints on the allowable relationships between different data items or different replicas of the same items. For instance, state consistency might require that two replicas store the same value when updates are not outstanding. Operation consistency concerns operations on the system and establishes constraints on what results they may return. For instance, operation consistency might require that a read of a file reflects the contents of the most recent write on that file. State consistency tends to be simpler and application dependent, while operation consistency tends to be more complex and application agnostic. Both types of consistency are important and, in our opinion, our communities should more clearly disentangle them.</p>
<p>虽然本文讨论了不同形式的一致性，但它更关注的是一致性的语义，而非一致性的实现机制。语义指的是系统能提供什么样的一致性属性，而机制指的是系统如何实现来保证这些一致性属性。语义和机制密切相关，但相比较而言，理解语义更重要一些。</p>
<p>While this article discusses different forms of consistency, it focuses on the semantics of consistency rather than the mechanisms of consistency. Semantics refer to what consistency properties the system provides, while mechanisms refer to how the system enforces those properties. Semantics and mechanisms are closely related but it is important to understand the former without needing to understand the latter.</p>
<p>本文的其余部分组织如下。第2章解释本文中使用的抽象系统模型和术语；第3章中介绍这两种一致性类型及其各种实例；第4章中说明了不同的学科如何使用这些一致性类型。</p>
<p>The rest of this article is organized as follows. We first explain the abstract system model and terminology used throughout the article in Section 2. We present the two types of consistency and their various embodiments in Section 3. We indicate how these consistency types occur across different disciplines in Section 4.</p>
<hr>
<h3 id="2-Abstract-model-抽象模型"><a href="#2-Abstract-model-抽象模型" class="headerlink" title="2 Abstract model     抽象模型"></a>2 Abstract model     抽象模型</h3><p>考虑一个系统，其具有多个Client，并且这些Client会向系统提交要执行的操作。Client可能是人类用户、计算机程序或其他与本系统无关的系统。而操作可能包括简单的读、写、读修改写、启动和提交事务、range queries。操作通常作用于数据项，这些数据项可以是数据块、文件、键值对、DNS条目、表中的行、内存等。</p>
<p>We consider a setting with multiple clients that submit operations to be executed by the system. Clients could be human users, computer programs, or other systems that do not concern us. Operations might include simple read and write, read-modify-write, start and commit a transaction, and range queries. Operations typically act on data items, which could be blocks, files, key-value pairs, DNS entries, rows of tables, memory locations, and so on.</p>
<p>系统有一个状态，其中包含了数据项的当前值。某些情况下，我们感兴趣的是Client的缓存，以及其他副本的一致性。这种情况下，缓存和其他副本被视为系统的一部分，而系统状态包含其内容。</p>
<p>The system has a state, which includes the current values of the data items. In some cases, we are interested in the consistency of client caches and other replicas. In these cases, the caches and other replicas are considered to be part of the system and the system state includes their contents.</p>
<p>操作的执行不是瞬间的，它是有起始时间的，从Client提交操作算开始，到Client从系统获得响应时结束。如果操作没有响应，则当系统不再主动处理时，算是操作完成。</p>
<p>An operation execution is not instantaneous; rather, it starts when a client submits the operation, and it finishes when the client obtains its response from the system. If the operation execution returns no response, then it finishes when the system is no longer actively processing it.</p>
<p>操作不同于操作的执行。操作是静态的，系统中的操作相对较少，例如读写操作。另一方面，操作执行是动态的，数量众多。Client可以多次执行相同的操作，但每次的操作执行都是独一无二的。虽然从技术上讲，我们应该将操作与操作执行分开，但当上下文比较清晰时，我们往往会模糊这一区别（例如，我们可能会说读操作完成了，而不是读操作的执行完成了）。</p>
<p>Operations are distinct from operation executions. Operations are static and a system has relatively few of them, such as read and write. Operation executions, on the other hand, are dynamic and numerous. A client can execute the same operation many times, but each operation execution is unique. While technically we should separate operations from operation executions, we often blur the distinction when it is clear from the context (e.g., we might say that the read operation finishes, rather than the execution of the read operation finishes).</p>
<hr>
<h3 id="3-Two-types-of-consistency-两种类型的一致性"><a href="#3-Two-types-of-consistency-两种类型的一致性" class="headerlink" title="3 Two types of consistency     两种类型的一致性"></a>3 Two types of consistency     两种类型的一致性</h3><p>这里我们关心的是，当多个Client同时或者几乎同时并发访问共享和数据副本时会发生什么？一般而言，一致性就是根据应用程序的需求约束操作的允许的结果。现在我们定义两种一致性类型。一种是对状态施加约束，另一种是对操作结果施加约束。</p>
<p>We are interested in what happens when shared and replicated data is accessed concurrently or nearly concurrently by many clients. Generally speaking, consistency places constraints on the allowable outcomes of operations, according to the needs of the application. We now define two broad types of consistency. One places constraints on the state, the other on the results of operations.</p>
<h4 id="3-1-State-consistency-状态一致性"><a href="#3-1-State-consistency-状态一致性" class="headerlink" title="3.1 State consistency     状态一致性"></a>3.1 State consistency     状态一致性</h4><p><font color=red>状态一致性与系统状态有关，它由用户期望的状态（即使存在并发访问和多份数据副本的情况下）满足的属性组成。</font>当数据可能因错误（程序崩溃、位翻转、bug等）被破坏时，状态一致性也适用，但这不是本文的重点。</p>
<p>State consistency pertains to the state of the system; it consists of properties that users expect the state to satisfy despite concurrent access and the existence of multiple replicas. State consistency is also applicable when data can be corrupted by errors (crashes, bit flips, bugs, etc), but this is not the focus of this article. </p>
<p>根据状态属性的表达方式，状态一致性可以分为许多子类型。接下来我们将解释这些子类型。</p>
<p>State consistency can be of many subcategories, based on how the properties of state are expressed. We explain these subcategories next.</p>
<h5 id="3-1-1-Invariants-不变式"><a href="#3-1-1-Invariants-不变式" class="headerlink" title="3.1.1 Invariants     不变式"></a>3.1.1 Invariants     不变式</h5><p>状态一致性最简单的子类型是由不变式定义的，<font color=red>所谓不变式，就是针对状态的一个必须为true的断言。</font>比如，在一个并发程序中，单链表不能包含环；在多核处理器系统中，如果两个CPU核的本地缓存保存的是针对相同内存地址的值，则他们的值必须是相同的；在社交网络中，用户x是用户y的朋友，那y也是x的朋友；在一个照片共享应用中，如果相册中包含了某个图像，那该图像的所有者就是相册。</p>
<p>The simplest subcategory of state consistency is one defined by an invariant—a predicate on the state that must evaluate to true. For instance, in a concurrent program, a singly linked list must not contain cycles. In a multiprocessor system, if the local caches of two processors keep a value for some address, it must be the same value. In a social network, if user x is a friend of user y then y is a friend of x. In a photo sharing application, if a photo album includes an image then the image’s owner is the album.</p>
<p>数据库系统中的两个重要的例子是，<font color=red>唯一性约束和引用完整性</font>。表中某一列上的唯一性约束要求该列中每个值只能出现一次。该属性对于表的主键而言至关重要。</p>
<p>In database systems, two important examples are uniqueness constraints and referential integrity. A uniqueness constraint on a column of a table requires that each value appearing in that column must occur in at most one row. This property is crucial for the primary keys of tables.</p>
<p>引用完整性涉及引用另一个表的键。数据库中经常需要通过在表A的列中包含表B中的键来表示两个表的关系。引用完整性要求表A中保存的键确实是表B中的键。比如，在银行的数据库中，假设有一个账户表，该表中包含一个表示账户所有者的列，即user id，同时，user id是用户表的主键，用户表中记录了用户的详细信息。引用完整性要求账户表中的id必须存在于用户表中。</p>
<p>Referential integrity concerns a table that refers to keys of another table. Databases may store relations between tables by including keys of a table within columns in another table. Referential integrity requires that the included keys are indeed keys in the first table. For instance, in a bank database, suppose that an accounts table includes a column for the account owner, which is a user id; meanwhile, the user id is the primary key in a users table, which has detailed information for each user. A referential integrity constraint requires that user ids in the accounts table must indeed exist in the users table.</p>
<p>基于不变式的状态一致性的另一个例子就是<font color=red>相互一致性</font>，用于使用像主备这样的技术来进行复制的分布式系统中。相互一致性要求没有未完成的更新时，副本要具有相同的状态。在更新期间，副本可能会暂时不一致，因为更新不可能同时应用到所有副本上。</p>
<p>Another example of state consistency based on invariants is mutual consistency, used in distributed systems that are replicated using techniques such as primary-backup [2]. Mutual consistency requires that replicas have the same state when there are no outstanding updates. During updates, replicas may diverge temporarily since the updates are not applied simultaneously at all replicas.</p>
<h5 id="3-1-2-Error-bounds-有界不一致"><a href="#3-1-2-Error-bounds-有界不一致" class="headerlink" title="3.1.2 Error bounds     有界不一致"></a>3.1.2 Error bounds     有界不一致</h5><p>如果状态是数字类型的数据，则一致性属性可以表示为与预期值的最大偏差或误差。比如，两个副本上的值最多可能相差ϵ。在物联网系统中，传感器（如温度计）的报告值与被测实际值的误差在ϵ范围内。此示例将系统状态与世界状态相关联。错误界限最初是在数据库社区中提出的，其基本思想后来又在分布式系统社区中复兴。</p>
<p>If the state contains numerical data, the consistency property could indicate a maximum deviation or error from the expected. For instance, the values at two replicas may diverge by at most ϵ. In an internet-of-things system, the reported value of a sensor, such as a thermometer, must be within ϵ from the actual value being measured. This example relates the state of the system to the state of the world. Error bounds were first proposed within the database community [1] and the basic idea was later revived in the distributed systems community [25].</p>
<h5 id="3-1-3-Limits-on-proportion-of-violations-不一致的比例限制"><a href="#3-1-3-Limits-on-proportion-of-violations-不一致的比例限制" class="headerlink" title="3.1.3 Limits on proportion of violations     不一致的比例限制"></a>3.1.3 Limits on proportion of violations     不一致的比例限制</h5><p>如果系统有很多属性或不变式，那期望所有属性或不变式都成立是不现实的，可以预期有一个很高的比例的属性或不变式是成立的。比如，系统可能需要在百万个用户中仅有一个用户的不变式被破坏。如果系统可以补偿一小部分用户数据的不一致，那这样做是有意义的。</p>
<p>If there are many properties or invariants, it may be unrealistic to expect all of them to hold, but rather just a high percentage. For instance, the system may require that at most one user’s invariants are violated in a pool of a million users; this could make sense if the system can compensate a small fraction of users for inconsistencies in their data.</p>
<h5 id="3-1-4-Importance-一致性等级"><a href="#3-1-4-Importance-一致性等级" class="headerlink" title="3.1.4 Importance     一致性等级"></a>3.1.4 Importance     一致性等级</h5><p>属性或不变式的重要性可以分等级，比如critical, important, advisable, desirable, 或 optional，用户可能希望所有时刻只要critical的属性能够维持就行了。开发者可以采用更昂贵，更高效的机制来保证更重要的不变式。比如，当用户在网站上修改密码时，系统需要确认所有副本上的用户账户中的密码都是一样的之后，再回复用户密码修改成功。这种属性是通过确认所有副本并等待答复来实现的，对于不太重要的属性来说，这是一种过于昂贵的机制。</p>
<p>Properties or invariants may be critical, important, advisable, desirable, or optional, where users expect only the critical properties to hold at all times. Developers can use more expensive and effective mechanisms for the more important invariants. For instance, when a user changes her password at a web site, the system might require all replicas of the user account to have the same password before acknowledging the change to the user. This property is implemented by contacting all replicas and waiting for replies, which can be an overly expensive mechanism for less important properties.</p>
<h5 id="3-1-5-Eventual-invariants-最终不变式"><a href="#3-1-5-Eventual-invariants-最终不变式" class="headerlink" title="3.1.5 Eventual invariants     最终不变式"></a>3.1.5 Eventual invariants     最终不变式</h5><p>不变式可能需要经过一定时间之后才能保持。比如，<font color=red>最终一致性</font>的规则中，每个副本不一定所有时刻都是相同的，只要在停止更新后最终变成一样就行了。这种最终一致性可能是合适的，因为副本可能在后台更新，或者使用一些反熵的机制，其中副本的接收和处理更新可能需要不确定的时间。最终一致性的概念是由分布式系统社区创造，尽管数据库社区之前就提出了在分区期间协调出现分歧的副本的想法。</p>
<p>An invariant may need to hold only after some time has passed. For example, under eventual consistency, replicas need not be the same at all times, as long as they eventually become the same when updates stop occurring. This eventual property is appropriate because replicas may be updated in the background or using some anti-entropy mechanism, where it takes an indeterminate amount of time for a replica to receive and process an update. Eventual consistency was coined by the distributed systems community [23], though the database community previously proposed the idea of reconciling replicas that diverge during partitions [9].</p>
<p>状态一致性仅限于状态上的属性，但许多情况下，Client不太关心状态，而更关心从系统获得的结果。换句话说，重要的是Client在与系统交互时观察到的行为。这些情况需要另一种不同形式的一致性，我们在下面讨论。</p>
<p>State consistency is limited to properties on state, but in many cases clients care less about the state and more about the results that they obtain from the system. In other words, what matters is the behavior that clients observe from interacting with the system. These cases call for a different form of consistency, which we discuss next.</p>
<h4 id="3-2-Operation-consistency-操作一致性"><a href="#3-2-Operation-consistency-操作一致性" class="headerlink" title="3.2 Operation consistency     操作一致性"></a>3.2 Operation consistency     操作一致性</h4><p>操作一致性关注的是Client执行的操作；它由表示操作是否返回可接受结果的属性组成。这些属性会将许多操作的执行联系在一起，就像下面的例子展示的那样。</p>
<p>操作一致性有多种子类型，这些子类型按照不同的方式定义了一致性属性。接下来我们将解释这些子类型。</p>
<p>Operation consistency pertains to the operation executions by clients; it consists of properties that indicate whether operations return acceptable results. These properties can tie together many operation executions, as shown in the examples below.</p>
<p>Operation consistency has subcategories, with different ways to define the consistency property. We explain these subcategories next.</p>
<h5 id="3-2-1-Sequential-equivalence-顺序等价性"><a href="#3-2-1-Sequential-equivalence-顺序等价性" class="headerlink" title="3.2.1 Sequential equivalence     顺序等价性"></a>3.2.1 Sequential equivalence     顺序等价性</h5><p>这种子类型从顺序执行操作（一次执行一个操作，没有并发。<em>个人理解这里的顺序执行就是串行执行</em>）得到的允许的结果的角度，来定义并发执行时允许得到的操作结果。具体而言，需要有一种方式，来执行任何Client子集提交的所有操作，然后将它们还原为一种正确的顺序执行。而具体的还原方式，则取决于特定的一致性属性。从技术角度上讲，所谓正确的顺序执行是跟系统相关的，因此也需要对系统进行指定，但是因过于明显而经常被忽略。</p>
<p>我们现在给出一些顺序等价的例子。</p>
<p>This subcategory defines the permitted operation results of a concurrent execution in terms of the permitted operation results in a sequential execution—one in which operations are executed one at a time, without concurrency. More specifically, there must be a way to take the execution of all operations submitted by any subset of clients, and then reduce them to a sequential execution that is correct. The exact nature of the reduction depends on the specific consistency property. Technically, the notion of a correct sequential execution is system dependent, so it needs to be specified as well, but it is often obvious and therefore omitted. </p>
<p>We now give some examples of sequential equivalence.</p>
<p><font color=red>线性化</font>是一种强一致性。直白的说，线性一致性的约束是这样的：每个操作都是在起始时间到结束时间之间的某个时刻上即时执行的，这些时刻上执行的操作形成了有效的执行序列。更精确的说法是，我们在并发执行的操作上定义一个偏序，如下：如果<code>op1</code>结束后<code>op2</code>才开始执行，则<code>op1 &lt; op2</code>。线性一致性必须存在一个所有操作及其结果形成的合法的总序<code>T</code>，<code>T</code>满足(1)<code>T</code>满足<code>&lt;</code>的要求，即如果<code>op1 &lt; op2</code>，那T中<code>op1</code>必须在<code>op2</code>之前；(2)<code>T</code>定义了正确的顺序执行。线性一致性通常用于定义并发数据结构的正确行为，最近它常被用于分布式系统中。</p>
<p>Linearizability [12] is a strong form of consistency. Intuitively, the constraint is that each operation must appear to occur at an instantaneous point between its start and finish times, where execution at these instantaneous points form a valid sequential execution. More precisely, we define a partial order &lt; from the concurrent execution, as follows: op1 &lt; op2 if op1 finishes before op2 starts. There must exist a legal total order T of all operations with their results, such that (1) T is consistent with &lt;, meaning that if op1 &lt; op2 then op1 appears before op2 in T, and (2) T defines a correct sequential execution. Linearizability has been traditionally used to define the correct behavior of concurrent data structures; more recently, it has also been used in distributed systems.</p>
<p><font color=red>顺序一致性</font>也是一种强一致性，尽管比线性一致性稍弱。直白的说，它要求操作的执行就像完全按照每个Client发出操作的顺序进行排序一样。更精确的说法是，我们定义一个偏序，如下：如果<code>op1</code>和<code>op2</code>都是由同一个Client发起的，并且<code>op1</code>结束后<code>op2</code>才开始执行，则<code>op1 &lt; op2</code>。必须存在一个总序<code>T</code>：(1)<code>T</code>满足&lt;的要求，即如果<code>op1 &lt; op2</code>，那<code>T</code>中<code>op1</code>必须在<code>op2</code>之前；(2)<code>T</code>定义了正确的顺序执行。这些条件与线性一致性很类似，唯一的不同是偏序&lt;仅仅反映的是每个Client上本地触发的操作的顺序。顺序一致性用于定义计算机中的强一致性内存模型，但也可以在并发数据结构的上下文中使用。</p>
<p>Sequential consistency [14] is also a strong form of consistency, albeit weaker than linearizability. Intuitively, it requires that operations execute as if they were totally ordered in a way that respects the order in which each client issues operations. More precisely, we define a partial order &lt; as follows: op1 &lt; op2 if both operations are executed by the same client and op1 finishes before op2 starts. There must exist a total order T such that (1) T is consistent with &lt;, and (2) T defines a correct sequential execution. These conditions are similar to linearizability, except that &lt; reflects just the local order of operations at each client. Sequential consistency is used to define a strongly consistent memory model of a computer, but it could also be used in the context of concurrent data structures.</p>
<p>下一个示例涉及支持事务的系统。直白的说，事务要求一个或多个操作必须作为整体来执行。更准确的说法是，有专门的操作来启动、提交和中止事务；而且操作的数据条目与事务关联。系统提供了隔离属性，确保事务之间不会相互干扰。隔离属性有多种：串行化、强会话串行化、保序串行化、快照隔离、读已提交、可重复读等等。所有这些都属于操作一致性的范畴，其中一些属于顺序等价性。下面是一些示例，所有这些示例都是在数据库系统的上下文中使用。</p>
<p>The next examples pertain to systems that support transactions. Intuitively, a transaction is a bundle of one or more operations that must be executed as a whole. More precisely, there are special operations to start, commit, and abort transactions; and operations on data items are associated with a transaction. The system provides an isolation property, which ensures that transactions do not significantly interfere with one another. There are many isolation properties: serializability, strong session serializability, order-preserving serializability, snapshot isolation, read committed, repeatable reads, etc. All of these are forms of operation consistency, and several of them are of the sequential equivalence subcategory. Here are some examples, all of which are used in the context of database systems.</p>
<p><font color=red>串行化</font>就是直观的保证每个事务看起来都是串行执行的。更准确的说法是，串行化对系统中的操作施加的约束是：与这些操作相对应的调度必须等效于事务的串行调度。串行调度称为调度的串行化。</p>
<p>Serializability [5] intuitively guarantees that each transaction appears to execute in series. More precisely, serializability imposes a constraint on the operations in a system: the schedule corresponding to those operations must be equivalent to a serial schedule of transactions. The serial schedule is called a serialization of the schedule.</p>
<p><font color=red>强会话串行化</font>解决了串行化的问题。<font color=red>串行化允许同一Client的事务的重排序</font>，但有时不希望如此。强会话串行化在串行化之上施加了额外的约束。更准确地说，每个事务都与一个会话相关联，约束条件是在保持串行化的基础上（如上所述），还必须遵守每个会话中事务的顺序：如果同一个会话中，事务<code>T1</code>发生在<code>T2</code>之前，那么串行化顺序中<code>T2</code>不会在<code>T1</code>之前。</p>
<p>Strong session serializability [8] addresses an issue with serializability. Serializability allows transactions of the same client to be reordered, which can be undesirable at times. Strong session serializability imposes additional constraints on top of serializability. More precisely, each transaction is associated with a session, and the constraint is that serializability must hold (as defined above) and the serialization must respect the order of transactions within every session: if transaction T1 occurs before T2 in the same session, then T2 is not serialized before T1.</p>
<p><font color=red>保序串行化</font>，也称为严格串行化或强串行化，要求串行化顺序必须保持事务的实时顺序。更准确地说，约束条件是在保证串行化的基础上，还须满足以下要求：如果事务<code>T1</code>在<code>T2</code>开始之前提交，那么串行化顺序中<code>T2</code>不会出现在T1之前。</p>
<p>Order-preserving serializability [24], also called strict serializability [6, 17] or strong serializability [7], requires that the serialization order respect the real-time ordering of transactions. More precisely, the constraint is that serializability must hold and the serialization must satisfy the requirement that, if transaction T1 commits before T2 starts, then T2 is not serialized before T1.</p>
<h5 id="3-2-2-Reference-equivalence-引用等效性"><a href="#3-2-2-Reference-equivalence-引用等效性" class="headerlink" title="3.2.2 Reference equivalence     引用等效性"></a>3.2.2 Reference equivalence     引用等效性</h5><p><em>本节看不太懂，个人理解：引用就是顺序的一种泛化，是人为定义的一种关系，满足这种关系的操作就符合引用等效性</em></p>
<p>引用等价是顺序等价的泛化。它通过要求并发执行等效于给定引用来定义允许的操作结果，其中等效的概念和引用取决于具体的一致性属性。现在我们给出一些事务系统的示例。这些示例经常出现在数据库系统的上下文中。</p>
<p>Reference equivalence is a generalization of sequential equivalence. It defines the permitted operation results by requiring the concurrent execution to be equivalent to a given reference, where the notion of equivalence and the reference depend on the consistency property. We now give some examples for systems with transactions. These examples occur often in the context of database systems.</p>
<p>快照隔离要求事务的行为与某个引用实现相同，即事务必须具有与引用实现相同的结果，并且操作必须返回相同的结果。引用实现如下。当事务开始时，它会被分配一个单调的开始时间戳。当事务读取数据时，它从开始时间戳对应的快照中读取。当事务<code>T1</code>希望提交时，系统获得单调的提交时间戳，并验证是否存在其他事务<code>T2</code>，满足(1)<code>T2</code>更新<code>T1</code>也更新的某个项目，并且(2)<code>T2</code>使用<code>T1</code>的开始和提交时间戳之间的提交时间戳提交。如果是，则<code>T1</code>中止；否则，<code>T1</code>将被提交，并且以<code>T1</code>的提交时间戳，将其所有更新立即应用。</p>
<p>Snapshot isolation [4] requires that transactions behave identically to a certain reference implementation, that is, transactions must have the same outcome as in the reference implementation, and operations must return the same results. The reference implementation is as follows. When a transaction starts, it gets assigned a monotonic start timestamp. When the transaction reads data, it reads from a snapshot of the system as of the start timestamp. When a transaction T1 wishes to commit, the system obtains a monotonic commit timestamp and verifies whether there is some other transaction T2 such that (1) T2 updates some item that T1 also updates, and (2) T2 has committed with a commit timestamp between T1’s start and commit timestamp. If so, then T1 is aborted; otherwise, T1 is committed and all its updates are applied instantaneously as of the time of T1’s commit timestamp.</p>
<p>有趣的是，接下来的两个属性是引用等价的示例，而其中引用本身则由另一个一致性属性定义。在第一个示例中，引用等效属性属于串行等效子类型，在第二个示例中，它属于引用等效子类型。</p>
<p>Interestingly, the next two properties are examples of reference equivalence where the reference is itself defined by another consistency property. This other property is in the serial equivalence subcategory in the first example, and it is in the reference equivalence subcategory in the second example.</p>
<p>单副本的可序列化性属于复制的数据库系统。复制系统的行为必须类似于引用系统，该引用系统是一个未被复制并提供可序列化性的系统。</p>
<p>One-copy serializability [5] pertains to a replicated database system. The replicated system must behave like a reference system, which is a system that is not replicated and provides serializability.</p>
<p>单副本快照隔离也适用于复制系统。要求是，它的行为必须像一个未复制的系统，并提供快照隔离。</p>
<p>One-copy snapshot isolation [15] also pertains to a replicated system. The requirement is that it must behave like a system that is not replicated and that provides snapshot isolation.</p>
<h5 id="3-2-3-Read-write-centric-以读写为中心"><a href="#3-2-3-Read-write-centric-以读写为中心" class="headerlink" title="3.2.3 Read-write centric     以读写为中心"></a>3.2.3 Read-write centric     以读写为中心</h5><p>上述操作一致性的子类型适用于具有任意操作的系统。而以约束读写为核心的一致性子类型适用于具有两个非常特定操作–读和写–的系统。这些系统包括许多存储系统，如块存储系统、键值存储系统和访问内存的处理器。通过关注这两个操作，这种子类型的一致性允许直接使用操作的语义。特别是，写入操作只返回确认或错误状态，而不返回任何信息，所以写操作不会影响一致性。因此，<font color=red>这种一致性属性侧重于读的结果</font>。正如我们现在所解释的，这些属性的共同点是，读取可以看到一组写入的值。每次读取都会受到系统中写入的影响；如果每次写入都覆盖整个数据项，则写入操作将相互覆盖，读取操作将返回其中一个写入的值。但是，如果写入只更新了数据项的一部分，则读取将以适当的顺序返回写入值的组合。无论哪种情况，关键的考虑因素都是可能影响读取的写入集，而不管写入是否是部分写入；我们说，读能看到写。这个概念用于定义几个已知的一致性属性，我们现在举例说明。</p>
<p>The above subcategories of operation consistency apply to systems with arbitrary operations. The read-write centric subcategory applies to systems with two very specific operations: read and write. These systems are important because they include many types of storage systems, such as block storage systems, key value storage systems, and processors accessing memory. By focusing on the two operations, this subcategory permits properties that directly evoke the semantics of the operations. In particular, a write operation returns no information other than an acknowledgment or error status, which has no consistency implications. Thus, the consistency properties focus on the results of reads. Common to these properties is the notion of a read seeing the values of a set of writes, as we now explain. Each read is affected by some writes in the system; if every write covers the entire data item, then writes overwrite each other and the read returns the value written by one of them. But if the writes update just part of a data item, the read returns a combination of the written values in some appropriate order. In either case, the crucial consideration is the set of writes that could have potentially affected the read, irrespective of whether the writes are partial or not; we say that the read sees those writes. This notion is used to define several known consistency properties, as we now exemplify.</p>
<p><font color=red>“Read-my-writes”</font>要求，按照它们的执行顺序，Client的读操作至少能看到<font color=red>同一Client</font>以前执行的所有写操作。这种一致性适用于当Client希望能观察自己的写操作，但在观察他人的写操作时可以容忍延迟的情况。“Read-my-writes”通常与另一个读写一致性属性相结合，比如有界陈旧性或操作最终一致性。所谓结合，意思是系统必须同时满足“Read-my-writes”一致性和另一个一致性属性。“Read-my-writes”最初是在分布式系统的上下文中定义的，然后在计算机体系结构中用于定义内存模型。</p>
<p>Read-my-writes [21] requires that a read by a client sees at least all writes previously executed by the same client, in the order in which they were executed. This property is relevant when clients expect to observe their own writes, but can tolerate delays before observing the writes of others. Typically, read-my-writes is combined with another read-write consistency property, such as bounded staleness or operational eventual consistency, defined below. By combined we mean that the system must provide both read-my-writes and the other property. Read-my-writes was originally defined in the context of distributed systems [21], then used in computer architecture to define memory models [19].</p>
<p><font color=red>有界陈旧性</font>直观地限制了读操作看到写操作所需的时间。更准确地说，该属性有一个参数<code>e</code>，因此读取必须至少在读取开始之前的<code>e</code>时间内看到所有已完成的写入。该属性适用于可以容忍不一致的时间范围为<code>e</code>的场景，或者是小于e的时间范围client无法感知的场景。有界陈旧性最初用在数据库系统的上下文中，后来也在云分布式系统的上下文中使用。</p>
<p>Bounded staleness [1], intuitively, bounds the time it takes for writes to be seen by reads. More precisely, the property has a parameter e, such that a read must see at least all writes that complete e time before the read started. This property is relevant when inconsistencies are tolerable in the short term as defined by e, or when time intervals smaller than e are imperceptible by clients (e.g., e is in the tens of milliseconds and clients are humans). Bounded staleness was originally defined in the context of database systems [1] and has been used more recently in the context of cloud distributed systems [20].</p>
<p>操作<font color=red>最终一致性</font>是最终一致性（状态一致性的一种形式）的变体。要求是每次写入最终都能被所有读取看到，如果Client停止执行写入，那么每次读取最终都会返回相同的最新值。</p>
<p>Operational eventual consistency is a variant of eventual consistency (a form of state consistency) defined using operation consistency. The requirement is that each write be eventually seen by all reads, and if clients stop executing writes then eventually every read returns the same latest value [22].</p>
<p>缓存一致性源于计算机体系结构，用于定义内存缓存的正确行为。直观地说，缓存一致性要求对单个数据项（内存位置）的读写满足某些属性。文献中的属性各不相同。一种是要求对于每个数据项：(1)某个Client的读取能返回该Client之前写入的值，除非另一个Client在这期间进行了写入；(2)如果写入和读取在时间上充分分离，并且在这两个操作之间没有发生其他写入，则读取能返回另一个Client的写入值；（3）写操作是序列化的。</p>
<p>Cache coherence originates from computer architecture to define the correct behavior of a memory cache. Intuitively, cache coherence requires that reads and writes to an individual data item (a memory location) satisfy some properties. The properties vary across the literature. One possibility [11] is to require that, for each data item: (1) a read by some client returns the value of the previous write by that client, unless another client has written in between, (2) a read returns the value of a write by another client if the write and read are sufficiently separated in time and if no other write occurred in between, and (3) writes are serialized.</p>
<h4 id="3-3-Discussion-讨论"><a href="#3-3-Discussion-讨论" class="headerlink" title="3.3 Discussion     讨论"></a>3.3 Discussion     讨论</h4><p>现在，我们根据抽象级别、复杂性、能力和应用程序依赖性来比较状态一致性和操作一致性。</p>
<p>We now compare state consistency and operation consistency in terms of their level of abstraction, complexity, power, and application dependence.</p>
<h5 id="3-3-1-Level-of-abstraction-抽象级别"><a href="#3-3-1-Level-of-abstraction-抽象级别" class="headerlink" title="3.3.1 Level of abstraction     抽象级别"></a>3.3.1 Level of abstraction     抽象级别</h5><p>操作一致性是一个端到端的属性，因为它处理的是Client可以直接观察到的结果。这与状态一致性相反，状态一致性处理Client通过执行操作间接观察到的系统状态。换句话说，操作一致性比状态一致性处于更高的抽象级别。因此，<font color=red>系统可能存在严重的状态不一致性，但可以从外部隐藏这些不一致性，以提供强大的操作一致性。</font></p>
<p>Operation consistency is an end-to-end property, because it deals with results that clients can observe directly. This is in contrast to state consistency, which deals with system state that clients observe indirectly by executing operations. In other words, operation consistency is at a higher level of abstraction than state consistency. As a result, a system might have significant state inconsistencies, but hide these inconsistencies externally to provide a strong form of operation consistency.</p>
<p>一个有趣的例子是一个有3个节点的存储系统，3个节点使用majority quorum进行复制，其中(1)为了写入数据，系统附加了一个单调的时间戳，并将数据存储在2个（大多数）节点上；(2)为了读取，系统从两个节点上获取数据；如果节点返回相同的数据，系统将数据返回给Client；否则，系统会选取时间戳最高的数据，将该数据及其时间戳存储在另一台节点中（以确保两个节点都有数据），然后将数据返回给客户端。此系统违反了相互一致性，因为当没有未完成的操作时，其中一个节点会与其他两个节点不一致。然而，这种不一致性在read返回的结果中是看不到的，因为read通过查询多数来过滤不一致的节点。事实上，此存储系统满足线性一致性，这是操作一致性的最强形式之一。</p>
<p>An interesting example is a storage system with three servers replicated using majority quorums [3], where (1) to write data, the system attaches a monotonic timestamp and stores the data at two (a majority of) servers, and (2) to read, the system fetches the data from two servers; if the servers return the same data, the system returns the data to the client; otherwise, the system picks the data with the highest timestamp, stores that data and its timestamp in another server (to ensure that two servers have the data), and returns the data to the client. This system violates mutual consistency, because when there are no outstanding operations, one of the servers deviates from the other two. However, this inconsistency is not observable in the results returned by reads, since a read filters out the inconsistent server by querying a majority. In fact, this storage system satisfies linearizability, one of the strongest forms of operation consistency.</p>
<h5 id="3-3-2-Complexity-复杂性"><a href="#3-3-2-Complexity-复杂性" class="headerlink" title="3.3.2 Complexity     复杂性"></a>3.3.2 Complexity     复杂性</h5><p>操作一致性比状态一致性更复杂。通过状态一致性，开发人员可以直接了解他们可以从系统中期望的状态。每个属性都关注于不依赖执行的特定数据项。因此，状态一致性是直观的，易于表达和理解。此外，可以通过分析系统状态的快照来检查状态一致性，这有助于调试。</p>
<p>Operation consistency is more complex than state consistency. With state consistency, developers gain a direct understanding of what states they can expect from the system. Each property concerns specific data items that do not depend on the execution. As a result, state consistency is intuitive and simple to express and under-stand. Moreover, state consistency can be checked by analyzing a snapshot of the system state, which facilitates debugging.</p>
<p>相比之下，操作一致性属性在操作之间建立了关系，这些操作随着时间的推移可能会分布在多个Client上，这会造成复杂性。从第3.2节中的示例可以看出，这种复杂性使得操作一致性不那么直观，更难理解。此外，检查操作一致性需要分析执行日志，这会使调试复杂化。</p>
<p>By contrast, operation consistency properties establish relations between operations that are spread over time and possibly over many clients, which creates complexity. This complexity makes operation consistency less intuitive and harder to understand, as can be observed from the examples in Section 3.2. Moreover, checking operation consistency requires analyzing an execution log, which complicates debugging.</p>
<h5 id="3-3-3-Power-能力"><a href="#3-3-3-Power-能力" class="headerlink" title="3.3.3 Power     能力"></a>3.3.3 Power     能力</h5><p>操作一致性和状态一致性具有不同的能力。操作一致性可以看到系统中的所有操作，从而约束操作的顺序和结果。如果系统是确定性的，那么操作一致性属性可以从操作中重构系统的状态，从而间接地约束状态，就像状态一致性一样。但是，当系统不确定时（例如，由于并发、计时或外部事件），通常不可能这样做。</p>
<p>Operation consistency and state consistency have different powers. Operation consistency can see all operations in the system, which permits constraining the ordering and results of operations. If the system is deterministic, operation consistency properties can reconstruct the state of the system from the operations, and thereby indirectly constrain the state much like state consistency. But doing so is not generally possible when the system is non-deterministic (e.g., due to concurrency, timing, or external events).</p>
<p>另一方面，状态一致性可以看到系统的整个状态，从而可以约束可能破坏状态的操作。如果系统在其状态下记录其所有操作，则状态一致性可以像操作一致性一样间接约束操作的结果。然而，通常禁止记录所有操作，因此这只是一种理论能力。</p>
<p>State consistency, on the other hand, can see the entire state of the system, which permits constraining operations that might break the state. If the system records all its operations in its state, then state consistency can indirectly constrain the results of operations much like operation consistency. However, it is often prohibitive to record all operations so this is only a theoretical capability.</p>
<h5 id="3-3-4-Application-dependence-应用程序依赖性"><a href="#3-3-4-Application-dependence-应用程序依赖性" class="headerlink" title="3.3.4 Application dependence     应用程序依赖性"></a>3.3.4 Application dependence     应用程序依赖性</h5><p>状态一致性往往依赖于应用程序，因为属性涉及状态，并且系统的正确状态因应用程序而异。因此，开发人员需要为每个系统找出正确的属性，这需要时间和精力。此外，在某些情况下，没有强制执行状态一致性的通用机制，开发人员必须编写与每个属性密切相关的应用程序代码。有两个值得注意的例外：相互一致性和最终一致性。这些属性通过引用复制状态而广泛应用于任何复制的系统，而与应用程序无关，并且有通用的复制机制来强制执行这些属性。</p>
<p>State consistency tends to be application dependent, because the properties concern state, and the correct state of a system varies significantly from application to application. As a result, developers need to figure out the right properties for each system, which takes time and effort. Moreover, in some cases there are no general mechanisms to enforce state consistency and developers must write application code that is closely tied to each property. There are two noteworthy exceptions: mutual consistency and eventual consistency. These properties apply broadly to any replicated system, by referring to the replicated state irrespective of the application, and there are general replication mechanisms to enforce such properties.</p>
<p>操作一致性通常与应用程序无关。它通过两种方式实现应用程序独立性。首先，一些属性通过将并发操作下的系统行为减少为顺序操作下的行为（如在顺序等价子类型中）或引用下的行为（如在引用等价子类型中）来考虑特定于应用程序的行为。其次，一些属性侧重于应用于许多系统的特定操作，如读和写（如以读写为中心的子类型）。理论上，操作一致性可能高度依赖于应用程序，但这并不常见。例如，许多设备都可以访问电子邮件系统，其中每个操作（读取、删除、移动）可能根据其语义和用户的期望对其响应有不同的约束。</p>
<p>Operation consistency is often application independent. It achieves application independence in two ways. First, some properties factor out the application-specific behavior, by reducing the behavior of the system under concurrent operations to behavior under sequential operations (as in the sequential equivalence subcategory), or behavior under a reference (as in the reference equivalence subcategory). Second, some properties focus on specific operations, such as read and write, that apply to many systems (as in the read-write centric subcategory). Theoretically, operation consistency can be highly application dependent, but this is not common. An example might be an email system accessible by many devices, where each operation (read, delete, move) might have different constraints on their response according to their semantics and the expectations of users.</p>
<h5 id="3-3-5-Which-type-to-use-使用哪一种"><a href="#3-3-5-Which-type-to-use-使用哪一种" class="headerlink" title="3.3.5 Which type to use?     使用哪一种"></a>3.3.5 Which type to use?     使用哪一种</h5><p>要决定使用哪种类型的一致性，我们建议考虑以下几点。首先，<font color=red>思考一致性的反面：必须避免的不一致是什么？</font>如果答案用不需要的状态来描述更容易的话（例如，两个副本不一致），则使用状态一致性。如果答案由操作的错误结果描述更容易的话（例如，读取返回陈旧数据），则使用操作一致性。</p>
<p>To decide what type of consistency to use, we suggest taking a few things into consideration. First, think about the negation of consistency: what are the inconsistencies that must be avoided? If the answer is most easily described by an undesirable state (e.g., two replicas diverge), then use state consistency. If the answer is most easily described by an incorrect result to an operation (e.g., a read returns stale data), then use operation consistency.</p>
<p>第二个重要的考虑因素是应用程序依赖性。许多操作一致性和一些状态一致性属性与应用程序无关（例如，可串行化、线性化、相互一致性、最终一致性）。我们建议在定义特定于应用程序的属性之前尝试使用这些属性，因为它们的机制已经很好地理解了。如果系统需要特定于应用程序的属性，并且状态和操作一致性都是自然选择，那么由于其简单性，我们建议使用状态一致性。</p>
<p>A second important consideration is application dependency. Many operation consistency and some state consistency properties are application independent (e.g., serializability, linearizability, mutual consistency, eventual consistency). We recommend trying to use such properties, before defining an application-specific one, because the mechanisms to enforce them are well understood. If the system requires an application specific property, and state and operation consistency are both natural choices, then we recommend using state consistency due to its simplicity.</p>
<h3 id="4-Consistency-in-different-disciplines-不同学科的一致性"><a href="#4-Consistency-in-different-disciplines-不同学科的一致性" class="headerlink" title="4 Consistency in different disciplines     不同学科的一致性"></a>4 Consistency in different disciplines     不同学科的一致性</h3><p>现在，我们将在第3节中讨论一致性在每个学科中的含义，为什么它与该学科相关，以及它如何与这两种类型的一致性相关。我们还指出了在一个学科中被认为是一致的概念，但在另一个学科中则不一致。</p>
<p>We now discuss what consistency means in each discipline, why it is relevant in that discipline, and how it relates to the two types of consistency in Section 3. We also point out concepts that are considered to be consistency in one discipline but not in another.</p>
<h4 id="4-1-Distributed-systems-分布式系统"><a href="#4-1-Distributed-systems-分布式系统" class="headerlink" title="4.1 Distributed systems     分布式系统"></a>4.1 Distributed systems     分布式系统</h4><p>在分布式系统中，一致性指的是状态一致性或操作一致性。早期的复制协议侧重于提供相互一致性，而许多云分布式系统则提供最终的一致性。这些是状态一致性的示例。一些系统旨在提供线性化能力或各种以读写为中心的一致性。这些是操作一致性的示例。</p>
<p>In distributed systems, consistency refers to either state or operation consistency. Early replication protocols focused on providing mutual consistency while many cloud distributed systems provide eventual consistency. These are examples of state consistency. Some systems aim at providing linearizability or various flavors of read-write centric consistency. These are examples of operation consistency.</p>
<p>在分布式系统中，一致性是一个重要的考虑因素，因为这类系统面临着许多妨碍或阻碍一致性的问题：由慢速网络分隔的Client、发生故障的机器、相互断开连接的Client、系统对大量Client的可扩展性以及高可用性。这些问题可能会导致难以提供高水平的一致性，因为一致性需要客户协调，而这可能是不可能的。因此，分布式系统可能会采用较弱的一致性级别，根据应用程序的需要进行选择。</p>
<p>Consistency is an important consideration in distributed systems because such systems face many concerns that preclude or hinder consistency: clients separated by a slow network, machines that fail, clients that disconnect from each other, scalability of the system to a large number of clients, and high availability. These concerns can make it hard to provide strong levels of consistency, because consistency requires client coordination that may not be possible. As a result, distributed systems may adopt weaker levels of consistency, chosen according to the needs of applications.</p>
<p>云系统是一种有趣的分布式系统，它面临着上述所有问题：系统是地理分布的（分布在全球各地），数据中心之间存在显著的延迟分离；机器经常失败，因为它们太多了；由于广域链路出现问题或拥塞，Client与远程数据中心断开连接；许多客户都很活跃，系统必须为所有客户提供良好的服务；而且该系统必须尽可能可用，因为企业在停机期间会亏损。由于这些挑战，云系统通常求助于较弱的一致性级别。</p>
<p>Cloud systems, an interesting type of distributed system, face all of the above concerns with intensity: the systems are geo-distributed (distributed around the globe) with significant latency separating data centers; ma-chines fail often because there are many of them; clients disconnect from remote data centers due to problems or congestion in wide-area links; many clients are active and the system must serve all of them well; and the system must be available whenever possible since businesses lose money during downtime. Because of these challenges, cloud systems often resort to weak levels of consistency.</p>
<h4 id="4-2-Database-systems-数据库系统"><a href="#4-2-Database-systems-数据库系统" class="headerlink" title="4.2 Database systems     数据库系统"></a>4.2 Database systems     数据库系统</h4><p>在数据库系统中，<font color=red>一致性是指状态一致性</font>。例如，考虑描述交易担保的ACID首字母缩略词。“C”代表一致性，在这种情况下，这意味着<font color=red>数据库始终处于开发人员认为有效的状态：系统必须保持不变式，</font>例如唯一性约束、引用完整性和特定于应用程序的属性（例如，如果y是x的朋友，则x是y的朋友）。这些都是状态一致性。</p>
<p>In database systems, consistency refers to state consistency. For example, consider the ACID acronym that de-scribes the guarantees of transactions. The “C” stands for consistency, which in this case means that the database is always in a state that developers consider valid: the system must preserve invariants such as uniqueness con-straints, referential integrity, and application-specific properties (e.g., x is a friend of y iff y is a friend of x). These are flavors of state consistency.</p>
<p>“A”代表原子性，“I”代表隔离。有趣的是，原子性和隔离是操作一致性的例子。原子性要求事务要么整体执行，要么根本不执行，而隔离要求事务看起来是自己执行，没有太多干扰。有许多不同级别的隔离（可串行化、快照隔离、已提交读取、可重复读取等），但它们都限制了操作的行为。</p>
<p>The “A” stands for atomicity and the “I” stands for isolation. Interestingly, atomicity and isolation are examples of operation consistency. Atomicity requires that a transaction either executes in its entirety or does not execute at all, while isolation requires that transactions appear to execute by themselves without much interference. There are many different levels of isolation (serializability, snapshot isolation, read committed, repeatable reads, etc), but they all constrain the behavior of operations.</p>
<p>虽然数据库系统社区将事务隔离与一致性和原子性分开，但在分布式系统社区中，事务隔离被视为一致性的一种形式，而在计算机体系结构社区中，类似于隔离的概念被称为原子性。我们不知道为什么这些术语在社区中有不同的含义。但我们怀疑其中的一个原因是，这些概念之间存在相互交织的想法，这是我们在本文中试图确定和澄清的。</p>
<p>Although the database systems community separates transaction isolation from consistency and atomicity, in the distributed systems community, transaction isolation is seen as a form of consistency, while in the computer architecture community, a concept analogous to isolation is called atomicity. We do not know exactly why these terms have acquired different meanings across communities. But we suspect that a reason is that there are intertwined ideas across these concepts, which is something we try to identify and clarify in this article.</p>
<p>一致性在数据库系统中很重要，因为数据是首要考虑的问题；事实上，数据可能比此类系统中操作的结果更重要（例如，只要数据不被破坏，操作就可能失败）。不同类型的一致性是由于数据库中存在不同类型的不变量，每个不变量都有自己的强制机制。例如，唯一性约束由执行引擎中的索引和检查强制执行；应用程序特定的约束由应用程序逻辑强制执行；而相互一致性是由replication manager实施的。</p>
<p>Consistency is important in database systems because data is of primary concern; in fact, data could be even more important than the result of operations in such systems (e.g., operations can fail as long as data is not destroyed). Different types of consistency arise because of the different classes of invariants that exist in the database, each with its own enforcement mechanism. For example, uniqueness constraints are enforced by an index and checks in the execution engine; application-specific constraints are enforced by the application logic; and mutual consistency is enforced by the replication manager.</p>
<h4 id="4-3-Computer-architecture-计算机体系结构"><a href="#4-3-Computer-architecture-计算机体系结构" class="headerlink" title="4.3 Computer architecture     计算机体系结构"></a>4.3 Computer architecture     计算机体系结构</h4><p>在计算机体系结构中，一致性是指操作的一致性。一个类似的称为coherence的概念也是操作一致性的一种形式。Consistency和Coherence 有细微的区别。Consistency涉及整个内存系统；它限制整个内存系统的读取和写入的行为；Coherence 涉及缓存子系统，这可以看作是负责给定内存位置的各种缓存操作的一致性。因此，Coherence 限制了加载和存储到单个内存位置的行为。</p>
<p>In computer architecture, consistency refers to operation consistency. A similar concept called coherence is also a form of operation consistency. Consistency and coherence have a subtle difference. Consistency concerns the entire memory system; it constrains the behavior of reads and writes—called loads and stores—across all the memory locations; an example is the sequential consistency property. Coherence concerns the cache subsystem; it can be seen as consistency of the operation of the various caches responsible for a given memory location. Thus, coherence constrains the behavior of loads and stores to an individual memory location.</p>
<p>Coherence 和consistency 是分开的，以允许系统的模块化体系结构：缓存一致性协议确保缓存子系统的正确行为，而系统的其余部分确保跨内存访问的一致性，而无需担心缓存子系统。</p>
<p>Coherence and consistency are separated to permit a modular architecture of the system: a cache coherence protocol ensures the correct behavior of the caching subsystem, while the rest of the system ensures consistency across memory accesses without worrying about the cache subsystem.</p>
<p>Consistency 和coherence 是计算机体系结构中的一个问题，因为越来越多的计算机系统有许多内核或处理器共享对公共内存的访问：在这样的系统中，内存位置上的并发操作和跨多个缓存的数据复制会导致数据共享问题。</p>
<p>Consistency and coherence arise as issues in computer architecture because increasingly computer systems have many cores or processors sharing access to a common memory: in such systems, there are concurrent operations on memory locations and data replication across many caches, which lead to problems of data sharing.</p>
<h3 id="5-Conclusion-总结"><a href="#5-Conclusion-总结" class="headerlink" title="5 Conclusion     总结"></a>5 Conclusion     总结</h3><p>正如我们在这里简要描述的那样，一致性是一个跨多学科的重要问题。这种重要性源于这些学科中的并发和复制的兴起，我们预期这一趋势会继续下去。不幸的是，一致性很微妙，很难把握，更糟糕的是，它在不同的领域有不同的名称和含义。我们希望通过确定两种广泛且非常不同的一致性类型，即跨学科的状态一致性和操作一致性，来阐明这一主题。</p>
<p>Consistency is a concern that spans many disciplines, as we briefly described here. This concern stems from the rise of concurrency and replication across these disciplines, a trend that we expect to continue. Unfortunately, consistency is subtle and hard to grasp, and to make matters worse, it has different names and meanings across communities. We hope to have shed some light on this subject by identifying two broad and very different types of consistency—state consistency and operation consistency—that can be seen across the disciplines.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>论文</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>【转载】TiDB 在 Raft 成员变更上踩的坑</title>
    <url>/2022/04/21/%E5%88%86%E5%B8%83%E5%BC%8F/raft/%E8%BD%AC%E8%BD%BD/14-3%5B%E8%BD%AC%E8%BD%BD%5DTiDB%20%E5%9C%A8%20Raft%20%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4%E4%B8%8A%E8%B8%A9%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<p><em><font color=red>个人理解</font>：本文最核心的观点，就是针对<code>2n</code>这样偶数节点组成的集群，除了过半节点之外，人为增加一种Quorum，即将两个<code>n</code>节点组成的集合中的其中一个作为Quorum，只要集群中所有节点都遵守这个约定，则<code>2n</code>节点的集群发生分区形成<code>2</code>个<code>n</code>节点集合时，其中之一依然可以继续提供服务，而不是像之前那样直接不可用。</em></p>
<span id="more"></span>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>上次跟好基 黄东旭 在咖啡厅撩天的时候谈笑风生地探讨了一个 <a href="https://pingcap.com/en/">TiDB</a> 使用 <a href="https://raft.github.io/">Raft</a> 时遇到的问题:</p>
<p>TiKV 层的 Raft 实现, 使用的是 Raft <a href="https://gist.github.com/ongardie/a11f32b70581e20d6bcd">单步变更</a> 算法(每次添加或删除一个节点), 例如副本由 <code>abc</code> 变成 <code>bcd</code> 过程中, 先加入 <code>d</code>, 变成 <code>abcd</code> , 再去掉 <code>a</code> 变成最终配置 <code>bcd</code>.</p>
<p>这中间经历的4节点的状态 <code>abcd</code>, 有可能在出现二分的网络割裂(<code>ad | bc</code>)时导致整个集群无法选出leader. 这种网络割裂在跨机房部署时容易出现, 例如 a, b, c 三个节点部署在3个机房:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> a      b      c</span><br><span class="line">----   ----   ----</span><br><span class="line">DC-1   DC-2   DC-3</span><br><span class="line"></span><br><span class="line">        | add `d` in DC-1</span><br><span class="line">        v</span><br><span class="line"></span><br><span class="line"> a      b      c     partitioned     a   |  b      c</span><br><span class="line"> d                   DAMN IT !!!     d   |</span><br><span class="line">----   ----   ----   ------------&gt;  ---- | ----   ----</span><br><span class="line">DC-1   DC-2   DC-3                  DC-1 | DC-2   DC-3</span><br><span class="line"></span><br><span class="line">        | remove `a`,</span><br><span class="line">        | WELL DONE !!!</span><br><span class="line">        v</span><br><span class="line"></span><br><span class="line">        b      c</span><br><span class="line"> d</span><br><span class="line">----   ----   ----</span><br><span class="line">DC-1   DC-2   DC-3</span><br></pre></td></tr></table></figure>

<ul>
<li>正常情况下, 任意一个机房和外界连接中断, 都可以用剩下的2个机房选出leader继续工作.</li>
<li>在成员变更过程中, 例如上面需要将DC-1中的 a 节点迁移到 d 节点, 中间状态 DC-1 有 ad 2个节点. 这时如果 DC-1 跟外界联系中断, 由于4节点的 majority 需要至少3个节点, 导致 DC-1 内部无法选出 leader, DC-2 和 DC-3 也不能一起选出一个leader.</li>
</ul>
<p>在4节点的中间状态, 任一 majority 都必须包含 DC-1, 从而 DC-1 就成了系统的<strong>故障单点</strong>.</p>
<p>当时给东旭一个提权重的方式解决这个问题. 后来想来这可能是一个分布式生产环境中比较常见的问题, 于是做下整理, 这个版本比当时给东旭的解决方案简化了一下, 加了一些简单的证明.</p>
<hr>
<p>这个问题的根本原因在于, <a href="https://raft.github.io/">Raft</a> 单步变更算法对 quorum 定义得过于死板, 仅用了 majority. 解决问题的关键就在于打破这个限制, 我们将从 quorum 的视角解释为何 Raft 的单步变更是一个 <strong>看起来很香的鸡肋</strong>. 然后再从工程的角度找一个简单又容易的实现方案, 也就是joint consensus.</p>
<p>从 quorum 的视角分析系统的方法, 可以参考我之前这篇文章: <a href="https://blog.openacid.com/algo/quorum/">多数派读写的少数派实现</a> .</p>
<h3 id="分析和尝试"><a href="#分析和尝试" class="headerlink" title="分析和尝试"></a>分析和尝试</h3><p>先看看在这个问题中, 整个系统的 quorum 集合都有哪些:</p>
<ul>
<li>初始状态 abc 的 quorum 的集合是 abc 所有的 majority: M(abc) &#x3D; {ab, ac, bc}, (abc虽然也是一个quorum, 但可用ab ∪ bc得到, 就不必列出了, 我们只需要列出 quorum 集合中无法由并集求出的那些集合);</li>
<li>最终状态 bcd 的 quorum 的集合 M(bcd) &#x3D; {bc, cd, bd};</li>
<li>单步变更的中间状态 abcd 的 quorum 集合也是一个 majority 集合: M(abcd) &#x3D; {abc, abd, acd, bcd};</li>
</ul>
<p>单步变更的过程是也就是 quorum 集合变化的过程:</p>
<p><strong>M(abc) → M(abcd) → M(bcd)</strong></p>
<p>在我们这个网络割裂造成的可用性问题中, 直接原因是中间状态的 quorum 要求至少3个节点, 如果网络割裂成<code>ad | bc</code>时, ad 或 bc 都不是一个 quorum. 导致无法选主.</p>
<p>那么要解决这个问题似乎也很简单: 在4节点的中间状态中, 试试也 <strong>允许 bc 作为一个合法的 quorum</strong> 看行不行? 重新定义4节点 abcd 的 quorum 集合是:</p>
<p>Q(abcd) &#x3D; M(abcd) ∪ {bc}</p>
<p>即, <strong>如果一条日志复制到 bc 或 abcd 的一个 majority, 都可以commit.</strong></p>
<blockquote>
<p>因为 bc 和 M(abcd) 中每个 quorum 都有交集, 加入 bc 后的 Q(abcd) 还是一个完整的 quorum 集合, 那就可以在新的中间状态安全的运行 paxos 或 Raft. 一致性仍然得到了保证!</p>
</blockquote>
<p>而整个变更过程也变成了: M(abc) → M(abcd) ∪ {bc} → M(bcd).</p>
<p>另外, <strong>如果 Raft 保证 M(abc) → M(abcd) 的单步变更正确性, 那它也可以保证 M(abc) → M(abcd) ∪ {bc} 的正确性</strong>.</p>
<blockquote>
<p>这是因为 Raft 单步变更的正确性保证是: 两个节点集合 C₁ 到 C₂ 的变更中, C₁ 的一个 quorum 跟 C₂ 的一个 quorum 都有交集.</p>
<p>同理 M(abcd) ∪ {bc} → M(bcd) 也能保证正确.</p>
</blockquote>
<p>这样我们就从治标的层面上解决了变更过程中的网络割裂造成的可用性问题.</p>
<p>然后再深入一点, 4节点的中间状态的 majority 具有这种可用性缺陷的原因在于, <strong>majority 集合 M(abcd) 不是 4节点的最大的 quorum 集合</strong>, majority 在节点数是 <strong>奇数</strong> 的情况下还算勉强可以用, 解决了大多数问题. 而在节点数是 <strong>偶数</strong> 的时候, <strong>majority 没有能力描述系统最大的 quorum 集合</strong>.</p>
<p><strong>majority 是 Raft 设计上的第一个不足</strong>. Raft 选择 majority 的同时, 就自宫的降低了自己的可用性.</p>
<h4 id="4节点系统的-majority-的缺陷"><a href="#4节点系统的-majority-的缺陷" class="headerlink" title="4节点系统的 majority 的缺陷"></a>4节点系统的 majority 的缺陷</h4><p>4节点系统中, 除了4个3节点的 quorum, 还可以至多包含3个2节点的quorum:</p>
<p>我们可以为4节点系统设计一个改进版的 quorum 集合 Q’(abcd) &#x3D; M(abcd) ∪ <strong>{ab, bc, ac}</strong>, 可以看到 Q’(abcd) 中任意2个元素都有交集, 运行 paxos 或 Raft 是完全没有问题的.</p>
<p>很多分布式系统的论文描述都以奇数个节点作为前提假设. 因为奇数节点可用性的性价比更高, 而忽略了偶数节点数的情况的介绍.</p>
<h4 id="majority-的扩张"><a href="#majority-的扩张" class="headerlink" title="majority 的扩张"></a>majority 的扩张</h4><p>综上, 我们可以改进下集群的 quorum 配置, 来提升系统的可用性(解决二分网络割裂问题). 假设节点集合是C, 例如 C &#x3D; {a,b,c}</p>
<ul>
<li><p>对奇数节点, n &#x3D; 2k+1, 还是沿用 <strong>多数派</strong> 节点的集合, 大部分场合都可以很好的工作:</p>
<p>[Q_{odd}(C) &#x3D; M(C) &#x3D; { q : q \subseteq C, |q| &gt; |C|&#x2F;2 }]</p>
</li>
<li><p>对偶数节点, n &#x3D; 2k, <strong>因为n&#x2F;2个节点跟n&#x2F;2+1个节点一定有交集</strong>, 我们可以向 M(C) 中加入几个大小为 n&#x2F;2 的节点集合, 再保证所有加入的 n&#x2F;2 个节点的集合都有交集, 就可以构建一个扩张的 quorum 集合了.</p>
<p>以本文的场景为例,</p>
<ul>
<li>可以设置 Q’ &#x3D; M(abcd) ∪ {ab, bc, ca}, Q’中任意2个元素都有交集;</li>
<li>也可以是 Q’ &#x3D; M(abcd) ∪ {bc, cd, bd};</li>
<li>但不能是 Q’ &#x3D; M(abcd) ∪ {ab, bc, cd}, 因为 ab 和 cd 没有交集;</li>
</ul>
<p>要找到一个更好的偶节点的 quorum 集合, 一个方法是可以把偶数节点的集群看做是一个奇数节点集群加上一个节点x: (D &#x3D; C \cup {x})</p>
<p>于是偶数节点的 quorum 集合就可以是 M(D) 的一个扩张:</p>
<p>[Q_{even}(D)_x &#x3D; M(D) \cup M(D \setminus {x})]</p>
<p>当然这个x可以随意选择, 例如在abcd的例子中, 如果选x &#x3D; d, 那么 Q’ &#x3D; M(abcd) ∪ {ab, bc, ca}; 如果选x &#x3D; a, 那么 Q’ &#x3D; M(abcd) ∪ {bc, cd, bd}. 这2个4节点 quorum 集合比 M(abcd) 包含更多的 quorum, 因此都可以提供比 M(abcd) 更好的可用性, 在本文开始提出的问题中, 都可以解决本文开头提到的网络割裂的问题.</p>
</li>
</ul>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>看了这几个例子之后, 我们发现, 成员变更的中间状态不需要必须是 majority 的 quorum 集合, 只要满足某些变更的正确性条件, 并包含bc就可以了.</p>
<p>例如, 在变更的中间状态,</p>
<ul>
<li>可以不选 M(abcd) ∪ {ab, bc, ac},</li>
<li>选 {abc, abd, acd, bcd, bc} 也可以,</li>
<li>去掉abc, 选{abd, acd, bcd, bc} 也可以.</li>
</ul>
<p>而且, 似乎那个看起来复杂(实则更简单的) joint consensus 也可以.</p>
<h3 id="成员变更的正确性条件"><a href="#成员变更的正确性条件" class="headerlink" title="成员变更的正确性条件"></a>成员变更的正确性条件</h3><p>我们都用 quorum 集合的方式, 替代节点集合方式来描述系统. 就像 <a href="https://blog.openacid.com/algo/quorum/">多数派读写的少数派实现</a> 中描述的. 例如:</p>
<ul>
<li>3节点 {abc}, 选择 majority 作为 quorum 集合, 则可以定义这个系统是 Q(abc) &#x3D; {ab,bc,ca}</li>
<li>4节点 {abcd}, 选择 majority 作为 quorum 集合, 则定义这个系统是 Q(abcd) &#x3D; {abc,abd,acd,bcd},</li>
<li>4节点 {abcd}, 选择 majority 的一个扩张作为 quorum 集合, 可以被定义为 Q’(abcd) &#x3D; {abc,abd,acd,bcd,ab,bc,ac},</li>
</ul>
<p>要选择一个正确且高效的成员变更算法, 需要满足几个条件. 假设系统要从 Q₁ 变更到 Q₂:</p>
<ul>
<li>提交的变更必须可见, 换句话说, 如果系统中有一个已提交的变更, 未提交的变更必须能被识别出来.</li>
<li>并发的变更只有一个能成功, 因此多个变更进程必须选择一个相同的 Q 作为提交变更的 quorum 集合. 多个进程共识的数据只有 Q₁, 因此变更必须提交到 Q₁ 或 Q₁ 的一个确定的扩张.</li>
<li>变更必须提交到 Q₂ 中的一个 quorum 中.</li>
</ul>
<blockquote>
<p>然鹅, Raft 最初的单步变更算法没有满足上面的第1条, 后来作者做了修正, 我们最后来聊.</p>
</blockquote>
<h3 id="一定要用joint-consensus"><a href="#一定要用joint-consensus" class="headerlink" title="一定要用joint consensus"></a>一定要用joint consensus</h3><p>joint consensus 完全满足上面的正确性保证, 且我们将看到, 它刚好在网络割裂的问题上有很好的表现.</p>
<p>从abc变更到bcd的过程中, joint consensus的中间状态 是通过 M(abc) 和 M(bcd) 的乘积构建的: Q &#x3D; M(abc) x M(bcd); 即, 一个joint quorum 同时包含 M(abc) 的一个 quorum 也同时包含 M(bcd) 的 quorum.</p>
<p>在我们的例子里, M(abc) &#x3D; {ab,bc,ca}, M(bcd) &#x3D; {bc, cd, bd}, 因此:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">M(abc) x M(bcd) = &#123;</span><br><span class="line">    ab ∪ bc,</span><br><span class="line">    ab ∪ cd,</span><br><span class="line">    ab ∪ bd,</span><br><span class="line">    bc ∪ bc,</span><br><span class="line">    bc ∪ cd,</span><br><span class="line">    bc ∪ bd,</span><br><span class="line">    ac ∪ bc,</span><br><span class="line">    ac ∪ cd,</span><br><span class="line">    ac ∪ bd,</span><br><span class="line">&#125; = &#123;</span><br><span class="line">    abc,</span><br><span class="line">    abcd,</span><br><span class="line">    abd,</span><br><span class="line">    acd,</span><br><span class="line">    bc,</span><br><span class="line">    bcd,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>刚好就是M(abcd) ∪ {bc}</strong></p>
<p>太优秀了有木有!!!</p>
<p>容易看出, joint consensus 不仅满足了成员变更的正确性条件, 而且刚好满足了我们的所有要求:</p>
<ul>
<li>容忍1个节点宕机;</li>
<li>一定包含{bc}, 容忍<code>ad | bc</code>的网络隔离.</li>
<li>另外, 整个变更过程, 不论有没有切换leader, 都可以通过2条日志的commit来完成.</li>
</ul>
<p>太优秀了有木有!!!</p>
<p>太优秀了有木有!!!</p>
<h3 id="Raft-单步变更的bug"><a href="#Raft-单步变更的bug" class="headerlink" title="Raft 单步变更的bug"></a>Raft 单步变更的bug</h3><p>不仅 Raft 的单步变更无法更详细的指定偶数节点集群的 quorum 集合, 更严重的是, 它在最初提出时是有 bug 的. 看似巧妙实则幼稚的单步变更, 在修正后就跟 joint consensus 相比没有任何优势了.</p>
<p>单步变更在 leader 切换和成员变更同时进行时会出现bug. 这个 bug 在2015年就已经被作者指出了:</p>
<blockquote>
<p>Unfortunately, I need to announce a bug in the dissertation version of membership changes (the single-server changes, not joint consensus). The bug is potentially severe, but the fix I’m proposing is easy to implement.</p>
</blockquote>
<p>以下是一个单步变更出 bug 的例子, 原成员是4节点abcd, 2个进程分别要加入u和加入v, 如果中间出现换主, 就会丢失一个已提交的变更:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C₀ = &#123;a, b, c, d&#125;</span><br><span class="line">Cᵤ = C₁ ∪ &#123;u&#125;</span><br><span class="line">Cᵥ = C₁ ∪ &#123;v&#125;</span><br><span class="line"></span><br><span class="line">Lᵢ: Leader in term `i`</span><br><span class="line">Fᵢ: Follower in term `i`</span><br><span class="line">☒ : crash</span><br><span class="line"></span><br><span class="line">    |</span><br><span class="line"> u  |         Cᵤ                  F₂  Cᵤ</span><br><span class="line">--- | ----------------------------------</span><br><span class="line"> a  | C₀  L₀  Cᵤ  ☒               L₂  Cᵤ</span><br><span class="line"> b  | C₀  F₀          F₁          F₂  Cᵤ</span><br><span class="line"> c  | C₀  F₀          F₁  Cᵥ          Cᵤ</span><br><span class="line"> d  | C₀              L₁  Cᵥ  ☒       Cᵤ</span><br><span class="line">--- | ----------------------------------</span><br><span class="line"> v  |                     Cᵥ                  time</span><br><span class="line">    +--------------------------------------------&gt;</span><br><span class="line">          t₁  t₂  t₃  t₄  t₅  t₆  t₇  t₈</span><br></pre></td></tr></table></figure>

<ul>
<li>t₁: <code>abcd</code> 4节点在 term 0 选出leader&#x3D;<code>a</code>, 和2个follower <code>b</code>, <code>c</code>;</li>
<li>t₂: <code>a</code> 广播一个变更日志<code>Cᵤ</code>, 使用新配置<code>Cᵤ</code>, 只发送到<code>a</code>和<code>u</code>, 未成功提交;</li>
<li>t₃: <code>a</code> 宕机</li>
<li>t₄: <code>d</code> 在 term 1 被选为leader, 2个follower是<code>b</code>,<code>c</code>;</li>
<li>t₅: <code>d</code> 广播另一个变更日志<code>Cᵥ</code>, 使用新配置<code>Cᵥ</code>, 成功提交到<code>c</code>,<code>d</code>,<code>v</code>;</li>
<li>t₆: <code>d</code> 宕机</li>
<li>t₇: <code>a</code> 在term 2 重新选为leader, 通过它本地看到的新配置<code>Cᵤ</code>, 和2个follower <code>u</code>, <code>b</code>;</li>
<li>t₈: <code>a</code> 同步本地的日志给所有人, 造成已提交的<code>Cᵥ</code>丢失.</li>
</ul>
<p>作者给出了这个问题的修正方法, 修正步骤很简单, 跟Raft的commit条件如出一辙: <strong>新leader必须提交一条自己的term的日志, 才允许接变更日志</strong>:</p>
<blockquote>
<p>The solution I’m proposing is exactly like the dissertation describes except that a leader may not append a new configuration entry until it has committed an entry from its current term.</p>
</blockquote>
<p>在上面这个例子中, 对应的就是L₁必须提交一条NoOp的日志: 以便L₂能发现自己的日志是旧的, 阻止L₂选为leader.</p>
<p>但是, 你品, 你细品…</p>
<p>品完后笔者一拍大腿: 这个修正实际上就是将单步变更升级成了joint consensus, 本质上都变成了: 一条变更在旧的配置中必须通过quorum互斥, 只能有1个变更被认为是committed. 单步变更需要一条业务日志或一条NoOp日志完成这件事情, joint consensus直接完成了这件事情:</p>
<p><strong>要保证正确性, 每次单步变更需要2次日志提交</strong>.</p>
<hr>
<p>单步变更之所以被提出, 起初是为了简化一些场景, 但实际上正确的单步变更没有任何简化, 反而更复杂了:</p>
<p>例如在 abc 到 bcd 的变更中, 使用单步变更算法, 需要2~4条日志, 如果用joint consensus, 只需要2条日志.</p>
<p>有人会说, 单步变更最少也只需要2条日志, 并且是大多数情况下都只需要2条日志. 但代码不是拼概率的, 任何一个小概率分支, 都必须被代码逻辑覆盖到. 即使这个分支被执行的几率是有万分之一. 由此导致的结果就是, 为了保证正确性, 单步变更必须使用跟joint consensus 几乎同样复杂的逻辑, 实现2步变更的逻辑, 而执行效率上, 没有任何优势.</p>
<p>Raft 作为 paxos 一个实现 (谁跟我杠paxos跟Raft不一样我跟谁急. 它的term, log seq对应ballot num, commit 对应 accept 和 learn, 集群变更是一个特殊的paxos 实例), 是理论到工程实现的一个漂亮的桥梁, 也是因为Raft实现的太漂亮, 导致一个设计失误, 也被广泛传播了.</p>
<p>东旭也表示 TiDB 下一步就会将单步变更升级为joint consensus, 彻底解决单步变更带来的可用性问题以及工程实现上的麻烦.</p>
<p>顺祝PingCAP的好基友们新的一年里事业迎风破浪, :DDD</p>
<p>本文链接: <a href="https://blog.openacid.com/distributed/raft-bug/">https://blog.openacid.com/distributed/raft-bug/</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>转载</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>【转载】单节点变更的问题和工程实践</title>
    <url>/2021/12/08/%E5%88%86%E5%B8%83%E5%BC%8F/raft/%E8%BD%AC%E8%BD%BD/14-2%5B%E8%BD%AC%E8%BD%BD%5D%E5%8D%95%E8%8A%82%E7%82%B9%E5%8F%98%E6%9B%B4%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<h3 id="单步成员变更的问题"><a href="#单步成员变更的问题" class="headerlink" title="单步成员变更的问题"></a>单步成员变更的问题</h3><h4 id="正确性问题"><a href="#正确性问题" class="headerlink" title="正确性问题"></a><font color=red>正确性问题</font></h4><p>Raft单步变更过程中如果发生Leader切换会出现正确性问题，可能导致已经提交的日志又被覆盖。Raft作者（Diego Ongaro）早在2015年就发现了这个问题，并且在Raft-dev详细的说明了这个问题：<a href="https://link.zhihu.com/?target=https://groups.google.com/g/raft-dev/c/t4xj6dJTP6E%22%20%5Ct%20%22_blank">https://groups.google.com/g/raft-dev/c/t4xj6dJTP6E</a>。</p>
<p>下面是一个Raft单步变更出问题的例子, 初始成员配置是a b c d这4节点，节点u和v要加入集群, 如果中间出现Leader切换, 就会丢失已提交的日志：</p>
<span id="more"></span>
<p><img src="/img/03RAFT%E6%80%BB%E7%BB%93_%E6%88%90%E5%91%98%E5%8F%98%E6%9B%B4(%E5%A4%A7%E8%AE%BA%E6%96%87)/v2-f40dd27f1d49bd9c1c00ddb939b7e534_720w.jpg" alt="img"></p>
<ul>
<li>t₀：节点a b c d的成员配置为C₀；</li>
<li>t₁：节点a b c d在Term 0选出a为Leader，b和c为Follower；</li>
<li>t₂：节点a同步成员变更日志Cᵤ，只同步到a和u，未成功提交；</li>
<li>t₃：节点a宕机；</li>
<li>t₄：节点d在Term 1被选为Leader，b和c为Follower；</li>
<li>t₅：节点d同步成员变更日志Cᵥ，同步到c、d、v，成功提交；</li>
<li>t₆：节点d同步普通日志E，同步到c、d、v，成功提交；</li>
<li>t₇：节点d宕机；</li>
<li>t₈：节点a在Term 2重新选为Leader，u和b为Follower;</li>
<li>t₉：节点a同步本地的日志Cᵤ给所有人，造成已提交的Cᵥ和E丢失。</li>
</ul>
<p>产生该原因，是上一任Leader的成员变更日志还没有同步到多数派就宕机了，新Leader一上任就进行成员变更，使用新的成员配置提交日志，之前上一任Leader重新上任之后可能形成另外一个多数派集合，产生脑裂，将已提交的日志覆盖，造成数据丢失。</p>
<p>Raft作者在发现这个问题之后，也给出了修复方法。修复方法很简单, 跟Raft的日志Commit条件类似：新任Leader必须在当前Term提交一条no-op日志之后，才允许同步成员变更日志。也即Leader在当前Term还未提交日志之前，不允许同步成员变更日志。</p>
<p><font color=red>对应上面这个例子，就是 d 当选Leader后必须先提交一条no-op日志，该no-op只有复制到C₀中Majority之后才能提交。然后L₁才能开始同步Cᵥ和E，这样，当a竞选L₂时，因日志不够新，所以当不了新Leader。</font></p>
<h4 id="可用性问题"><a href="#可用性问题" class="headerlink" title="可用性问题"></a>可用性问题</h4><p>单步成员变更每次只能增加或者减少一个成员，在做成员替换的时候需要分两次变更，第一次变更先将新成员加入进来，第二次变更再将老成员删除，中间如果如果网络分区，有可能会导致服务不可用。</p>
<p>比如原集群包含3个节点abc，现在要将a替换为d，则使用单节点变更算法的话，需要两次单节点变更：abc -&gt; abcd -&gt; bcd。</p>
<p>考虑a、b、c三个成员部署在三个机房，d跟a在同一个机房。则当集群为abcd时，有可能在出现二分的网络分区(ad | bc)导致整个集群不可用。</p>
<p>如果使用联合共识算法的话，则不存在正确性和可用性问题。</p>
<hr>
<h3 id="单步成员变更的工程实践"><a href="#单步成员变更的工程实践" class="headerlink" title="单步成员变更的工程实践"></a>单步成员变更的工程实践</h3><h4 id="单步成员变更日志使用什么配置"><a href="#单步成员变更日志使用什么配置" class="headerlink" title="单步成员变更日志使用什么配置"></a>单步成员变更日志使用什么配置</h4><p>单步成员变更日志无论使用新成员配置Cnew还是老成员配置Cold都不会破坏Cnew与Cold的多数派至少有一个节点相交，因此单步成员变更日志既可以使用新成员配置Cnew也可以使用老成员配置Cold，两种方式各有利弊。</p>
<table>
<thead>
<tr>
<th></th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>单步成员变更日志使用Cold配置</td>
<td>可避免单步成员变更的正确性问题，可以省略掉Leader上任后的no-op日志。增加成员时可能只需要更小的多数派集合</td>
<td>减少成员时可能需要更大的多数派集合</td>
</tr>
<tr>
<td>单步成员变更日志使用Cnew配置</td>
<td>需要处理单步成员变更的正确性问题，减少成员时可能只需要更小的多数派集合</td>
<td>增加成员时可能需要更大的多数派集合</td>
</tr>
</tbody></table>
<p>Raft论文中单步成员变更日志使用新成员配置Cnew，etcd中单步成员变更日志使用老成员配置Cold。</p>
<h4 id="单步成员变更日志什么时候生效"><a href="#单步成员变更日志什么时候生效" class="headerlink" title="单步成员变更日志什么时候生效"></a>单步成员变更日志什么时候生效</h4><table>
<thead>
<tr>
<th></th>
<th align="left">Leader</th>
<th>Follower</th>
</tr>
</thead>
<tbody><tr>
<td>单步成员变更，使用新成员配置</td>
<td align="left">开始同步成员变更日志之前</td>
<td>成员变更日志持久化完成后</td>
</tr>
<tr>
<td>单步成员变更，使用老成员配置</td>
<td align="left">理论上只需要在下一次成员变更开始之前生效，实际一般在成员变更日志提交后就生效</td>
<td>理论上只需要在下一次成员变更开始之前生效，实际一般在成员变更日志提交后就生效</td>
</tr>
</tbody></table>
<p>如果成员变更日志使用老成员配置，理论上只需要在下一次成员变更开始之前生效即可，但实际为了让新加入的节点尽快开始服务，一般在成员变更日志提交后就生效。</p>
<p><font color=red>Raft论文中单步成员变更日志使用新成员配置Cnew，本地持久化完成就生效；etcd中单步成员变更日志使用老成员配置Cold，提交后再生效。</font></p>
<hr>
<p>本文链接:<a href="https://zhuanlan.zhihu.com/p/359206808">https://zhuanlan.zhihu.com/p/359206808</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>raft</category>
        <category>转载</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title>000总结</title>
    <url>/2021/12/11/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/000%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h4><p>系统由状态，以及一些能够转换该状态的操作组成。随着系统的运行，它通过一些操作历史（history of operations，个人理解就是一系列操作，以及每个操作后的状态，以及操作的调起和完成时间），从一种状态转换到另一种状态。</p>
<span id="more"></span>
<h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p>进程是一个逻辑上的单线程程序，执行计算和调起操作。</p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>操作会引起系统从一个状态到另一个状态的转换。例如，一个单变量系统可能有“read”和“write”这样的操作；计数器可能具有诸如递增、递减和读取之类的操作；SQL存储可能具有诸如selects和updates之类的操作。</p>
<h4 id="调起时间和完成时间"><a href="#调起时间和完成时间" class="headerlink" title="调起时间和完成时间"></a>调起时间和完成时间</h4><p>操作是需要时间的。比如在多线程系统中，一个函数调用就是一种操作；在分布式系统中，一个操作可能就是发送请求到服务节点，然后收到一个响应；</p>
<p>每个操作都有一个调起时间，完成时有一个更大的完成时间；这两个时间都由假想的、完全同步的全球可访问的时钟提供。我们认为这些时钟提供的是<font color=red>实际时间顺序（real-time order）的时钟</font>；与之相反的就是只跟踪因果顺序的时钟；</p>
<h4 id="并发"><a href="#并发" class="headerlink" title="并发"></a>并发</h4><p>因为操作需要耗时，所以两个并发的操作在时间上就有可能重叠。比如操作A和B，A先开始，然后B开始，然后A结束，之后B结束。只要他们在时间上有重叠，那我们说操作A和B是并发的。</p>
<h4 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h4><p>网络分区是不可避免的，消息总会延迟、丢弃、重发或者乱序。所以我们希望软件能保持某种<font color=red>直观的正确性。</font>但是如何定义正确呢？</p>
<p>给定一些与<font color=red>操作和状态相关的“规则”</font>，针对系统的操作历史应该始终遵循这些规则。我们称这些规则为“一致性模型”。（一致性模型，就是规则的集合，就是符合规则的操作历史的集合。什么是正确？符合规则就是正确！）</p>
<p>正式的说法是，<font color=red>一致性模型是所有允许的操作历史的集合。</font>如果运行一个程序，执行了某种操作后得到了某种结果，该操作以及结果在允许的操作历史集合中，那就说这种操作是一致的（满足这种一致性模型的）。某个操作历史不在一致性模型中，那就是这种操作历史是不一致的（不满足这种一致性模型）。如果每个可能的执行都在允许的范围内，则说系统满足这种一致性模型。我们当然是希望真实系统满足“直观正确的”一致性模型，这样我们就可以编写可预测的程序。<font color=red>比如当我们说一个操作历史不符合序列化一致性时，就表示该操作历史不是序列性模型的历史集合中的成员；</font></p>
<p>正式的说法就是：<font color=red>我们把更严格，集合更小的一致性模型成为更强的一致性模型；而集合更大，要求更宽松的一致性模型称为较弱的一致性模型；</font></p>
<h4 id="total-order"><a href="#total-order" class="headerlink" title="total order !!!"></a>total order !!!</h4><p>所有操作都能按照某种顺序排序。这个顺序不一定就是实际时间（real-time）上的顺序，只要有顺序就行。</p>
<blockquote>
<p>假设A是一个集合 {1,2,3} ；R是集合A上的关系，例如{&lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;1,2&gt;,&lt;1,3&gt;,&lt;2,3&gt;}</p>
<ul>
<li>自反性：任取一个A中的元素x，如果都有&lt;x,x&gt;在R中，那么R是自反的。</li>
<li>对称性：任取一个A中的元素x,y，如果&lt;x,y&gt; 在关系R上,那么&lt;y,x&gt; 也在关系R上，那么R是对称的。</li>
<li>反对称性：任取一个A中的元素x,y(x!&#x3D;y)，如果&lt;x,y&gt; 在关系R上,那么&lt;y,x&gt; 不在关系R上，那么R是反对称的。</li>
<li>传递性：任取一个A中的元素x,y,z，如果&lt;x,y&gt;,&lt;y,z&gt; 在关系R上，那么 &lt;x,z&gt; 也在关系R上，那么R是对称的。</li>
</ul>
<p>偏序： 设R是非空集合A上的关系，如果R是自反的，反对称的，和传递的，则称R是A上的偏序关系。</p>
<p>全序：如果R是A上的偏序关系，那么对于任意的A集合上的 x,y，都有 x &lt;&#x3D; y，或者 y &lt;&#x3D; x，二者必居其一，那么则称R是A上的全序关系。</p>
<p>偏序关系就是指存在“序”这个概念，满足自反性、反对称性和传递性。但是并不是任何两个元素之间都有这个序的先后关系。比如集合之间的包含关系，就是一个偏序关系。但并不是任何两个集合之间都存在包含关系。</p>
<p align=right>以上内容来自于：https://www.zhihu.com/question/36758436/answer/680927456</p>
</blockquote>
<p>因此，total order应该就是说：操作历史集合中的操作，都是可以排序的，而且满足自反、反对称和传递的特点。</p>
<hr>
<h4 id="real-time"><a href="#real-time" class="headerlink" title="real-time !!!"></a>real-time !!!</h4><p>上面介绍调起时间和完成时间时：每个操作都有一个调起时间，完成时有一个更大的完成时间；这两个时间都由假想的、完全同步的全球可访问的时钟提供。我们认为这些时钟提供的是<font color=red>实际时间顺序（real-time order）的时钟</font>；与之相反的就是只跟踪因果顺序的时钟；</p>
<p>一种一致性模型如果需要满足real time的要求，那它肯定也满足total order。</p>
<hr>
<h3 id="一致性的强弱"><a href="#一致性的强弱" class="headerlink" title="一致性的强弱"></a>一致性的强弱</h3><p>下面的图（改编自 <a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Bailis, Davidson, Fekete et al</a> 和<a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti &amp; Vukolic</a>的论文）展示了一致性模型之间的关系。其中的箭头表示了一致性模型之间的包含关系。比如严格序列化（Strict serializable）一致性意味着它既满足序列化（serializability）一致性，也满足线性（linearizability）一致性。图中的颜色表示在异步网络环境下的可用性。</p>
<p><img src="/img/000%E6%80%BB%E7%BB%93/image-20220309083251705.png" alt="image-20220309083251705"></p>
<p>上图展示了按层次结构排列的一致性模型。<font color=red>最强模型是严格序列化（Strict serializable），它统一了两个不相交的一致性模型族：多对象事务上的一致性模型族和单对象操作的一致性模型族。</font>当我们说模型x意味着y时，我们的意思是，对于x中约定的操作历史，也肯定满足y的规则约束，所以x比y“强”。</p>
<p>在多对象这一边，强序列化意味着序列化；序列化又意味着可重复读（repeatable read）和快照隔离（snapshot isolation）；而可重复读和快照隔离又都意味着单调原子视图（monotonic atomic view）。可重复读还意味着cursor stability；cursor stability和单调原子视图又同时意味着read committed；read committed意味着read uncommitted；</p>
<p>在单对象这一边，强序列化意味着线性一致性；线性一致性又意味着顺序（sequential）一致性；顺序一致性意味着因果（causal）一致性；因果一致性意味着writes follow reads和PRAM；PRAM意味着单调读、单调写和read your writes；</p>
<p>可用性方面：</p>
<ul>
<li>粉红色的Unavailable：表示发生网络错误时是不可用的。一些或者所有节点必须暂停操作，以便保证安全性；</li>
<li>橙色的Sticky可用：表示只要client只与相同节点通信，那在无故障节点上是可用的；</li>
<li>蓝色的完全可用：表示即使网络瘫痪了，在非故障节点上也是可用的；</li>
</ul>
<hr>
<h3 id="一致性带来的成本"><a href="#一致性带来的成本" class="headerlink" title="一致性带来的成本"></a>一致性带来的成本</h3><p><a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf">CAP定理</a>表示对于一致性，可用性和分区容错性三者而言，任何系统最多能保证其中两个属性。虽然Eric Brewer的CAP是用非正式术语描述的，但是CAP定理有着非常精确的定义：</p>
<ol>
<li><font color=red>一致性这里就是线性一致性</font>，所以<font color=red>CAP定理适用于所有的线性一致性系统；</font></li>
<li>可用性表示对于非失败节点发起的请求必须能成功完成。因为网络分区的时间可能持续任意长的时间，这就表示节点不能简单地将响应延迟到分区修复之后；</li>
<li>分区容错性意味着允许发生网络分区。在网络正常的情况下提供线性一致性和可用性是很容易的，但是<font color=red>当网络不可靠时，同时提供这两种特性是不可能的。</font>因此如果网络不是完全可靠的（实际上它就是不可靠的），你就不能选择CA，所以构建在商用硬件之上的实际分布式系统中，最多可以保证AP或者CP。</li>
</ol>
<p>你可能会有这样的疑问：线性一致性不是一致性模型的唯一模型，我可以通过提供顺序一致性、串行一致性或者快照隔离，来绕过CAP定理。但是有其他证据表明，你也无法构建具有顺序一致性、序列一致性、可重复读、分区隔离，cursor stability等，或是更强的一致性模型的完全可用系统。在Peter Bailis的论文<a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Highly Available Transactions paper</a>中指出，上图中基于红色区域的一致性模型的系统时不可能完全可用的。</p>
<p>更强的一致性模型意味着更多的协调（更多的消息交互），以确保这些操作按照正确的顺序进行。这不仅造成了<font color=red>可用性较低，而且还暴露出更高的延迟。</font></p>
<p>我们采用的是混合的数据存储系统或者数据库，融合不同的一致性模型来实现冗余、可用性、性能和安全性目标。<font color=red>在保证可用性和性能方面，尽可能采用弱一致性模型，如果算法需要有严格的操作顺序要求，则采用强一致性模型。</font></p>
<hr>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><h4 id="Strict-Serializability-严格序列化"><a href="#Strict-Serializability-严格序列化" class="headerlink" title="Strict Serializability     严格序列化"></a>Strict Serializability     严格序列化</h4><p>不那么正式地说法，严格序列化意味着操作以跟这些操作的实时顺序（real-time ordering，即由上面提到的全局时钟保证的）一致的顺序进行执行。比如，如果操作A的完成时间比B的开始时间要早，那在序列化的顺序中，A要出现B的前面。</p>
<p>特点：</p>
<ul>
<li>total order和real time：具有实时顺序的序列化系统。</li>
<li>“事务性”模型：操作（通常称为“事务”）包括几个按顺序执行的子操作。严格序列化保证操作（事务）以原子方式进行：即事务的子操作不会与其他事务的子操作交叉。</li>
<li>“多对象”模型：操作针对的是系统中的多个对象。</li>
<li>在网络分区的情况下，不是完全或粘性（sticky）可用的；即网络分区时，部分或所有节点将无法继续执行操作。</li>
</ul>
<p>严格序列化意味着序列化（serializability）和线性化（linearizability）。可以认为严格序列化，是在保证事务性多对象操作的序列化顺序的基础上，增加了线性化的实时顺序约束；或者可以这样认为，严格序列化的数据库就是把整个数据库视为一个对象的线性化系统。</p>
<hr>
<h4 id="单对象操作一致性"><a href="#单对象操作一致性" class="headerlink" title="单对象操作一致性"></a>单对象操作一致性</h4><h5 id="Linearizability-线性一致性"><a href="#Linearizability-线性一致性" class="headerlink" title="Linearizability 线性一致性"></a>Linearizability 线性一致性</h5><p>假设有一个所有进程都可以访问的全局状态，针对这个全局状态的操作都是<font color=red>原子</font>的，而不是相互交织的，这就得到了线性一致性的模型：每个操作都会在其调起和完成之间的某个时刻<strong>原子地</strong>生效。</p>
<p>线性一致性有个很重要的一个约束：一旦一个操作完成了，那所有人必须都能看到该操作的结果（或者是更加靠后的结果）。因为操作起作用的时间，肯定不会晚于其结束时间，而该操作之后（即调起时间晚于该操作完成时间的操作）操作，它发生作用的时间肯定要晚于其调起的时间，进而肯定要晚于前一个操作的结束时间。举例而言：如果一个操作成功写入了<code>b</code>，那该写操作之后的读操作，必须能读到<code>b</code>（或者如果同时有其他写操作的话，能读到更新的值）。</p>
<p>线性一致性在时间上的约束，也保证所有修改后的结果，都能在操作完成之后的其他读操作是可见的。所以，线性一致性不允许出现读取到旧数据的情况；也不允许出现非单调读的现象——即先读到新值，然后又读到了旧值。</p>
<p>特点：</p>
<ul>
<li>最强的单对象一致性模型之一；</li>
<li>满足<font color=red>total order和real time</font>，即每个操作都是<font color=red>原子的，按照这些操作的实时顺序发生的</font>；</li>
<li>在网络分区的情况下，不是完全或粘性（sticky）可用的；即网络分区时，部分或所有节点将无法继续执行操作。</li>
</ul>
<hr>
<h5 id="sequential-顺序一致性"><a href="#sequential-顺序一致性" class="headerlink" title="sequential 顺序一致性"></a>sequential 顺序一致性</h5><p>直白的说，允许操作在时间上不那么严格，即操作可以在调用之前或完成之后生效，但保留了进程内部的操作必须按该进程规定的顺序进行的约束，这就是比线性一致性稍弱的一致性模型：顺序一致性。</p>
<p>特点：</p>
<ul>
<li>total order，并且事件顺序与每个进程内的操作顺序一致（即保证总顺序中，<font color=red>单个进程内的操作还是按real-time的顺序进行的</font>）。</li>
<li>在有网络分区的情况下不是完全可用或粘性可用的；即网络发生分区时，若干或者所有节点都无法再继续取得进展；</li>
<li>可以读取过期的状态，但一旦进程A观察到进程B的某些操作，它就不可能再观察到B之前的状态。这种约束与total order属性相结合，使得顺序一致性也是一种强一致性模型。</li>
</ul>
<hr>
<h5 id="Causal-Consistency-因果一致性"><a href="#Causal-Consistency-因果一致性" class="headerlink" title="Causal Consistency   因果一致性"></a>Causal Consistency   因果一致性</h5><p>不强制一个进程内的所有操作都是按序执行的，而只要求具有因果依赖关系的操作必须按序。这种一致性模型要比约束同一进程内所有操作必须按序的一致性稍弱，同一进程内没有因果依赖的操作可以以任意顺序出现。</p>
<p>特点：</p>
<ul>
<li><p>因果相关的操作应该<font color=red>以相同的顺序被所有进程观察到</font>，但是每个进程对于因果不相关的操作的顺序可能会有不同的看法。</p>
</li>
<li><p><font color=red>粘性可用</font>：发生网络分区时，只要CLIENT始终访问的是同一个无故障节点，那该节点上肯定会有所进展。</p>
</li>
<li><p>不是total order。</p>
</li>
<li><p>因果一致性首次来源于Lamport对<code>happens-before</code>关系的定义，它通过将一个操作与同一进程之前的操作，以及其他进程上的相关操作（通过消息交互使得操作的结果可见）关联起来，捕捉到了潜在因果关系的概念。</p>
</li>
</ul>
<hr>
<h5 id="PRAM-流水线随机访问存储系统"><a href="#PRAM-流水线随机访问存储系统" class="headerlink" title="PRAM    流水线随机访问存储系统"></a>PRAM    流水线随机访问存储系统</h5><p>PRAM（Pipeline Random Access Memory）这个概念来源于Lipton &amp; Sandberg的论文《PRAM: A Scalable Shared Memory》，这种一致性模型的约束是：<font color=red>由单个进程进行的多个写操作，会按照该进程执行它们的顺序被其他所有进程观察到；然而多个进程的写操作则可能以不同的顺序被外部观察到。</font></p>
<p>特点：</p>
<ul>
<li>PRAM实际上等同于read your writes，单调写和单调读；</li>
<li>粘性可用：在网络分区的情况下，只要client始终访问的是同一个节点，那所有进程都可以取得进展。</li>
<li>无total order;</li>
</ul>
<hr>
<h5 id="Read-Your-Writes-读你的写"><a href="#Read-Your-Writes-读你的写" class="headerlink" title="Read Your Writes     读你的写"></a>Read Your Writes     读你的写</h5><p>read your writes，或者叫read my writes保证这样的规则：如果一个进程执行了写操作w，然后该进程又执行了读操作r，那r必须能够看到w的结果；</p>
<p>注意：read your writes不是应用于不同进程上的规则。比如它不保证，如果进程1写成功了，那进程2一定能观察到写操作的结果；</p>
<p>特点：</p>
<ul>
<li>粘性可用：如果发生了网络分区，那只要CLIENT始终访问的是同一个节点，那该节点依然可以取得进展；</li>
<li>Viotti 和 Vukolić在论文中定义了read your writes：如果对于特定进程而言，其执行的写操作在读操作之前完成，那写操作的结果必须是对该读操作而言是可见的。换句话说，会话顺序（仅限于相同进程中的先写后读），必须是可见顺序的子集；</li>
</ul>
<hr>
<h5 id="Writes-Follow-Reads-读后写"><a href="#Writes-Follow-Reads-读后写" class="headerlink" title="Writes Follow Reads     读后写"></a>Writes Follow Reads     读后写</h5><p>writes follow reads：也叫做会话因果关系，保证这样的规则：如果一个进程读到了v，v来自于写操作w1，然后该进程执行写操作w2，那w2必须在w1之后可见。Once you’ve read something, you can’t change that read’s past.</p>
<p>writes follow reads是完全可用的，即使发生网络分区，那每个节点依然可以取得进展；</p>
<hr>
<h5 id="Monotonic-Reads-单调读"><a href="#Monotonic-Reads-单调读" class="headerlink" title="Monotonic Reads 单调读"></a>Monotonic Reads 单调读</h5><p>单调读的规则：如果一个进程先执行读操作r1，然后是r2，那r2读取到的值，不能是r1所反映出的写操作之前的状态；</p>
<p>直白点说就是读操作不能倒退：对于写操作w，以及读操作r1和r2，r1在r2之前执行，并且都是由相同的进程执行的，那如果r1见到了w的结果，那r2也必须能见到w的结果；</p>
<p>特点：</p>
<ul>
<li>单调读的规则适用于相同进程的读操作，而不适用于不同进程的操作；</li>
<li>完全可用的：即使发生了网络分区，那所有节点还是可以取得进展；</li>
</ul>
<hr>
<h5 id="Monotonic-Writes-单调写"><a href="#Monotonic-Writes-单调写" class="headerlink" title="Monotonic Writes 单调写"></a>Monotonic Writes 单调写</h5><p>单调写保证这样的场景：一个进程先进行了写操作w1，然后是写操作w2，那其他所有进程都是先观察到w1，然后是w2；</p>
<p>特点：</p>
<ul>
<li>只适用于相同进程的写操作，而非不同进程的写操作；</li>
<li>完全可用：即使发生了网络分区，所有节点都可以取得进展；</li>
</ul>
<hr>
<h4 id="多对象事务一致性"><a href="#多对象事务一致性" class="headerlink" title="多对象事务一致性"></a>多对象事务一致性</h4><p>下面的一致性模型都是“事务”一致性模型：操作（通常称为“事务”）包括按顺序执行的几个基本子操作。也都是“多对象”一致性模型：操作可以作用于系统中的多个对象上。</p>
<p><a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999规范</a>定义的隔离异常现象有：</p>
<blockquote>
<p>P1（“脏读”）：SQL事务T1修改了某行。然后，SQL事务T2在T1执行提交之前读取该行。如果T1随后执行回滚，T2将读取到从未提交过的行，可以认为该行从未存在过。</p>
<p>P2（“不可重复读取”）：SQL事务T1读取一行。然后，SQL事务T2修改或删除该行并执行提交。如果T1随后尝试重新读取该行，它可能会读到修改后的值或发现该行已被删除。</p>
<p>P3（“幻读”）：事务T1读取满足某些搜索条件的行。事务T2执行SQL语句，这些语句会生成满足事务T1使用的搜索条件的一行或多行。如果事务T1随后以相同的搜索条件重复再次读取，它将获得不同的行集合。</p>
</blockquote>
<p>此外，还有脏写：</p>
<blockquote>
<p>脏写：假设T1修改了x，而且T2在T1提交或者回滚之前也修改了x，那如果T1或T1回滚的话，x的最终的值是未知的。</p>
<p>Dirty Write—Suppose T1 modifies x and T2 further modifies x before T1 commits or<br>aborts. If either T1 or T2 aborts, it is unclear what the real value of x should be.</p>
</blockquote>
<hr>
<h5 id="Serializability-序列化"><a href="#Serializability-序列化" class="headerlink" title="Serializability     序列化"></a>Serializability     序列化</h5><p><font color=red>如果操作历史相当于以某种单一原子顺序发生的，但不约束调用和完成时间，这就是所谓的序列化。</font>这种一致性可能比你预期的强很多，也弱很多。”弱”体现在，它对real-time没有限制；”强”体现在，它排除了很多操作历史，因为它需要有<font color=red>线性顺序：total order</font>：</p>
<ul>
<li>它在real-time上并无约束，即使在单进程上也没有。如果进程A完成了写操作w，然后进程B开始读操作r，那r不一定读到w的结果。</li>
<li>它对total order有强约束，但它允许乱序。比如，一个序列化的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</li>
</ul>
<p>特点：</p>
<ul>
<li>“事务”一致性模型：序列化保证事务以原子的方式进行：事务的子操作不会与其他事务的子操作交错执行。</li>
<li>“多对象”一致性模型：实际上，序列化不仅作用于事务中涉及的特定对象，而且可以把整个系统当成一个整体—-操作可以作用于谓词之上（<em>比如where</em>），比如所有的cat。</li>
<li>序列化无法达到完全或者粘性可用。如果发生了网络分区，那部分或所有节点将无法继续执行操作。</li>
<li>序列化意味着可重复读，快照隔离等。</li>
</ul>
<p>隔离级别在SERIALIZABLE时，并行执行的事务保证是串行化的。串行执行就是下一个事务在上一个事务执行完成之后才能执行。如果用禁止的异常情况来定义其隔离级别：序列化不会出现P3（幻读）现象：</p>
<blockquote>
<p>P3（“幻读”）：事务T1读取满足某些搜索条件的行。事务T2执行SQL语句，这些语句会生成满足事务T1使用的搜索条件的一行或多行。如果事务T1随后以相同的搜索条件重复再次读取，它将获得不同的行集合。</p>
</blockquote>
<hr>
<h5 id="Repeatable-Read"><a href="#Repeatable-Read" class="headerlink" title="Repeatable Read"></a>Repeatable Read</h5><p>可重复读类似于序列化，但是与之不同的是，它允许幻读：事务T1使用谓词进行读取，比如“所有名为Dikembe”的人员集合，另一个事务T2在T1提交之前，新增了名为”Dikembe”的人员信息。虽然单个对象在读取时是稳定的，但谓词本身可能不是。</p>
<p>特点：</p>
<ul>
<li>在网络分区的场景下不是完全可用；在存在网络分区的情况下，一些或所有节点可能无法取得进展。</li>
<li>可重复读取意味着<a href="https://jepsen.io/consistency/models/cursor-stability">游标稳定性</a>，<a href="https://jepsen.io/consistency/models/read-committed">读已提交</a>等。</li>
<li>没有real-time的约束。如果进程A完成了写操作w，然后进程B开始读操作r，r不一定能观察到w。</li>
<li>也不需要保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</li>
<li>像序列化一样，可重复读允许重排序。比如，一个可重复读的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</li>
</ul>
<p>可重复读不会出现P2（不可重复读取）现象：：</p>
<blockquote>
<p>P2（“不可重复读取”）：SQL事务T1读取一行。然后，SQL事务T2修改或删除该行并执行提交。如果T1随后尝试重新读取该行，它可能会读到修改后的值或发现该行已被删除。</p>
</blockquote>
<hr>
<h5 id="Snapshot-Isolation-快照隔离"><a href="#Snapshot-Isolation-快照隔离" class="headerlink" title="Snapshot Isolation     快照隔离"></a>Snapshot Isolation     快照隔离</h5><p>在快照隔离的系统中，每个事务像是在独立且一致的数据库快照上进行操作。在提交之前，其更改仅对本事务可见，提交后，该事务的所有修改对其之后开始的事务才是原子可见的。如果事务T1修改了对象x，而另一个事务T2在T1的快照开始之后、T1提交之前提交了对<em>x</em>的写入，则T1必须中止。</p>
<p>特点：</p>
<ul>
<li>不能完全可用；存在网络分区的情况下，一些或所有节点可能无法取得进展。</li>
<li>快照隔离意味着读已提交。但是它没有任何实际时间上的约束。如果进程A完成了写操作w，然后进程B开始读操作r，r不一定能观察到w。</li>
<li>快照隔离也不需要保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。</li>
<li>快照隔离允许重排序。比如，一个快照隔离的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</li>
</ul>
<p>Berenson、Bernstein等<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">首次定义的快照隔离</a>：</p>
<blockquote>
<p>…将事务开始读取已提交的数据快照中的数据时的时间戳，作为开始时间戳。此时间戳可以是事务首次读取数据之前的任何时间。只要自开始时间戳以来的快照数据是可维护的，那运行在快照隔离中的事务的读操作将永远不会被阻塞。事务的写操作（更新、插入和删除）也会反映到快照中，如果事务再次访问（读取或更新）数据，则将再次读取该快照。开始时间戳之后的其他事务的更新操作，对于本事务是不可见的。</p>
<p>当事务T1准备提交时，它将获得一个提交时间戳，该时间戳将大于任何现有的开始时间戳或提交时间戳。只有不存在符合下面条件的事务T2的情况下，T1才能提交成功：T2的提交时间戳在T1执行期间，即[开始时间戳，提交时间戳]期间，并且T2和T1对同一份数据进行了写操作。如果存在这样的T2，那T1将会中止。这种被称为First-committer-wins的特性，防止了更新丢失的现象（现象P4）。T1提交后，其更新对于开始时间戳大于T1提交时间戳的所有事物都可见了。</p>
</blockquote>
<hr>
<h5 id="Cursor-Stability-游标稳定性"><a href="#Cursor-Stability-游标稳定性" class="headerlink" title="Cursor Stability     游标稳定性"></a>Cursor Stability     游标稳定性</h5><p>游标稳定性是读已提交的增强版，可以防止updates的丢失。它引入了游标（cursor）的概念，用于指向事务访问的特定对象。事务中可以包含多个游标。当事务中使用游标读取某个对象时，在释放游标或提交事务之前，任何其他事务都不能修改该对象。</p>
<p>游标稳定性可以防止更新丢失，即事务T1读取、修改和写回对象x，但另一个事务T2在T1读取x后更新x，这就导致T2的更新丢失了。</p>
<p>特点：</p>
<ul>
<li>不是完全可用的；在网络分区的情况下，一些或所有节点可能无法取得进展。</li>
<li>游标稳定性没有任何的real-time上的约束。如果进程A完成了写操作w，然后进程B开始读操作r，r不一定能观察到w。</li>
<li>游标稳定性也不保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</li>
</ul>
<p>游标稳定性要强于读已提交，所以它禁止ANSI定义的脏写和脏读，但是允许不可重复读和幻读。</p>
<hr>
<h5 id="Monotonic-Atomic-View-单调原子视图"><a href="#Monotonic-Atomic-View-单调原子视图" class="headerlink" title="Monotonic Atomic View     单调原子视图"></a>Monotonic Atomic View     单调原子视图</h5><p>单调原子视图比读已提交要强一些，防止事务读到之前提交的事务部分（非全部）结果。它表示ACID中的原子约束，即事务中的所有操作要么全生效，要么全不生效。只要T1中的某个写操作能被T2观察到，那T1的所有操作的结果都对T2都是可见的。这在强制外键约束和确保索引和物化视图反映其底层对象方面尤其有用。</p>
<p>特点：</p>
<ul>
<li>完全可用：在存在网络分区的情况下，每个节点都可以取得进展。</li>
<li>没有real-time的约束。如果进程A完成写操作w，进程B开始读操作r，r不一定保证能看到w的结果。</li>
<li>不保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</li>
<li>类似于<a href="https://jepsen.io/consistency/models/serializable">串行化</a>，单调原子视图允许重排序。比如，一个单调原子视图的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</li>
</ul>
<p>单调原子视图是在Bailis、Davidson、Fekete等人在<a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">《高可用交易：优点和局限性》</a>中提出的：</p>
<blockquote>
<p>在MAV（Monotonic Atomic View）下，一旦一个事务Ti的一些操作结果被另一个事务Tj观察到，此后，Ti的所有操作结果都能被Tj观察到。也就是说，如果事务Tj读取事务Ti写入的对象的某个版本，则Tj稍后读取的对象无法返回Ti应用了更高版本的值。</p>
</blockquote>
<p>由于单调原子视图强于读已提交，因此它禁止了脏读和脏写，但是允许出现不可重复度和幻读。</p>
<hr>
<h5 id="Read-Committed-读已提交"><a href="#Read-Committed-读已提交" class="headerlink" title="Read Committed     读已提交"></a>Read Committed     读已提交</h5><p>读已提交加强了<a href="https://jepsen.io/consistency/models/read-uncommitted">Read uncommitted（读未提交）</a>，防止了脏读：不允许事务观察到未提交事务的写操作。</p>
<p>特点：</p>
<ul>
<li>完全可用的：存在网络分区的情况下，每个节点都可以取得进展。；</li>
<li>读已提交没有任何real-time上的约束。如果进程A完成写操作w，然后进程B开始读操作r，则r不一定保证能观察到w。</li>
<li>读已提交也不保证单个进程上的多个事务之间的顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</li>
<li>类似于串行化，读已提交允许重排序。比如，一个读已提交的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</li>
</ul>
<p>读已提交不会出现P1（脏读）现象：</p>
<blockquote>
<p>P1（“脏读”）：SQL事务T1修改了某行。然后，SQL事务T2在T1执行提交之前读取该行。如果T1随后执行回滚，T2将读取到从未提交过的行，可以认为该行从未存在过。</p>
</blockquote>
<hr>
<h5 id="Read-Uncommitted-读未提交"><a href="#Read-Uncommitted-读未提交" class="headerlink" title="Read Uncommitted    读未提交"></a>Read Uncommitted    读未提交</h5><p>读未提交是这样的一致性模型，它禁止脏写，即两个事务在提交之前同时修改同一对象。</p>
<p>特点：</p>
<ul>
<li>完全可用：存在网络分区的情况下，每个节点都可以取得进展。</li>
<li>读未提交没有任何实际时间上的约束。如果进程A完成写操作w，然后进程B开始读操作r，则r不一定保证能观察到w。</li>
<li>也不保证单个进程上的多个事务之间的顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</li>
<li>类似于<a href="https://jepsen.io/consistency/models/serializable">串行化</a>，读未提交允许重排序。比如，一个读已提交的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>00Strong consistency models--aphyr</title>
    <url>/2021/12/29/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/00Strong%20consistency%20models--aphyr/</url>
    <content><![CDATA[<p>网络分区是不可避免的，交换机、网卡、主机硬件、操作系统、磁盘，虚拟化层、程序的语义等等，都会使我们的消息延迟、丢弃、重发或者乱序。所以我们希望我们的软件保持某种<font color=red>直观的正确性。</font></p>
<span id="more"></span>
<p>Network partitions <a href="http://aphyr.com/posts/288-the-network-is-reliable">are going to happen</a>. Switches, NICs, host hardware, operating systems, disks, virtualization layers, and language runtimes, not to mention program semantics themselves, all conspire to delay, drop, duplicate, or reorder our messages. In an uncertain world, we want our software to maintain some sense of <em>intuitive correctness</em>.</p>
<p>但是如何定义正确呢？本文中我们将介绍一些“强”一致性模型，看看它们是如何结合在一起的。</p>
<p>Well, obviously we want intuitive correctness. Do The Right Thing(TM)! But what exactly <em>is</em> the right thing? How might we describe it? In this essay, we’ll take a tour of some “strong” consistency models, and see how they fit together.</p>
<h3 id="Correctness-什么是正确"><a href="#Correctness-什么是正确" class="headerlink" title="Correctness  什么是正确"></a>Correctness  什么是正确</h3><p>首先，我们这样定义算法的抽象行为：一个系统由状态，以及一些能够转换该状态的操作组成。随着系统的运行，它通过一些操作历史（history of operations，个人理解就是一系列操作，以及每个操作后的状态），从一种状态转换到另一种状态。</p>
<p>There are many ways to express an algorithm’s abstract behavior–but just for now, let’s say that a <em>system</em> is comprised of a <em>state</em>, and some <em>operations</em> that transform that state. As the system runs, it moves from state to state through some history of operations.</p>
<p>比如某个系统中，状态就是一个变量，而操作就是对变量的读写。那下面的Ruby代码，就是对这个变量的赋值和打印：</p>
<p>For instance, our state might be a variable, and the <em>operations</em> on the state could be the writes to, and reads from, that variable. In this simple Ruby program, we write and read a variable several times, printing it to the screen to illustrate the reads.</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">x = <span class="string">&quot;a&quot;</span>; puts x; puts x</span><br><span class="line">x = <span class="string">&quot;b&quot;</span>; puts x</span><br><span class="line">x = <span class="string">&quot;c&quot;</span></span><br><span class="line">x = <span class="string">&quot;d&quot;</span>; puts x</span><br></pre></td></tr></table></figure>

<p>看到上面的代码，我们已经有了这个程序正确性的直观模型：它应该打印“aabd”。</p>
<p>We already have an <em>intuitive</em> model of this program’s correctness: it should print “aabd”. Why? Because each of the statements <em>happen in order</em>. First we <code>write the value a</code>, then <code>read the value a</code>, then <code>read the value a</code>, then <code>write the value b</code>, and so on.</p>
<p>这样的系统中，我们设置了变量的值为<code>a</code>，那后续就应该读到<code>a</code>，即读操作应该返回最近一次写入的值（register系统的<font color=red>不变式</font>），我们称这样的系统（具有单一值的单个变量）为register系统。</p>
<p>Once we set a variable to some value, like <code>a</code>, reading it should return <code>a</code>, until we change the value again. Reading a variable returns the most recently written value. We call this kind of system–a variable with a single value–a <em>register</em>.</p>
<p>给变量赋值了<code>a</code>，然后读该变量就应该返回<code>a</code>。我们认为这是理所应当的，但是系统可能会返回任意值，比如<code>A</code>，<code>d</code>甚至是<code>the moon</code>，如果确实如此，那直观上我们会说系统是不正确的。</p>
<p>We’ve had this model drilled into our heads from the first day we started writing programs, so it feels like second nature–but this is <em>not</em> the only way variables could work. A variable could return <em>any</em> value for a read: <code>a</code>, <code>d</code>, or <code>the moon</code>. If that happened, we’d say the system was <em>incorrect</em>, because those operations don’t align with our <em>model</em> of how variables are supposed to work.</p>
<p>这暗示了系统的“正确性”定义：给定一些与<font color=red>操作和状态相关的“规则”</font>，针对系统的操作历史应该始终遵循这些规则。我们称这些规则为“一致性模型”。（一致性模型，就是规则的集合，就是符合规则的操作历史的集合。什么是正确？符合规则就是正确！）</p>
<p>This hints at a definition of <em>correctness</em> for a system: given some <em>rules</em> which relate the operations and state, the history of operations in the system should always <em>follow those rules</em>. We call those rules a <em>consistency model</em>.</p>
<p>我们将register系统的规则表述为简单的英语语句，但规则也可以是任意复杂的数学结构。比如“读操作返回两次写入之前的值，如果该值是4，则返回cat或dog；否则将其值加上3后返回”，<font color=red>这就是规则，就是一致性模型</font>；或者“每次读取总是返回0”也是一致性模型；甚至说“根本没有规则，任何操作都是允许的”，这也是一种一致性模型，这是最容易满足的一致性模型。</p>
<p>We phrased our rules for registers as simple English statements, but they could be arbitrarily complicated mathematical structures. “A read returns the value from two writes ago, plus three, except when the value is four, in which case the read may return either cat or dog” is a consistency model. As is “Every read always returns zero”. We could even say “There are no rules at all; every operation is permitted”. That’s the <em>easiest</em> consistency model to satisfy; every system obeys it trivially.</p>
<p>正式的说法是，<font color=red>一致性模型是所有允许的操作历史的集合。</font>如果我们运行一个程序，并且在允许的操作集合中执行了一系列操作，那么就说这种特定的执行是一致的（满足这种一致性模型的）。如果程序的操作历史不在一致性模型中，并且程序偶尔出错，那么我们说这种操作历史是不一致的（不满足这种一致性模型）。如果每个可能的执行都在允许的范围内，则说系统满足这种一致性模型。我们当然是希望真实系统满足“直观正确的”一致性模型，这样我们就可以编写可预测的程序。</p>
<p>More formally, we say that a consistency model is the <em>set of all allowed histories of operations</em>. If we run a program and it goes through a sequence of operations in the allowed set, that particular execution <em>is consistent</em>. If the program screws up occasionally and goes through a history <em>not</em> in the consistency model, we say the history was <em>inconsistent</em>. If <em>every</em> possible execution falls into the allowed set, the system <em>satisfies</em> the model. We want real systems to satisfy “intuitively correct” consistency models, so that we can write predictable programs.</p>
<hr>
<h3 id="Concurrent-histories-并发操作"><a href="#Concurrent-histories-并发操作" class="headerlink" title="Concurrent histories   并发操作"></a>Concurrent histories   并发操作</h3><p>假设现在有两个控制线程（这里也可以是进程，或者是其他可以并行进行的操作）对同一个register进行修改，那上面的register不变式就被打破了。</p>
<p>Now imagine a concurrent program, like one written in Node.js or Erlang. There are multiple logical threads of control, which we term “processes”. If we run a concurrent program with two processes, each of which works with the same register, our earlier register invariant could be violated.</p>
<img src="/img/00Strong consistency models--aphyr/image-20220224122231411.png" alt="image-20220224122231411" style="zoom:50%;" />

<p>假设两个线程分别是<code>top</code>和<code>bottom</code>。<code>top</code>进行的操作是：<code>write a, read, read</code>。<code>bottom</code>同时进行操作是<code>read, write b, read</code>。因为程序是并发的，所以两个线程的操作交织在一起，交织的操作序列可以有很多种排序—-只要单个线程的操作按照指定的顺序执行就行。在上图中展示出的操作序列是：<code>top write a, bottom read a, top read a, bottom write b, top read b, bottom read b</code>。</p>
<p>There are two processes at work here: call them “top” and “bottom”. The top process tries to write a , read , read . The bottom process, meanwhile, tries to read , write b , read . Because the program is concurrent, the operations from these two processes could interleave in more than one order–so long as the operations for a single process happen in the order that process specifies. In this particular case, top writes a , bottom reads a , top reads a , bottom writes b , top reads b , and bottom reads b .</p>
<p>并发是无处不在的，多线程的操作以任意顺序交织在一起。但是单个线程对操作历史有个约束：属于单个线程的操作必须按照顺序完成。</p>
<p>In this light, the concept of concurrency takes on a different shape. We might imagine every program as concurrent by default–when executed, operations could happen in any order. A thread, a process–in the logical sense, anyway–is a constraint over the history: operations belonging to the same thread must take place in order. Logical threads impose a partial order over the allowed operations.</p>
<p>引入并发后，即使是从单个线程的角度看，我们的register系统的不变式也被打破了。比如<code>top</code>线程，先是写入了<code>a</code>，然后读到了 <code>a</code>，最后读到了<code>b</code>，这个<code>b</code>并不是自己写入的。所以我们必须对一致性模型就行某种程度上的relax，以便能适应这种并发的情况：一个线程允许读到其他线程最近写入的值。这样的register系统中，两个线程就会进行协调和同步，因为他们共享了状态。</p>
<p>Even with that order, our register invariant–from the point of view of an individual process–no longer holds. The process on top wrote a , read a , then read b –which is not the value it wrote. We must relax our consistency model to usefully describe concurrency. Now, a process is allowed to read the most recently written value from any process, not just itself. The register becomes a place of coordination between two processes; they share state.</p>
<h3 id="Light-cones-光锥（实际就是指操作的时间区间）"><a href="#Light-cones-光锥（实际就是指操作的时间区间）" class="headerlink" title="Light cones 光锥（实际就是指操作的时间区间）"></a>Light cones 光锥（实际就是指操作的时间区间）</h3><p>然而，现实世界中，线程间的距离是遥远的，操作是需要耗时的。</p>
<p>However, this is not the full story: in almost every real-world system, processes are distant from each other. An uncached value in memory, for instance, is likely on a DIMM thirty centimeters away from the CPU. It takes light over a full nanosecond to travel that distance–and real memory accesses are much slower. A value on a computer in a different datacenter could be thousands of kilometers–hundreds of milliseconds–away. We just can’t send information there any faster; physics, thus far, forbids it.</p>
<p>这就表示<font color=red>操作不是瞬时的</font>，操作是有时间区间的。比如我们写一个变量，这个写操作会传递到内存，到另一个节点上，甚至到月亮上，然后内存中的状态被修改后，会返回一个确认消息，收到确认消息后，我们才知道操作发生并且完成了。</p>
<p>This means our operations are <em>no longer instantaneous</em>. Some of them might be so fast as to be negligible, but in full generality, operations <em>take time</em>. We <em>invoke</em> a write of a variable; the write travels to memory, or another computer, or the moon; the memory changes state; a confirmation travels back; and then we <em>know</em> the operation took place.</p>
<img src="/img/00Strong consistency models--aphyr/concurrent-read.jpg" alt="concurrent-read.jpg" style="zoom:50%;" />

<p>消息发送的快慢，意味着操作历史的“模糊性”。如果消息传递的快些或者慢些，那操作历史中的操作顺序就可能会发生变化。比如这里的例子中，当值为<code>a</code>时，<code>bottom</code>线程发起读操作，读操作进行过程中，<code>top</code>线程写入了<code>b</code>，这个写动作在读操作之前到达，那<code>bottom</code>这里读到的就是<code>b</code>，而非<code>a</code>。</p>
<p>The delay in sending messages from one place to another implies <em>ambiguity</em> in the history of operations. If messages travel faster or slower, they could take place in unexpected orders. Here, the bottom process invokes a read when the value is <code>a</code>. While the read is in flight, the top process writes <code>b</code>–and by happenstance, its write arrives <em>before</em> the read. The bottom process finally completes its read and finds <code>b</code>, not <code>a</code>.</p>
<p>这就又违反了之前约定好的并发register系统的一致性模型。<code>bottom</code>线程读到的值，并不是在发起读操作那一时刻的值。那如果用操作完成时间，而非操作调用时间，来表示操作的true time呢？这还是有问题，如果读操作在写操作之前到达，那读操作会返回<code>a</code>，而非当前值（true time时刻的值）<code>b</code>。</p>
<p>This history violates our concurrent register consistency model. The bottom process did <em>not</em> read the current value at the time it invoked the read. We might try to use the completion time, rather than the invocation time, as the “true time” of the operation, but this fails by symmetry as well; if the read arrives <em>before</em> the write, the process would receive <code>a</code> when the current value is <code>b</code>.</p>
<p>在一个分布式系统中（操作需要时间的系统），必须再次relax一致性模型：<font color=red>允许这些模棱两可的顺序。</font></p>
<p>In a distributed system–one in which it takes time for an operation to take place–we must <em>relax</em> our consistency model again; allowing these ambiguous orders to happen.</p>
<p>How far must we go? Must we allow <em>all</em> orderings? Or can we still impose some sanity on the world?</p>
<h3 id="Linearizability-线性一致性"><a href="#Linearizability-线性一致性" class="headerlink" title="Linearizability 线性一致性"></a>Linearizability 线性一致性</h3><img src="/img/00Strong consistency models--aphyr/finite-concurrency-bounds.jpg" alt="finite-concurrency-bounds.jpg" style="zoom:50%;" />

<p>仔细考虑的话，事件的顺序并不是完全模糊的，它还是有界限在的：消息的发送总有一个起点；操作的结果肯定是在该操作调起之后才能返回。</p>
<p>On careful examination, there are some bounds on the order of events. We can’t send a message back in time, so the <em>earliest</em> a message could reach the source of truth is, well, <em>instantly</em>. An operation cannot take effect <em>before</em> its invocation.</p>
<p>同样，收到操作已完成的消息的时间也不会倒流，这就表示：任何操作在完成之后（收到完成的消息了），那他所作的修改肯定已经生效了。</p>
<p>Likewise, the message informing the process that its operation completed cannot travel back in time, which means that no operation may take effect <em>after</em> its completion.</p>
<p>我们假设有一个全局状态，所有线程都可以访问它；如果假设针对这个全局状态的操作都是<font color=red>原子</font>的，而不是相互交织的，那就可以排除掉很多不可能的操作历史。这就得到了线性一致性的模型：<font color=red>每个操作都会在其调起和完成之间的某个时刻<strong>原子地</strong>生效。</font></p>
<p>If we assume that there is a single global state that each process talks to; if we assume that operations on that state take place <em>atomically</em>, without stepping on each other’s toes; then we can rule out a great many histories indeed. We know that <strong>each operation appears to take effect atomically at some point between its invocation and completion</strong>.</p>
<p>我们称这种一致性模型就是线性一致性；尽管操作是并发执行的，但每个操作都是以线性顺序发生的。</p>
<p>We call this consistency model <em>linearizability</em>; because although operations are concurrent, and take time, there is some place–or the appearance of a place–where every operation happens in a nice linear order.</p>
<img src="/img/00Strong consistency models--aphyr/linearizability-complete-visibility.jpg" alt="linearizability-complete-visibility.jpg" style="zoom:50%;" />

<p>“单一全局状态”不必是单个节点；操作实际上也不必是原子操作。状态可以分布在多台机器上，对其进行的操作也可以经历多个步骤才能完成。<font color=red>只要从外部观察者的角度来看，操作历史等同于原子的修改单一状态即可</font>。一般情况下，一个线性一致性的系统通常由多个协调性的线程组成，每个线程本身就是线性的。</p>
<p>The “single global state” doesn’t have to be a single node; nor do operations actually have to be atomic. The state could be split across many machines, or take place in multiple steps–so long as the external history, from the point of view of the processes, appears <em>equivalent</em> to an atomic, single point of state. Often, a linearizable system is made up of smaller coordinating processes, each of which is itself linearizable; and <em>those</em> processes are made up of carefully coordinated smaller processes, and so on, down to <a href="http://en.wikipedia.org/wiki/Compare-and-swap">linearizable operations provided by the hardware</a>.</p>
<p>线性一致性有个很重要的一个约束，<font color=red>就是一旦一个操作完成了，那所有人必须都能看到该操作的结果（或者是更加靠后的结果）。</font>因为操作起作用的时间，肯定不会晚于其结束时间，而该操作之后（即调起时间晚于该操作完成时间的操作）操作，它发生作用的时间肯定要晚于其调起的时间，进而肯定要晚于前一个操作的结束时间。举例而言：如果一个操作成功写入了<code>b</code>，那该写操作之后的读操作，必须能读到<code>b</code>（或者如果同时有其他写操作的话，能读到更新的值）。</p>
<p>Linearizability has powerful consequences. Once an operation is complete, <em>everyone</em> must see it–or some later state. We know this to be true because each operation <em>must</em> take place before its completion time, and any operation invoked subsequently <em>must</em> take place after the invocation–and by extension, after the original operation itself. Once we successfully write <code>b</code>, every subsequently invoked read <em>must</em> see <code>b</code>–or some later value, if more writes occur.</p>
<p>我们可以用线性化的原子约束（atomic constraint of linearizability）来安全的修改状态。比如我们可以定义一个<code>compare-and-set</code>操作，这个操作只有当register的值是old-value的时候，才把register的值设置为new-value。我们可以用<code>compare-and-set</code>作为互斥锁、信号量、管道、计数器等等的基础。</p>
<p>We can use the atomic constraint of linearizability to <em>mutate state safely</em>. We can define an operation like <code>compare-and-set</code>, in which we set the value of a register to a new value if, and only if, the register currently has some other value. We can use <code>compare-and-set</code> as the basis for mutexes, semaphores, channels, counters, lists, sets, maps, trees–all kinds of shared data structures become available. Linearizability guarantees us the safe interleaving of changes.</p>
<p>此外，线性一致性在时间上的约束，也保证所有修改后的结果，都能在操作完成之后的其他读操作是可见的。所以，<font color=red>线性一致性不允许出现读取到旧数据的情况；也不允许出现非单调读的现象——即先读到新值，然后又读到了旧值。</font></p>
<p>Moreover, linearizability’s time bounds guarantee that those changes will be visible to other participants after the operation completes. Hence, linearizability prohibits stale reads. Each read will see <em>some</em> current state between invocation and completion; but not a state prior to the read. It also prohibits <em>non-monotonic reads</em>–in which one reads a new value, then an old one.</p>
<p>基于这些约束，这样就能解释为什么线性一致性的系统会被选为很多并发编程的基础。比如Javascript 中的所有变量都是（独立地）线性化的；Java中的volatile变量也是如此。很多语言都有互斥锁和信号量，这些也是线性的。</p>
<p>Because of these strong constraints, linearizable systems are easier to reason about–which is why they’re chosen as the basis for many concurrent programming constructs. All variables in Javascript are (independently) linearizable; as are volatile variables in Java, atoms in Clojure, or individual processes in Erlang. Most languages have mutexes and semaphores; these are linearizable too. Strong assumptions yield strong guarantees.</p>
<p>但如果我们不能满足这些假设，会发生什么呢？</p>
<p>But what happens if we can’t satisfy those assumptions?</p>
<h3 id="Sequential-consistency-顺序一致性"><a href="#Sequential-consistency-顺序一致性" class="headerlink" title="Sequential consistency 顺序一致性"></a>Sequential consistency 顺序一致性</h3><img src="/img/00Strong consistency models--aphyr/sequential-history.jpg" alt="sequential-history.jpg" style="zoom:50%;" />

<p>如果我们允许进程在<font color=red>时间上不那么严格</font>，也就是说，操作就可以在调用之前或完成之后生效，但保留了线程内部的操作必须按该线程规定的顺序进行的约束，那么就得到一个比线性一致性稍弱的一致性模型：<font color=red>顺序一致性。</font></p>
<p>If we allow processes to skew in time, such that their operations can take effect <em>before</em> invocation, or <em>after</em> completion–but retain the constraint that operations from any given process must take place in that process’ order–we get a weaker flavor of consistency: <em>sequential consistency</em>.</p>
<p>顺序一致性要比线性一致性允许更多的操作历史。它实际上也非常有用，比如，用户将视频上传至YouTube时，YouTube将视频放到队列中进行处理，然后返回给用户该视频的网页，但是实际上用户当时还不能在网页上看到该视频，要等几分钟过后视频处理完成后才能看到。队列这里就起到了异步处理的作用，同时它还保留了顺序。</p>
<p>Sequential consistency allows more histories than linearizability–but it’s still a useful model: one that we use every day. When a user uploads a video to YouTube, for instance, YouTube puts that video into a <em>queue</em> for processing, then returns a web page for the video right away. We can’t actually <em>watch</em> the video at that point; the video upload <em>takes effect</em> a few minutes later, when it’s been fully processed. Queues remove <em>synchronous</em> behavior while (depending on the queue) preserving order.</p>
<p>很多缓存也是一种顺序一致性系统。比如在Twitter上写了tweet，或者在Facebook上发帖，需要时间才能渗透到缓存系统的各个层。不同的用户会在不同的时间看到我的消息，但<font color=red>能保证每个用户都会按顺序看到我的操作，一旦看到了，那帖子就不会消失，而且如果写了多条评论，它们将按顺序显示</font>。</p>
<p>Many caches also behave like sequentially consistent systems. If I write a tweet on Twitter, or post to Facebook, it takes time to percolate through layers of caching systems. Different users will see my message at different times–but each user will see my operations <em>in order</em>. Once seen, a post shouldn’t disappear. If I write multiple comments, they’ll become visible sequentially, not out of order.</p>
<h3 id="Causal-consistency-因果一致性"><a href="#Causal-consistency-因果一致性" class="headerlink" title="Causal consistency 因果一致性"></a>Causal consistency 因果一致性</h3><p>如果不强制一个线程的所有操作都是按序执行的，而只要求<font color=red>具有因果依赖关系的操作必须按序</font>。比如，每个人看到的针对一个blog的所有评论都是相同的顺序出现的，并且要保证只有某条回复A依赖的回复B可见之后，A才可见。如果把像”本操作依赖操作X”这样的因果关系显式编码到操作中，那数据库可以直到它依赖的所有操作都可见之后，才使得该操作可见。</p>
<p>We don’t have to enforce the order of <em>every</em> operation from a process. Perhaps, only <em>causally related</em> operations must occur in order. We might say, for instance, that all comments on a blog post must appear in the same order for everyone, and insist that any <em>reply</em> be visible to a process <em>only after the post it replies to</em> is visible. If we encode those causal relationships like “I depend on operation X” as an explicit part of each operation, the database can delay making operations visible until it has all the operation’s dependencies.</p>
<p>这种一致性模型要比约束同一线程内所有操作必须按序的一致性稍弱，这种模型中<font color=red>同一线程内没有因果依赖的操作可以以任意顺序出现。</font></p>
<p>This is weaker than ordering every operation from the same process–operations from the same process with independent causal chains could execute in any relative order–but prevents many unintuitive behaviors.</p>
<h3 id="Serializable-consistency-序列一致性"><a href="#Serializable-consistency-序列一致性" class="headerlink" title="Serializable consistency 序列一致性"></a>Serializable consistency 序列一致性</h3><img src="/img/00Strong consistency models--aphyr/serializable-history.jpg" alt="serializable-history.jpg" style="zoom:50%;" />

<p><font color=red>如果操作历史相当于以某种单一原子顺序发生的，但不约束调用和完成时间，这就是所谓的序列一致性。</font>这种一致性可能比你预期的强很多，也弱很多。</p>
<p>If we say that the history of operations is equivalent to one that took place in some single atomic order–but say nothing about the invocation and completion times–we obtain a consistency model known as <em>serializability</em>. This model is both much stronger and much weaker than you’d expect.</p>
<p>序列化的”弱”体现在，他允许更多的操作历史，因为它对时间和顺序没有限制。在上面的图中，消息可以在过去，也可以在未来发送，这就造成了时间线的交叉。在序列化的数据库中，像<code>read x</code>这样的事务，是允许<code>x</code>在未初始化时的time 0进行的，也允许它被无限地推迟到遥远的未来！像<code>write 2 to x</code>这个事务可能会立即执行，也可以被推迟到时间线的末尾，即永远也不会发生。</p>
<p>Serializability is <em>weak</em>, in the sense that it permits many types of histories, because it places no bounds on time or order. In the diagram to the right, it’s as if messages could be sent arbitrarily far into the past or future, that causal lines are allowed to cross. In a serializable database, a transaction like <code>read x</code> is always allowed to execute at time 0, when <code>x</code> had not yet been initialized. Or it might be delayed infinitely far into the future! The transaction <code>write 2 to x</code> could execute right now, <em>or</em> it could be delayed until the end of time, never appearing to occur.</p>
<p>比如像下面的代码，可以打印出<code>nil</code>，<code>1</code>或<code>2</code>。因为这些操作可以以任意顺序执行。这就是序列化一致性弱的体现。这里我们假设每行代码代表单独的操作而且这些操作都执行成功了。</p>
<p>For instance, in a serializable system, the program</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = 1</span><br><span class="line">x = x + 1</span><br><span class="line">puts x</span><br></pre></td></tr></table></figure>

<p>is allowed to print <code>nil</code>, <code>1</code>, <em>or</em> <code>2</code>; because the operations can take place in any order. This is a surprisingly weak constraint! Here, we assume that each line represents a single operation and that all operations succeed.</p>
<p>另一方面，序列化的强体现在，它也排除了很多操作历史，因为它需<font color=red>要有线性顺序</font>。比如下面的代码，只能有一种执行顺序，就是<code>x</code>的值从<code>nil -&gt; 1 -&gt; 2 -&gt; 3</code>，然后最后打印出<code>3</code>。</p>
<p>On the other hand, serializability is <em>strong</em>, in the sense that it prohibits large classes of histories, because it demands a linear order. The program</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print x if x = 3</span><br><span class="line">x = 1 if x = nil</span><br><span class="line">x = 2 if x = 1</span><br><span class="line">x = 3 if x = 2</span><br></pre></td></tr></table></figure>

<p>can only be ordered in one way. It doesn’t happen in the same order we <em>wrote</em>, but it will reliably change <code>x</code> from <code>nil</code> -&gt; <code>1</code> -&gt; <code>2</code> -&gt; <code>3</code>, and finally print <code>3</code>.</p>
<p>由于序列化允许操作的任意乱序执行（只要这些操作是原子的），因此它在实际应用中并不是很有用。大多数声称提供序列化的数据库实际上提供了<font color=red>强序列化</font>，它与线性一致性有相同的时间界限。更加复杂的是，大多数SQL数据库声称的序列一致性实际上表示<a href="http://www.bailis.org/papers/hat-hotos2013.pdf">较弱的东西</a>，比如可重复读，cursor稳定性或者快照隔离。</p>
<p>Because serializability allows arbitrary reordering of operations (so long as the order appears atomic), it is not particularly useful in real applications. Most databases which claim to provide serializability actually provide <em>strong serializability</em>, which has the same time bounds as linearizability. To complicate matters further, what most SQL databases term the SERIALIZABLE consistency level <a href="http://www.bailis.org/papers/hat-hotos2013.pdf">actually means something weaker</a>, like repeatable read, cursor stability, or snapshot isolation.</p>
<h3 id="Consistency-comes-with-costs-一致性带来的成本"><a href="#Consistency-comes-with-costs-一致性带来的成本" class="headerlink" title="Consistency comes with costs 一致性带来的成本"></a>Consistency comes with costs 一致性带来的成本</h3><p>我们说过，“弱”一致性模型比“强”一致性模型允许更多的操作历史。例如，线性一致性需要保证操作在调用时和完成时之间进行。然而，要求顺序就以为这需要协调。错略的说，我们需要排除的操作历史越多，那系统中的参与者就需要更加谨慎，更多的沟通。</p>
<p>We’ve said that “weak” consistency models <em>allow more histories</em> than “strong” consistency models. Linearizability, for example, guarantees that operations take place between the invocation and completion times. However, <em>imposing order requires coordination</em>. Speaking loosely, the more histories we exclude, the more careful and communicative the participants in a system must be.</p>
<p>你可能已经听说过<a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf">CAP定理</a>，它表示对于一致性，可用性和分区容错性三者而言，任何系统最多能保证其中两个属性。虽然Eric Brewer的CAP是用非正式术语描述的，但是CAP定理有着非常精确的定义：</p>
<ol>
<li><font color=red>一致性这里就是线性一致性</font>，特别是对于一个线性一致性的<code>register</code>系统来说。<code>register</code>系统可以扩展到sets, lists, maps，关系数据库等等。所以<font color=red>CAP定理适用于所有的线性一致性系统；</font></li>
<li>可用性表示对于非失败节点发起的请求必须能成功完成。因为网络分区的时间可能持续任意长的时间，这就表示节点不能简单地将响应延迟到分区修复之后；</li>
<li>分区容错性意味着允许发生网络分区。在网络正常的情况下提供线性一致性和可用性是很容易的，但是<font color=red>当网络不可靠时，同时提供这两种特性是不可能的。</font>因此如果网络不是完全可靠的（实际上它就是不可靠的），你就不能选择CA，所以构建在商用硬件之上的实际分布式系统中，最多可以保证AP或者CP。</li>
</ol>
<p>You may have heard of the <a href="http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf">CAP theorem</a>, which states that given consistency, availability, and partition tolerance, any given system may guarantee <em>at most</em> two of those properties. While Eric Brewer’s CAP conjecture was phrased in these informal terms, the CAP <em>theorem</em> has very precise definitions:</p>
<ol>
<li>Consistency means linearizability, and in particular, a linearizable register. Registers are equivalent to other systems, including sets, lists, maps, relational databases, and so on, so the theorem can be extended to cover <em>all kinds</em> of linearizable systems.</li>
<li>Availability means that every request to a non-failing node must complete successfully. Since network partitions are allowed to last <em>arbitrarily long</em>, this means that nodes cannot simply defer responding until after the partition heals.</li>
<li>Partition tolerance means that partitions <em>can happen</em>. Providing consistency and availability when the network is reliable is <em>easy</em>. Providing both when the network is not reliable is <em>provably impossible</em>. If your network is <em>not</em> perfectly reliable–and it isn’t–you cannot choose CA. This means that all practical distributed systems on commodity hardware can guarantee, at maximum, either AP or CP.</li>
</ol>
<img src="/img/00Strong consistency models--aphyr/family-tree.jpg" alt="family-tree.jpg" style="zoom: 50%;" />



<p>你可能会有这样的疑问：线性一致性不是一致性模型的唯一模型，我可以通过提供顺序一致性、串行一致性或者快照隔离，来绕过CAP定理。</p>
<p>“Hang on!” you might exclaim. “Linearizability is not the end-all-be-all of consistency models! I could work around the CAP theorem by providing <em>sequential</em> consistency, or serializability, or snapshot isolation!”</p>
<p>确实，CAP定理只是说我们不可能构建完全可用的线性一致性系统。但是问题是，有其他证据表明，你也无法构建具有顺序一致性、序列一致性、可重复读、分区隔离，cursor stability等，或是更强的一致性模型的完全可用系统。在Peter Bailis的论文<a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Highly Available Transactions paper</a>中指出，基于红色区域的一致性模型的系统时不可能完全可用的。</p>
<p>This is true; the CAP theorem only says that we cannot build totally available linearizable systems. The problem is that we have <em>other</em> proofs which tell us that you cannot build totally available systems with sequential, serializable, repeatable read, snapshot isolation, or cursor stability–or any models stronger than those. In this map from Peter Bailis’ <a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Highly Available Transactions paper</a>, models shaded in red <em>cannot be fully available</em>.</p>
<p>如果我们把可用性的概念relax一些，比如CLIENT只能与相同的节点进行通信，那有些类型的一致性模型就是可以实现的了，比如因果一致性，PRAM，以及read-your-write一致性。</p>
<p>If we <em>relax</em> our notion of availability, such that client nodes must always talk to the same server, <em>some</em> types of consistency become achievable. We can provide causal consistency, PRAM, and read-your-writes consistency.</p>
<p><font color=red>如果需要完全可用，那我们可以提供单调读（monotonic reads）、单调写（monotonic writes）、read committed、monotonic atomic view等等这样的一致性模型。</font>比如像Riak、Cassandra这样的分布式存储系统，低隔离级别的ANSI SQL数据库系统等等就是这样的。这些一致性模型不像前面那些图中画的那样有线性顺序，它们仅仅提供部分有序，这种部分有序可以组合成patchwork或者web，因为部分有序，所以这些一致性模型可以由更多的操作历史。</p>
<p>If we demand <em>total</em> availability, then we can provide monotonic reads, monotonic writes, read committed, monotonic atomic view, and so on. These are the consistency models provided by distributed stores like Riak and Cassandra, or ANSI SQL databases on the lower isolation settings. These consistency models don’t have linear orders like the diagrams we’ve drawn before; instead, they provide <em>partial</em> orders which come together in a patchwork or web. The orders are <em>partial</em> because they admit a broader class of histories.</p>
<h3 id="A-hybrid-approach-一种混合的方法"><a href="#A-hybrid-approach-一种混合的方法" class="headerlink" title="A hybrid approach 一种混合的方法"></a>A hybrid approach 一种混合的方法</h3><img src="/img/00Strong consistency models--aphyr/weak-not-unsafe.jpg" alt="weak-not-unsafe.jpg" style="zoom:50%;" />



<p>有一些算法需要线性一致性来保证安全性，比如要构建一个分布式锁的服务，那就需要为线性一致性，因为如果没有时间约束的话，那就有可能持有一个过去或者未来的锁。另一方面，还有很多算法不需要线性一致性，比如最终一致性的sets, lists, trees, maps等等，只需要弱一些的一致性模型就行了。</p>
<p>Some algorithms depend on linearizability for safety. If we want to build a distributed lock service, for instance, linearizability is <em>required</em>; without hard time boundaries, we could hold a lock from the future or from the past. On the other hand, many algorithms <em>don’t</em> need linearizability. Eventually consistent sets, lists, trees, and maps, for instance, can be safely expressed as <a href="http://hal.upmc.fr/docs/00/55/55/88/PDF/techreport.pdf">CRDTs</a> even in “weak” consistency models.</p>
<p>更强的一致性模型意味着更多的协调（更多的消息交互），以确保这些操作按照正确的顺序进行。这不仅造成了<font color=red>可用性较低，而且还暴露出更高的延迟。</font>这也就就是为什么现代CPU的内存模型默认情况下并不是线性的，除非显式标注。现代CPU会将针对内存的操作进行重排序，这样做虽然难以理解，但是却带来了惊人的性能优势。现实中的节点分布在不同位置的分布式系统，节点间通信通常由数百毫秒的延迟，通常也会做出类似的权衡。</p>
<p>Stronger consistency models also tend to require more coordination–more messages back and forth–to ensure their operations occur in the correct order. Not only are they less available, but they can also <em>impose higher latency constraints</em>. This is why modern CPU memory models are <em>not</em> linearizable by default–unless you explicitly say so, modern CPUs will reorder memory operations relative to other cores, or worse. While more difficult to reason about, the performance benefits are phenomenal. Geographically distributed systems, with hundreds of milliseconds of latency between datacenters, often make similar tradeoffs.</p>
<p>所以在实际的应用中，我们采用的是混合的数据存储系统或者数据库，融合不同的一致性模型来实现冗余、可用性、性能和安全性目标。<font color=red>在保证可用性和性能方面，尽可能采用弱一致性模型，如果算法需要有严格的操作顺序要求，则采用强一致性模型。</font>比如，可以向S3，Riak或Cassandra中写入大量数据，然后将这些数据的指针线性的写入Postgres，Zookeeper或者Etcd中。一些数据库支持多个一致性模型，比如关系数据库中的隔离级别是可以调整的，或者Cassandra和Riak中的线性化事务，这有助于减少使用的系统数量。不过，底线是：任何人如果说他们的一致性模型是唯一正确的选择，那么这种说法很可能只是一种销售策略，不可能存在两全其美的东西。</p>
<p>So in practice, we use <em>hybrid</em> data storage, mixing databases with varying consistency models to achieve our redundancy, availability, performance, and safety objectives. “Weaker” consistency models wherever possible, for availability and performance. “Stronger” consistency models where necessary, because the algorithm being expressed demands a stricter ordering of operations. You can write huge volumes of data to S3, Riak or Cassandra, for instance, then write a <em>pointer</em> to that data, linearizably, to Postgres, Zookeeper or Etcd. Some databases admit multiple consistency models, like tunable isolation levels in relational databases, or Cassandra and Riak’s linearizable transactions, which can help cut down on the number of systems in play. Bottom line, though: anyone who says their consistency model is the only right choice is likely trying to sell something. You can’t have your cake and eat it too.</p>
<p>说了这么多一致性模型相关的东西之后，下一篇文章中我想谈谈我们如何验证线性一致性系统的正确性。</p>
<p>Armed with a more nuanced understanding of consistency models, I’d like to talk about how we go about <em>verifying</em> the correctness of a linearizable system. In the next Jepsen post, we’ll discuss the linearizability checker I’ve built for testing distributed systems: <a href="http://aphyr.com/posts/314-computational-techniques-in-knossos">Knossos</a>.</p>
<p>For a more formal definition of these models, try Dziuma, Fatourou, and Kanellou’s <a href="http://www.ics.forth.gr/tech-reports/2013/2013.TR439_Survey_on_Consistency_Conditions.pdf">Survey on consistency conditions</a></p>
<hr>
<p>原文：<a href="https://aphyr.com/posts/313-strong-consistency-models">https://aphyr.com/posts/313-strong-consistency-models</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>01Consistency Models</title>
    <url>/2021/01/29/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/01Consistency%20Models/</url>
    <content><![CDATA[<h2 id="Consistency-Models"><a href="#Consistency-Models" class="headerlink" title="Consistency Models"></a>Consistency Models</h2><p>下面的图（改编自 <a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Bailis, Davidson, Fekete et al</a> 和<a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti &amp; Vukolic</a>的论文）展示了一致性模型之间的关系。其中的箭头表示了一致性模型之间的包含关系。比如严格序列化（Strict serializable）一致性意味着它既满足序列化（serializability）一致性，也满足线性（linearizability）一致性。图中的颜色表示在异步网络环境下的可用性。</p>
<span id="more"></span>
<p>This clickable map (adapted from <a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Bailis, Davidson, Fekete et al</a> and <a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti &amp; Vukolic</a>) shows the relationships between common consistency models for concurrent systems. Arrows show the relationship between consistency models. For instance, strict serializable implies both serializability and linearizability, linearizability implies sequential consistency, and so on. Colors show how available each model is, for a distributed system on an asynchronous network.</p>
<p>![image-20220309083251705](&#x2F;img&#x2F;01Consistency Models–jepsen&#x2F;image-20220309083251705.png)</p>
<p>上图展示了按层次结构排列的一致性模型。<font color=red>最强模型是严格序列化（Strict serializable），它统一了两个不相交的一致性模型族：多对象事务上的一致性模型族和单对象操作的一致性模型族。</font>当我们说模型x意味着y时，我们的意思是，对于x中约定的操作历史，也肯定满足y的规则约束，所以x比y“强”。</p>
<p>This diagram shows consistency models arranged in a hierarchy. The strongest model shown is <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializable</a>. Strict serializable unifies two disjoint families of consistency models: those over multi-object transactions, and those for single object operations. When we say that model x implies y, we mean that for every history where x holds, y does too; x is “stronger” than y.</p>
<p>在多对象这一边，强序列化意味着序列化；序列化又意味着可重复读（repeatable read）和快照隔离（snapshot isolation）；而可重复读和快照隔离又都意味着单调原子视图（monotonic atomic view）。可重复读还意味着cursor stability；cursor stability和单调原子视图又同时意味着read committed；read committed意味着read uncommitted；</p>
<p>On the multi-object side, strict serializable implies <a href="https://jepsen.io/consistency/models/serializable">serializable</a>, which in turn implies <a href="https://jepsen.io/consistency/models/repeatable-read">repeatable read</a> and <a href="https://jepsen.io/consistency/models/snapshot-isolation">snapshot isolation</a>. Snapshot isolation and repeatable read both imply <a href="https://jepsen.io/consistency/models/monotonic-atomic-view">monotonic atomic view</a>. Repeatable read implies <a href="https://jepsen.io/consistency/models/cursor-stability">cursor stability</a>. Cursor stability and monotonic atomic view both imply <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>, which implies <a href="https://jepsen.io/consistency/models/read-uncommitted">read uncommitted</a>.</p>
<p>在单对象这一边，强序列化意味着线性一致性；线性一致性又意味着顺序（sequential）一致性；顺序一致性意味着因果（causal）一致性；因果一致性意味着writes follow reads和PRAM；PRAM意味着单调读、单调写和read your writes；</p>
<p>For single-object models, strict serializable implies <a href="https://jepsen.io/consistency/models/linearizable">linearizable</a>, which implies <a href="https://jepsen.io/consistency/models/sequential">sequential</a>, which implies <a href="https://jepsen.io/consistency/models/causal">causal</a>. Causal implies <a href="https://jepsen.io/consistency/models/writes-follow-reads">writes follow reads</a> and <a href="https://jepsen.io/consistency/models/pram">PRAM</a>. PRAM implies <a href="https://jepsen.io/consistency/models/monotonic-reads">monotonic reads</a>, [monotonic writes](consistency&#x2F;models&#x2F;monotonic writes), and <a href="https://jepsen.io/consistency/models/read-your-writes">read your writes</a>.</p>
<p><font color=red>可用性</font>方面：</p>
<ul>
<li>粉红色的Unavailable：表示发生网络错误时是不可用的。一些或者所有节点必须暂停操作，以便保证安全性；</li>
<li>橙色的Sticky可用：表示只要client只与相同节点通信，那在无故障节点上是可用的；</li>
<li>蓝色的完全可用：表示即使网络瘫痪了，在非故障节点上也是可用的；</li>
</ul>
<p>All models at or stronger than cursor stability, snapshot isolation, and sequential cannot be totally or available in asynchronous networks. All models at or stronger than read your writes can be at most sticky available. Weaker models (of those listed here) can be totally available.</p>
<h3 id="Fundamental-Concepts-基本概念"><a href="#Fundamental-Concepts-基本概念" class="headerlink" title="Fundamental Concepts     基本概念"></a>Fundamental Concepts     基本概念</h3><p>Jepsen分析分布式系统的安全属性——特别是识别违反一致性模型的行为。本文首先明确一些一致性模型的基本定义、直观解释和理论基础。</p>
<p>Jepsen analyses the safety properties of distributed systems–most notably, identifying violations of consistency models. But what are consistency models? What phenomena do they allow? What kind of consistency does a given program really need?</p>
<p>In this reference guide, we provide basic definitions, intuitive explanations, and theoretical underpinnings of various consistency models for engineers and academics alike.</p>
<h4 id="Systems-系统"><a href="#Systems-系统" class="headerlink" title="Systems 系统"></a>Systems 系统</h4><p><font color=red>分布式系统是一种并发系统，关于并发控制的许多文献都适用于分布式系统。实际上，我们将要讨论的大多数概念最初都是为单节点并发系统制定的。然而，分布式系统和单节点并发系统在可用性和性能方面存在一些重要差异。</font></p>
<p><em>Distributed</em> systems are a type of <em>concurrent</em> system, and much of the literature on concurrency control applies directly to distributed systems. Indeed, most of the concepts we’re going to discuss were originally formulated for single-node concurrent systems. There are, however, some important differences in <em>availability</em> and <em>performance</em>.</p>
<p>系统有一个随时间变化的逻辑“状态”。例如，一个一个整数变量就是最简单的系统，其状态为’0’、’3’或’42’；互斥锁只有两种状态：锁定或解锁；键值系统的状态可能是键到值的映射，例如：“{cat:1，dog:1}”或“{cat:4}”。</p>
<p>Systems have a logical <em>state</em> which changes over time. For instance, a simple system could be a single integer variable, with states like <code>0</code>, <code>3</code>, and <code>42</code>. A mutex has only two states: locked or unlocked. The states of a key-value store might be maps of keys to values, for instance: <code>&#123;cat: 1, dog: 1&#125;</code>, or <code>&#123;cat: 4&#125;</code>.</p>
<h4 id="Processes-进程"><a href="#Processes-进程" class="headerlink" title="Processes 进程"></a>Processes 进程</h4><p>进程是一个逻辑上的单线程程序，执行计算和调起操作。单个进程不是异步的——我们通过独立的进程来模拟异步计算。我们说“逻辑上的单线程”是为了强调，虽然一个进程一次只能做一件事，但它的实现可能会分布在多个线程、操作系统进程，甚至物理节点上，只要这些组件提供了一个连贯的单线程程序的假象。</p>
<p>A <em>process</em><a href="https://jepsen.io/consistency#fn-1">1</a> is a logically single-threaded program which performs computation and runs operations. Processes are never asynchronous—we model asynchronous computation via independent processes. We say “logically single-threaded” to emphasize that while a process can only do one thing at a time, its implementation may be spread across multiple threads, operating system processes, or even physical nodes—just so long as those components provide the illusion of a coherent singlethreaded program.</p>
<h4 id="Operations-操作"><a href="#Operations-操作" class="headerlink" title="Operations 操作"></a>Operations 操作</h4><p>操作会引起系统从一个状态到另一个状态的转换。例如，一个单变量系统可能有“read”和“write”这样的操作；计数器可能具有诸如递增、递减和读取之类的操作；SQL存储可能具有诸如selects和updates之类的操作。</p>
<p>An operation is a transition from state to state. For instance, a single-variable system might have operations like <code>read</code> and <code>write</code>, which get and set the value of that variable, respectively. A counter might have operations like <em>increments</em>, <em>decrements</em>, and <em>reads</em>. An SQL store might have operations like <em>selects</em> and <em>updates</em>.</p>
<h4 id="Functions-Arguments-amp-Return-Values-函数、参数和返回值"><a href="#Functions-Arguments-amp-Return-Values-函数、参数和返回值" class="headerlink" title="Functions, Arguments &amp; Return Values 函数、参数和返回值"></a>Functions, Arguments &amp; Return Values 函数、参数和返回值</h4><p>理论上我们可以给每个状态转移命名。比如锁有两种状态转换：<code>lock</code>和<code>unlock</code>；整数变量可以有无数个读写操作：<code>read-the-value-1</code>，<code>read-the-value-2</code>和<code>write-1</code>，<code>write-2</code>等；</p>
<p>In theory, we could give every state transition a unique name. A lock has exactly two transition: <code>lock</code> and <code>unlock</code>. An integer register has an infinite number of reads and writes: <code>read-the-value-1</code>, <code>read-the-value-2</code>, …, and <code>write-1</code>, <code>write-2</code>, ….</p>
<p>为了使其易于处理，我们将这些转换表示为函数及其参数。函数的例子有<code>&#39;read&#39;、&#39;write&#39;、&#39;cas&#39;、&#39;increment&#39;</code>等，比如在单变量系统中，写入<code>1</code>的操作可以这样表示：<code>&#123;:f :write, :value 1&#125;</code>；比如对于键值系统中，给key为<code>a</code>的值加上3，可以表示为：<code>&#123;:f :increment, :value [&quot;a&quot; 3]&#125;</code>；在事务存储系统中，操作可能比较复杂，比如，先读取<code>a</code>的当前值，得到<code>2</code>，然后将<code>b</code>的值置为<code>2</code>,可以表示为：<code>&#123;:f :txn, :value [[:read &quot;a&quot; 2] [:write &quot;b&quot; 3]]&#125;</code></p>
<p>To make this more tractable, we break up these transitions into <em>functions</em> like <code>read</code>, <code>write</code>, <code>cas</code>, <code>increment</code>, etc., and <em>values</em> that parameterize those functions. In a single register system, a write of 1 could be written:</p>
<figure class="highlight clj"><table><tr><td class="code"><pre><span class="line">&#123;<span class="symbol">:f</span> <span class="symbol">:write</span><span class="punctuation">,</span> <span class="symbol">:value</span> <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p>Given a key-value store, we might increment the value of key “a” by 3 like so:</p>
<figure class="highlight clj"><table><tr><td class="code"><pre><span class="line">&#123;<span class="symbol">:f</span> <span class="symbol">:increment</span><span class="punctuation">,</span> <span class="symbol">:value</span> [<span class="string">&quot;a&quot;</span> <span class="number">3</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>In a transactional store, the value could be a complex transaction. Here we read the current value of <code>a</code>, finding 2, and set <code>b</code> to <code>3</code>, in a single state transition:</p>
<figure class="highlight clj"><table><tr><td class="code"><pre><span class="line">&#123;<span class="symbol">:f</span> <span class="symbol">:txn</span><span class="punctuation">,</span> <span class="symbol">:value</span> [[<span class="symbol">:read</span> <span class="string">&quot;a&quot;</span> <span class="number">2</span>] [<span class="symbol">:write</span> <span class="string">&quot;b&quot;</span> <span class="number">3</span>]]&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Invocation-amp-Completion-Times-调起时间和完成时间"><a href="#Invocation-amp-Completion-Times-调起时间和完成时间" class="headerlink" title="Invocation &amp; Completion Times 调起时间和完成时间"></a>Invocation &amp; Completion Times 调起时间和完成时间</h4><p>操作是需要时间的。比如在多线程系统中，一个函数调用就是一种操作；在分布式系统中，一个操作可能就是发送请求到服务节点，然后收到一个响应；</p>
<p>Operations, in general, take time. In a multithreaded program, an operation might be a function call. In distributed systems, an operation might mean sending a request to a server, and receiving a response.</p>
<p>为了对此进行建模，我们定义每个操作都有一个调起时间，完成时有一个更大的完成时间；这两个时间都由假想的、完全同步的全球可访问的时钟提供。我们认为这些时钟提供的是实际时间顺序（real-time order）的时钟；与之相反的就是只跟踪因果顺序的时钟；</p>
<p>To model this, we say that each operation has an <em>invocation time</em> and, should it complete, a strictly greater <em>completion time</em>, both given by an imaginary<a href="https://jepsen.io/consistency#fn-2">2</a>, perfectly synchronized, globally accessible clock.<a href="https://jepsen.io/consistency#fn-3">3</a> We refer to these clocks as providing a <em>real-time</em> order, as opposed to clocks that only track causal ordering.<a href="https://jepsen.io/consistency#fn-4">4</a></p>
<h4 id="Concurrency-并发"><a href="#Concurrency-并发" class="headerlink" title="Concurrency 并发"></a>Concurrency 并发</h4><p>因为操作需要耗时，所以两个并发的操作在时间上就有可能重叠。比如操作A和B，A先开始，然后B开始，然后A结束，之后B结束。只要他们在时间上有重叠，那我们说操作A和B是并发的。</p>
<p>Since operations take time, two operations might overlap in time. For instance, given two operations A and B, A could begin, B could begin, A could complete, and then B could complete. We say that two operations A and B are <em>concurrent</em> if there is some time during which A and B are both executing.</p>
<p>因为进程都是单线程的，所以这就表示同一进程执行的两个操作肯定不是并发的。</p>
<p>Processes are single-threaded, which implies that no two operations executed by the same process are ever concurrent.</p>
<h4 id="Crashes-崩溃"><a href="#Crashes-崩溃" class="headerlink" title="Crashes 崩溃"></a>Crashes 崩溃</h4><p>由于某种原因，操作可能无法完成（比如超时了，或者是组件崩溃了），所以该操作没有完成时间，因此认为该操作与之后调起的每个操作都是重叠的。</p>
<p>当进程某个操作处于这种状态时，就表示该进程卡住了，不能再次调用另一个操作。如果该进程又调起了新的操作，则这违反了进程同一时间只做一件事的约束。</p>
<p>If an operation does not complete for some reason (perhaps because it timed out or a critical component crashed) that operation <em>has no completion time</em>, and must, in general, be considered concurrent with every operation after its invocation. It may or may not execute.</p>
<p>A process with an operation is in this state is effectively stuck, and can never invoke another operation again. If it <em>were</em> to invoke another operation, it would violate our single-threaded constraint: processes only do one thing at a time.</p>
<h4 id="Histories-操作历史"><a href="#Histories-操作历史" class="headerlink" title="Histories 操作历史"></a>Histories 操作历史</h4><p>操作历史是操作的集合，同时还包括他们的并发结构（调用时间和完成时间）。</p>
<p>A <em>history</em> is a collection of operations, including their concurrent structure.</p>
<p><font color=red>一些论文将历史表示未操作的集合，并且每个操作附带两个数，表示它们的调起时间和结束时间。通过对比进程间的时间窗口来推断进程的并发结构。</font></p>
<p>Some papers represent this as a set of operations, where each operation includes two numbers, representing their invocation and completion time; concurrent structure is inferred by comparing the time windows between processes.</p>
<p>Jepsen represents a history as an ordered list of invocation and completion operations, effectively splitting each operation in two. This representation is more convenient for algorithms which iterate over the history, keeping a representation of concurrent operations and possible states.</p>
<h4 id="Consistency-Models-一致性模型"><a href="#Consistency-Models-一致性模型" class="headerlink" title="Consistency Models 一致性模型"></a>Consistency Models 一致性模型</h4><p>一致性模型就是历史的集合。我们使用一致性模型来定义系统的合法性。<font color=red>当我们说一个操作历史不符合序列化一致性时，就表示该操作历史不是序列性模型的历史集合中的成员；</font></p>
<p>A <em>consistency model</em> is a set of histories. We use consistency models to define which histories are “good”, or “legal” in a system. When we say a history “violates serializability” or “is not serializable”, we mean that the history is not in the set of serializable histories.</p>
<p>如果说一致性模型A意味着B，那就表示A是B的子集。比如线性一致性意味着顺序一致性，这是因为线性一致性的历史集合中的操作，肯定也存在于顺序一致性模型的历史集合中。这使得我们可以用层级结构来描述一致性模型间的关系。</p>
<p>We say that consistency model A implies model B if A is a subset of B. For example, linearizability implies sequential consistency because every history which is linearizable is also sequentially consistent. This allows us to relate consistency models in a hierarchy.</p>
<p>正式的说法就是：<font color=red>我们把更严格，集合更小的一致性模型成为更强的一致性模型；而集合更大，要求更宽松的一致性模型称为较弱的一致性模型；</font></p>
<p>Speaking informally, we refer to smaller, more restrictive consistency models as “stronger”, and larger, more permissive consistency models as “weaker”.</p>
<p>并非所有一致性模型都可以直接比较。通常，两个模型允许不同的行为，但都不包含另一个。</p>
<p>Not all consistency models are directly comparable. Often, two models allow different behavior, but neither contains the other.</p>
<hr>
<p>原文：<a href="https://jepsen.io/consistency">https://jepsen.io/consistency</a></p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>02Strict Serializability</title>
    <url>/2021/02/13/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/02Strict%20Serializability/</url>
    <content><![CDATA[<h3 id="Strict-Serializability-严格序列化"><a href="#Strict-Serializability-严格序列化" class="headerlink" title="Strict Serializability     严格序列化"></a>Strict Serializability     严格序列化</h3><p>不那么正式地说法，严格序列化（Strict Serializability，又称PL-SS、严格1SR、强1SR）意味着操作以跟这些操作的实时顺序（real-time ordering，即由前面”Invocation &amp; Completion Times”中提到的全局时钟保证的）一致的顺序进行执行。比如，如果操作A的完成时间比B的开始时间要早，那在序列化的顺序中，A要出现B的前面。</p>
<span id="more"></span>
<p>Informally, strict serializability (a.k.a. PL-SS, Strict 1SR, Strong 1SR) means that operations appear to have occurred in some order, consistent with the <a href="https://jepsen.io/consistency#invocation--completion-times">real-time ordering</a> of those operations; e.g. if operation A completes before operation B begins, then A should appear to precede B in the serialization order.</p>
<p><font color=red>严格序列化是一种“事务性”模型：操作（通常称为“事务”）包括几个按顺序执行的子操作。严格序列化保证操作（事务）以原子方式进行：即事务的子操作不会与其他事务的子操作交叉。</font></p>
<p>Strict serializability is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive operations performed in order. Strict serializability guarantees that operations take place <em>atomically</em>: a transaction’s sub-operations do not appear to interleave with sub-operations from other transactions.</p>
<p><font color=red>严格序列化还有一个“多对象”属性：操作针对的是系统中的多个对象。</font>事实上，严格序列化不仅适用于事务中涉及的特定对象，而且适用于整个系统。</p>
<p>It is also a <em>multi-object</em> property: operations can act on multiple objects in the system. Indeed, strict serializability applies not only to the <em>particular</em> objects involved in a transaction, but to <em>the system as a whole</em>–operations may act on predicates, like “the set of all cats”.</p>
<p>严格序列化性在网络分区的情况下，不是完全或粘性（sticky）可用的；在网络分区的情况下，部分或所有节点将无法继续执行操作。</p>
<p>Strict serializability cannot be totally or sticky available; in the event of a network partition, some or all nodes will be unable to make progress.</p>
<p><font color=red>严格序列化意味着序列化（serializability）和线性化（linearizability）。可以认为严格序列化，是在保证事务性多对象操作的序列化顺序的基础上，增加了线性化的实时顺序约束。或者可以这样认为，严格序列化的数据库就是把整个数据库视为一个对象的线性化系统。</font></p>
<p>Strict serializability implies <a href="https://jepsen.io/consistency/models/serializable">serializability</a> and <a href="https://jepsen.io/consistency/models/linearizable">linearizability</a>. You can think of strict serializability as serializability’s total order of transactional multi-object operations, <em>plus</em> linearizability’s real-time constraints. Alternatively, you can think of a strict serializable database as a linearizable object in which the object’s state is the entire database.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally 正式说法"></a><a href="https://jepsen.io/consistency/models/strict-serializable#formally">Formally</a> 正式说法</h4><p>Herlihy和Wing的论文《Linearizability: A Correctness Condition for Concurrent Objects》中介绍线性一致性时，他们将严格序列化定义为：<font color=red>具有实时顺序的序列化系统。</font></p>
<blockquote>
<p>如果操作历史相当于事务是按顺序执行的，即事务之间没有交叉，那就认为该操作历史是序列化的；</p>
<p>没有交叠的事务可以以显而易见的方式来定义他们间的（部分）优先顺序；</p>
<p>如果操作历史中的事务顺序与他们的优先顺序一致，那就认为操作历史是严格序列化的；</p>
</blockquote>
<p>这里所谓的“显而易见的方式”就是指，如果事务A在事务B开始之前就完成了，那操作历史中，事务A要在先于事务B；这就是线性一致性中的实时顺序约束。</p>
<p>When Herlihy and Wing <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">introduced linearizability</a>, they defined strict serializability in terms of a serializable system which is compatible with real-time order.</p>
<blockquote>
<p>A history is serializable if it is equivalent to one in which transactions appear to execute sequentially, i.e., without interleaving. A (partial) precedence order can be defined on non-overlapping pairs of transactions in the obvious way. A history is strictly serializable if the transactions’ order in the sequential history is compatible with their precedence order.</p>
</blockquote>
<p>“The obvious way” means that transaction A precedes transaction B if A completes before B begins; this is the real-time constraint from <a href="https://jepsen.io/consistency/models/linearizable">linearizability</a>.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>03Linearizability</title>
    <url>/2021/03/01/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/03Linearizability/</url>
    <content><![CDATA[<h3 id="Linearizability-线性化"><a href="#Linearizability-线性化" class="headerlink" title="Linearizability 线性化"></a>Linearizability 线性化</h3><p><font color=red>线性化是最强的<strong>单对象</strong>一致性模型之一</font>，它表示每个操作都是<font color=red>原子的，按照这些操作的实时顺序发生的</font>；比如：如果操作A在操作B调起之前完成，那操作B应该在操作A之后起作用；</p>
<span id="more"></span>
<p>Linearizability is one of the strongest single-object consistency models, and implies that every operation appears to take place atomically, in some order, consistent with the real-time ordering of those operations: e.g., if operation A completes before operation B begins, then B should logically take effect after A.</p>
<p>这种模型在有网络分区的情况下不是完全可用或粘性可用的；即在网络发生分区时，若干或者所有节点都无法再继续进行操作；</p>
<p>This model cannot be totally or sticky available; in the event of a network partition, some or all nodes will be unable to make progress.</p>
<p>线性化是一个<font color=red>单对象</font>模型，但这里的“一个对象”不同的系统有不同的范围定义。比如某些key-value系统在单个key上提供线性一致性；另一些系统则在一个表中的多个键上，或者是数据库中的多个表上提供一致性；</p>
<p>Linearizability is a single-object model, but the scope of “an object” varies. Some systems provide linearizability on individual keys in a key-value store; others might provide linearizable operations on multiple keys in a table, or multiple tables in a database—but not between different tables or databases, respectively.</p>
<p>如果需要在多个对象上进行线性化，则需要严格序列化（strict serializability）；如果无需实时顺序的约束，但是依然想要每个进程观察到相同的操作总序（total order），则需要顺序一致性；</p>
<p>When you need linearizability across multiple objects, try <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>. When <a href="https://jepsen.io/consistency#invocation--completion-times">real-time</a> constraints are not important, but you still want every process to observe the same total order, try <a href="https://jepsen.io/consistency/models/sequential">sequential consistency</a></p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally  正式说法"></a><a href="https://jepsen.io/consistency/models/linearizable#formally">Formally</a>  正式说法</h4><p>Herlihy 和 Wing在他们的论文《Linearizability: A Correctness Condition for Concurrent Objects》中引入了线性一致性的概念。（下面的话不好翻）如果并发系统中的操作历史H，等价于非并发系统中的操作历史S，并且H中的操作的部分实时顺序，与S中的操作顺序匹配的话，并且S中的操作顺序保留了对象的单线程语义，则认为H是线性一致性的。</p>
<p>Herlihy &amp; Wing introduced linearizability in their 1990 paper <a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">Linearizability: A Correctness Condition for Concurrent Objects</a>. Ignoring some bookeeping necessary to fill in the possible results of incomplete operations, a history H is one in which there is an equivalent sequential (e.g. non-concurrent) history S, and the partial real-time order of operations in H is consistent with the total order of S, and which preserves the objects’s single-threaded semantics.</p>
<p>Viotti and Vukolić重新强调了操作历史的三个理论上的约束：</p>
<ul>
<li>SingleOrder：所有操作间是全序关系（total order）；</li>
<li>RealTime：实时顺序约束；</li>
<li>RVal：关联对象类型上遵守单线程原则；</li>
</ul>
<p>Viotti and Vukolić <a href="https://arxiv.org/pdf/1512.00168.pdf">rephrase this definition</a> in terms of three set-theoretic constraints on histories:</p>
<ul>
<li>SingleOrder (there exists some total order of operations)</li>
<li>RealTime (consistent with the real time bound)</li>
<li>RVal (obeying the single-threaded laws of the associated object’s datatype)</li>
</ul>
<hr>
<p>《分布式系统与一致性》</p>
<p>在通常情况下，每个调用事件和返回事件都有确定的发生时间，我们按这个发生时间的顺序得到一个确定的历史H，如果恰巧出现了发生时间完全相同的两个事件怎么办？可以把这两个事件任意排列，谁先谁后都可以，这并不影响对线性一致性的判断。</p>
<p>线性一致性同样具有原子性。线性一致性也被称为原子一致性。</p>
<p>线性一致性要求保持偏序关系。保持偏序关系是一种更苛刻的要求，所以线性一致性是比顺序一致性更强的一种一致性。保持偏序关系给线性一致性赋予了顺序一致性所不具有的一个特性，那就是实时性。</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>04sequential</title>
    <url>/2021/04/02/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/04sequential/</url>
    <content><![CDATA[<h3 id="Sequential-Consistency-顺序一致性"><a href="#Sequential-Consistency-顺序一致性" class="headerlink" title="Sequential Consistency     顺序一致性"></a>Sequential Consistency     顺序一致性</h3><p>顺序一致性是并发系统的一个强大安全属性。非正式地说，顺序一致性意味着操作似乎以某种<font color=red>总的顺序（total order）</font>发生，并且该顺序与每个进程内的操作顺序一致（即保证总顺序中，<font color=red>单个进程内的操作还是按实时的顺序进行的</font>）。</p>
<span id="more"></span>
<p>Sequential consistency is a strong safety property for concurrent systems. Informally, sequential consistency implies that operations appear to take place in some total order, and that that order is consistent with the order of operations on each individual process.</p>
<p>这种模型在有网络分区的情况下不是完全可用或粘性可用的；即在网络发生分区时，若干或者所有节点都无法再继续取得进展；</p>
<p>Sequential consistency cannot be totally or sticky available; in the event of a network partition, some or all nodes will be unable to make progress.</p>
<p>顺序一致性系统中，一个进程可能远远领先于或落后于其他进程（即不保证进程间的顺序是符合实时顺序的）。例如，它们<font color=red>可以读取过期的状态</font>。然而，一旦进程A观察到进程B的某些操作，它就不可能再观察到B之前的状态。这种约束与total ordering属性相结合，使得顺序一致性也是一种强一致性模型。</p>
<p>A process in a sequentially consistent system may be far ahead, or behind, of other processes. For instance, they may read arbitrarily stale state. However, once a process A has observed some operation from process B, it can never observe a state <em>prior</em> to B. This, combined with the total ordering property, makes sequential consistency a surprisingly strong model for programmers.</p>
<p>如果想要实时顺序约束（比如想通过channel来告知其他进程某个事件，并且其他进程观察该事件），则需要线性一致性；如果需要完全可用，并且无需total order，则可以尝试因果一致性（causal consistency）；</p>
<p>When you need real-time constraints (e.g. you want to tell some other process about an event via a side channel, and have that process observe that event), try <a href="https://jepsen.io/consistency/models/linearizable">linearizability</a>. When you need total availability, and a total order isn’t required, try <a href="https://jepsen.io/consistency/models/causal">causal consistency</a>.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally  正式说法"></a><a href="https://jepsen.io/consistency/models/sequential#formally">Formally</a>  正式说法</h4><p>Leslie Lamport在论文《How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs》中定义了顺序一致性。他使用”顺序一致性”来表达：</p>
<blockquote>
<p><font color=red>操作的执行结果，就像是所有处理器的所有操作都按照某种总的顺序执行一样，并且从单独的处理器来看，每个处理器的操作都以其程序指定的顺序出现在该总顺序中。</font></p>
</blockquote>
<p>Leslie Lamport defined sequential consistency in his 1979 paper <a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/How-to-Make-a-Multiprocessor-Computer-That-Correctly-Executes-Multiprocess-Programs.pdf">How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs</a>. He uses “sequentially consistent” to imply…</p>
<blockquote>
<p>… the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.</p>
</blockquote>
<p>Viotti 和 Vukolić 将顺序一致性分解为三个属性：</p>
<ol>
<li>SingleOrder：操作有一个总的顺序（total order）</li>
<li>PRAM：由单个进程进行的多个写操作，会按照该进程执行它们的顺序被其他所有进程观察到；然而多个进程的写操作则可能以不同的顺序被外部观察到。</li>
<li>RVal：顺序必须与数据类型的语义一致</li>
</ol>
<p><a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a> decompose sequential consistency into three properties:</p>
<ol>
<li>SingleOrder (there exists some total order of operations)</li>
<li><a href="https://jepsen.io/consistency/models/pram">PRAM</a></li>
<li>RVal (the order must be consistent with the semantics of the datatype)</li>
</ol>
<hr>
<p>《分布式系统与一致性》中的资料：</p>
<p>Lamport在《How to make a multiprocessor computer that correctly executes multiprocess programs》中定义了顺序一致性：</p>
<ul>
<li>任意执行的结果和好像在处理器上执行的所有操作都按照某一种顺序排序执行的结果是一样的，并且每个处理器上的操作都会按照程序指定的顺序出现在操作序列中。</li>
<li>“The result of any execution is the same as if the operations of all the processors were executed in some sequential order,and the operations of each individual processor appear in this sequence in the order specified by its program.”</li>
</ul>
<p>另外，从全局视角一致的角度，Wiki中对Consistency Model有如下描述：</p>
<ul>
<li>“writes to variables by different processors have to be seen in the same order by all processors”（不同的处理器对变量的写操作从所有的处理器角度来看必须是相同的顺序）</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>05Causal Consistency</title>
    <url>/2022/04/13/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/05Causal%20Consistency/</url>
    <content><![CDATA[<h3 id="Causal-Consistency-因果一致性"><a href="#Causal-Consistency-因果一致性" class="headerlink" title="Causal Consistency     因果一致性"></a>Causal Consistency     因果一致性</h3><p><font color=red>因果一致性要求，因果相关的操作应该以相同的顺序被所有流程观察到，但是每个进程对于因果不相关的操作的顺序可能会有不同的看法。</font></p>
<span id="more"></span>
<p>Causal consistency captures the notion that causally-related operations should appear in the same order on all processes—though processes may disagree about the order of causally independent operations.</p>
<p><font color=red>举个例子</font>，假设用一个对象表示三人共用的聊天室，Attiya问了一句”shall we have lunch?”，Barbarella和Cyrus分别响应了”yes”和”no”。因果一致性允许Attiya观察到的顺序是：”lunch?”, “yes”, “no”；Barbarella观察到的顺序是：”lunch?”, “no”, “yes”；不会有人会观察到”yes”或”no”出现在”lunch?”的前面；</p>
<p>For example, consider a single object representing a chat between three people, where Attiya asks “shall we have lunch?”, and Barbarella &amp; Cyrus respond with “yes”, and “no”, respectively. Causal consistency allows Attiya to observe “lunch?”, “yes”, “no”; and Barbarella to observe “lunch?”, “no”, “yes”. However, no participant <em>ever</em> observes “yes” or “no” prior to the question “lunch?”.</p>
<p>收敛的因果系统要求系统中对象的值在相同的操作可见后收敛到相同的值。这样的系统中，用户可以短暂地观察到“lunch”, “yes”和“lunch”, “no”——但每个人最终都会达成一致（任选一个顺序）：”lunch?”, “yes”, “no”；</p>
<p><em>Convergent</em> causal systems require that the values of objects in the system converge to identical values, once the same operations are visible. In such a system, users could transiently observe “lunch”, “yes”; and “lunch”, “no”—but everyone would eventually agree on (to pick an arbitrary order) “lunch”, “yes”, “no”.</p>
<p><font color=red>因果一致性是粘性可用的</font>：即使发生了网络分区，只要client始终（stick to）访问的是同一个无故障节点，那该节点上肯定会有所进展。</p>
<p>Causal consistency is sticky available: even in the presence of network partitions, every client connected to a non-faulty node can make progress. However, clients must stick to the same server.</p>
<p>因果一致性的一个稍微强一点的版本，即实时因果一致性，被证明是始终可用的单向收敛系统中（an always-available, one-way convergent system）最强的一致性模型。大多数“因果一致性”系统实际上提供了这些更强的属性，例如RTC或causal+。</p>
<p>A slightly stronger version of causal consistency, <a href="http://www.cs.cornell.edu/lorenzo/papers/cac-tr.pdf">Real-Time Causal</a>, is proven to be the strongest consistency model in an always-available, one-way convergent system. Most “causally consistent” systems actually provide these stronger properties, such as RTC or <a href="https://www.cs.cmu.edu/~dga/papers/cops-sosp2011.pdf">causal+</a>.</p>
<p><font color=red>如果需要一个total order（因果一致性没有total order）</font>，并且愿意牺牲可用性和延迟的话，可以考虑顺序一致性。如果需要完全可用性，则只能放弃因果一致性（以及read-your-writes），不过依然可以使用writes follow reads，单调读和单调写。</p>
<p>When a total order is required, and you’re willing to sacrifice availability (and latency), consider <a href="https://jepsen.io/consistency/models/sequential">sequential consistency</a>. If you need total availability, you’ll have to give up causal (and read-your-writes), but can still obtain <a href="https://jepsen.io/consistency/models/writes-follow-reads">writes follow reads</a>, <a href="https://jepsen.io/consistency/models/monotonic-reads">monotonic reads</a>, and <a href="https://jepsen.io/consistency/models/monotonic-writes">monotonic writes</a>.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally 正式说法"></a><a href="https://jepsen.io/consistency/models/causal#formally">Formally</a> 正式说法</h4><p>因果内存系统首次来源于Lamport对<code>happens-before</code>关系的定义，它通过将一个操作与同一进程之前的操作，以及其他进程上的相关操作（通过消息交互使得操作的结果可见）关联起来，捕捉到了潜在因果关系的概念。</p>
<p>Causal memory stems from Lamport’s definition of the <a href="https://amturing.acm.org/p558-lamport.pdf">happens-before relation</a>, which captures the notion of potential causality by relating an operation to previous operations by the same process, and to operations on <em>other</em> processes whose effects could have been visible thanks to messages exchanged between those processes.</p>
<p>这一模型在Ahamad, Neiger, Burns等人的论文中以多处理器内存模型的形式出现。不过一个更有用的参考可能是Mahajan、Alvisi和Dahlin的《Consistency、Availability and Convergence》中提到的操作历史，它由一系列的读写操作组成，操作间的<code>happen-before</code>关系构成一个有向无环图，并且happend-before需要：</p>
<ol>
<li>反映每个进程（“节点”）上的序列顺序，以及</li>
<li>每个读操作需要返回最近的并发写操作的结果；</li>
</ol>
<p>This was formalized into a multiprocessor memory model in a <a href="https://link.springer.com/article/10.1007%2FBF01784241">1993 paper</a> by Ahamad, Neiger, Burns, et al, but a more useful (and non-paywalled) reference might be Mahajan, Alvisi, and Dahlin’s <a href="http://www.cs.cornell.edu/lorenzo/papers/cac-tr.pdf">Consistency, Availability, and Convergence</a>, which takes a history (called “an execution”) composed of read and write operations, asks for a directed acyclic happens-before relation over that execution, and requires that happens-before…</p>
<ol>
<li>Reflects the serial ordering on each process (“node”), and</li>
<li>Each read returns the latest preceding concurrent writes</li>
</ol>
<p>Mahajan等人将冲突解决（如何处理并发写入）留给实现来决定。</p>
<p>Mahajan et al leave conflict resolution (what to do with concurrent writes) up to the implementation, requiring only that all concurrent writes are returned for a read.</p>
<p>一个更简洁的定义，来自于Viotti and Vukolić，将因果一致性分解为三个属性：</p>
<ol>
<li>因果可视性：happends-before关系是可见顺序的子集；</li>
<li>因果冲突：happends-before关系是仲裁顺序的子集，所谓仲裁顺序，就是一个定义了如何解决冲突的total order；</li>
<li>RVal：返回值应与数据类型的定义一致；例如，读操作应能反映出最近的写操作；</li>
</ol>
<p>A more concise definition, from <a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a>, decomposes causal consistency into three properties:</p>
<ol>
<li>CausalVisibility (the happens-before relation is a subset of the visibility order)</li>
<li>CausalArbitration (the happens-before relation is a subset of the <em>arbitration order</em>: a total order which defines how conflicts are resolved)</li>
<li>RVal (return values should be consistent with the definition of the datatype; e.g. reads should reflect recent writes)</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>06PRAM</title>
    <url>/2022/04/19/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/06PRAM/</url>
    <content><![CDATA[<h3 id="PRAM"><a href="#PRAM" class="headerlink" title="PRAM"></a>PRAM</h3><p>PRAM（流水线随机访问存储系统，Pipeline Random Access Memory）这个概念来源于Lipton &amp; Sandberg的论文《PRAM: A Scalable Shared Memory》，这篇论文尝试对现有的一致性内存模型进行relax，以期获得更好的并发性和性能。这种一致性模型的约束是：<font color=red>由单个进程进行的多个写操作，会按照该进程执行它们的顺序被其他所有进程观察到；然而多个进程的写操作则可能以不同的顺序被外部观察到。</font></p>
<span id="more"></span>
<p>PRAM (Pipeline Random Access Memory) comes from Lipton &amp; Sandberg’s 1988 paper <a href="ftp://ftp.cs.princeton.edu/techreports/1988/180.pdf">PRAM: A Scalable Shared Memory</a>, which attempts to <em>relax</em> existing coherent memory models to obtain better concurrency (and therefore performance). It enforces that any pair of writes executed by a single process are observed (everywhere) in the order the process executed them; however, writes from different processes may be observed in different orders.</p>
<p>PRAM实际上等同于read your writes，单调写和单调读；</p>
<p>PRAM is exactly equivalent to <a href="https://jepsen.io/consistency/models/read-your-writes">read your writes</a>, <a href="https://jepsen.io/consistency/models/monotonic-writes">monotonic writes</a>, and <a href="https://jepsen.io/consistency/models/monotonic-reads">monotonic reads</a>.</p>
<p>PRAM是粘性可用的：在网络分区的情况下，只要client始终访问的是同一个节点，那所有进程都可以取得进展。</p>
<p>PRAM is sticky available: in the event of a network partition, all processes can make progress so long as clients always stick to the same server.</p>
<p>如果需要更严格的，保证write follow reads的一致性模型，则可以尝试因果一致性，它同样是粘性可用的，并且提供了更直观的语义；如果需要完全可用，则考虑牺牲掉 read your writes特性，只选择单调读+单调写；</p>
<p>For a more strict consistency model which also enforces that <a href="https://jepsen.io/consistency/models/writes-follow-reads">writes follow reads</a>, try <a href="https://jepsen.io/consistency/models/causal">causal consistency</a>: it’s just as available, and provides more intuitive semantics. If you need total availability, consider sacrificing <a href="https://jepsen.io/consistency/models/read-your-writes">read your writes</a> and choosing just <a href="https://jepsen.io/consistency/models/monotonic-reads">monotonic reads</a> + <a href="https://jepsen.io/consistency/models/monotonic-writes">monotonic writes</a>.</p>
<h4 id="Formally-正式定义"><a href="#Formally-正式定义" class="headerlink" title="Formally  正式定义"></a><a href="https://jepsen.io/consistency/models/pram#formally">Formally</a>  正式定义</h4><p>Lipton 和 Sandberg对PRAM的定义被表达为一种实现，而且其分析侧重于连续内存相关的性能。一个更抽象的定义来自Viotti 和 Vukolić：<font color=red>如果会话顺序（每个进程上的操作顺序）是可见性顺序（给定操作可见的操作）的子集，则满足PRAM要求。</font></p>
<p>Lipton &amp; Sandberg’s <a href="ftp://ftp.cs.princeton.edu/techreports/1988/180.pdf">definition</a> is phrased as an implementation, and its analysis focuses on performance relative to coherent memory. A more abstract definition comes from <a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a>, where PRAM is satisfied if the session order (the order of operations on each process) is a subset of the visibility order (what operations are visible to a given operation).</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>07Read Your Writes</title>
    <url>/2022/04/25/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/07Read%20Your%20Writes/</url>
    <content><![CDATA[<h3 id="Read-Your-Writes"><a href="#Read-Your-Writes" class="headerlink" title="Read Your Writes"></a>Read Your Writes</h3><p>read your writes，或者叫read my writes保证这样的规则：如果一个进程执行了写操作w，然后该进程又执行了读操作r，那r必须能够看到w的结果；</p>
<span id="more"></span>
<p>注意：read your writes不是应用于不同进程上的规则。比如它不保证，如果进程1写成功了，那进程2一定能观察到写操作的结果；</p>
<p>read your writes是粘性可用的：如果发生了网络分区，那只要CLIENT始终访问的是同一个节点，那该节点依然可以取得进展；</p>
<p><em>Read your writes</em>, also known as <em>read my writes</em>, requires  that if a process performs a write <em>w</em>, then that same process performs a  subsequent read <em>r</em>, then <em>r</em> must observe <em>w</em>’s  effects.</p>
<p>Note that read your writes does not apply to operations performed by  <em>different</em> processes. There is no guarantee, for instance, that if  process 1 writes a value successfully, that process 2 will subsequently observe  that write.</p>
<p>Read your writes is sticky available: if a network partition occurs, every  node can make progress, so long as clients never change which server they talk  to.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally  正式说法"></a><a href="https://jepsen.io/consistency/models/read-your-writes#formally">Formally</a>  正式说法</h4><p>Viotti 和 Vukolić在论文中定义了read your writes：对于任意的写操作和读操作而言，如果对于特定进程而言，其执行的写操作在读操作之前完成，那写操作的结果必须是对该读操作而言是可见的。换句话说，会话顺序（仅限于相同进程中的先写后读），必须是可见顺序的子集；</p>
<p><a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a> define  read your writes straightforwardly: for any write, and any read, if the write  falls before the read in a given session (process), then the write must be  visible to the read. In other terms, the session order (restricted to just  writes -&gt; reads), is a subset of the visibility order.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>08Writes Follow Reads</title>
    <url>/2022/05/03/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/08Writes%20Follow%20Reads/</url>
    <content><![CDATA[<h3 id="Writes-Follow-Reads"><a href="#Writes-Follow-Reads" class="headerlink" title="Writes Follow Reads"></a>Writes Follow Reads</h3><p>writes follow reads：也叫做会话因果关系，保证这样的规则：如果一个进程读到了v，v来自于写操作w1，然后该进程执行写操作w2，那w2必须在w1之后可见。Once you’ve read something, you can’t change that read’s past.</p>
<span id="more"></span>
<p><em>Writes follow reads</em>, also known as <em>session causality</em>, ensures that if a process reads a value <em>v</em>, which came from a write <em>w1</em>, and later performs write <em>w2</em>, then <em>w2</em> must be visible after <em>w1</em>. Once you’ve read something, you can’t change that read’s past.</p>
<p>writes follow reads是完全可用的，即使发生网络分区，那每个节点依然可以取得进展；</p>
<p>Writes follow reads is a totally available property. Every node can make progress regardless of network partitions.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally    正式说法"></a><a href="https://jepsen.io/consistency/models/writes-follow-reads#formally">Formally</a>    正式说法</h4><p>Viotti 和 Vukolić在论文中，以可见顺序（哪个写操作的结果被哪个读操作见到）、会话顺序（相同进程执行的操作）和仲裁顺序（本质上就是并发进行的操作中，哪个操作先执行）的概念定义了writes follow reads规则。writes follow reads就是：可见顺序和会话顺序，仅限于读写操作之间，必须是仲裁顺序的子集。</p>
<p><a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a> define writes follow reads in terms of the visibility order (which effects are visible to which reads), the session order (the order of operations on each process) and the arbitration order (essentially, which concurrent updates take precedence): the visibility and session orders, restricted to read&#x2F;write pairs, must be a subset of the arbitration order.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>09Monotonic Reads</title>
    <url>/2022/05/14/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/09Monotonic%20Reads/</url>
    <content><![CDATA[<h3 id="Monotonic-Reads-单调读"><a href="#Monotonic-Reads-单调读" class="headerlink" title="Monotonic Reads 单调读"></a>Monotonic Reads 单调读</h3><p>单调读的规则：如果一个进程先执行读操作r1，然后是r2，那r2读取到的值，不能是r1所反映出的写操作之前的状态；直白点说就是，读操作不能倒退；</p>
<span id="more"></span>
<p>单调读的规则适用于相同进程的读操作，而不适用于不同进程的操作；</p>
<p>单调读可以是完全可用的：即使发生了网络分区，那所有节点还是可以取得进展；</p>
<p><em>Monotonic reads</em> ensures that if a process performs read <em>r1</em>, then <em>r2</em>, then <em>r2</em> cannot observe a state prior to the writes which were reflected in <em>r1</em>; intuitively, reads cannot go backwards.</p>
<p>Monotonic reads does not apply to operations performed by <em>different</em> processes, only reads by the same process.</p>
<p>Monotonic reads can be totally available: even during a network partition, all nodes can make progress.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally  正式说法"></a><a href="https://jepsen.io/consistency/models/monotonic-reads#formally">Formally</a>  正式说法</h4><p>Viotti 和 Vukolić在论文中，以会话顺序（相同进程执行的操作）和仲裁顺序（本质上就是哪个操作先执行）的概念定义了单调读规则。单调读就是：对于操作a、b和c，其中a是写操作，b和c是读操作；b在c之前执行，并且都是由相同的进程执行的，那如果b见到了a的结果，那c也必须能见到a的结果；</p>
<p><a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a> define monotonic reads in terms of a session order (the order of operations performed by the same process) and a visibility order (which writes are visible to which reads). For all operations <em>a</em>, <em>b</em>, and <em>c</em>, where <em>b</em> and <em>c</em> are reads, if a is visible to b, and b is executed prior to, and by the same process as c, then a must be visible to c.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>10Monotonic Writes</title>
    <url>/2022/05/17/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/10Monotonic%20Writes/</url>
    <content><![CDATA[<h3 id="Monotonic-Writes-单调写"><a href="#Monotonic-Writes-单调写" class="headerlink" title="Monotonic Writes 单调写"></a>Monotonic Writes 单调写</h3><p>单调写保证这样的场景：一个进程先进行了写操作w1，然后是写操作w2，那其他所有进程都是先观察到w1，然后是w2；</p>
<span id="more"></span>
<p>单调写规则只适用于相同进程的写操作，而非不同进程的写操作；</p>
<p>单调写可以是完全可用的：即使发生了网络分区，所有节点都可以取得进展；</p>
<p><em>Monotonic writes</em> ensures that if a process performs write <em>w1</em>, then <em>w2</em>, then all processes observe <em>w1</em> before <em>w2</em>.</p>
<p>Monotonic writes does not apply to operations performed by <em>different</em> processes, only writes by the same process.</p>
<p>Monotonic writes can be totally available: even during a network partition, all nodes can make progress.</p>
<h4 id="Formally-正式说法"><a href="#Formally-正式说法" class="headerlink" title="Formally  正式说法"></a><a href="https://jepsen.io/consistency/models/monotonic-writes#formally">Formally</a>  正式说法</h4><p>Viotti 和 Vukolić在论文中，以会话顺序（相同进程执行的操作）和仲裁顺序（本质上就是哪个操作先执行）的概念定义了单调写规则。单调写就是：会话顺序，仅限于写操作之间，必须是仲裁顺序的子集。</p>
<p><a href="https://arxiv.org/pdf/1512.00168.pdf">Viotti and Vukolić</a> define monotonic writes in terms of a session order (the order of operations performed by the same process) and an arbitration order (essentially, which operations take precedence). The session order, constrained to only pairs of writes, must be a subset of the arbitration order.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>11Serializability</title>
    <url>/2022/05/23/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/11Serializability/</url>
    <content><![CDATA[<h3 id="Serializability"><a href="#Serializability" class="headerlink" title="Serializability"></a>Serializability</h3><p>非正式地说，序列化意味着事务似乎以某种总的顺序（total order, 全序?）发生。</p>
<p>Informally, serializability means that transactions appear to have occurred in some total order.</p>
<span id="more"></span>
<p>序列化是一种“事务”模型：操作（通常称为“事务”）包括按顺序执行的几个基本子操作。序列化保证事务以原子的方式进行：事务的子操作不会与其他事务的子操作交错执行。</p>
<p>Serializability is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. Serializability guarantees that operations take place <em>atomically</em>: a transaction’s sub-operations do not appear to interleave with sub-operations from other transactions.</p>
<p>序列化也是一个“多对象”属性：操作可以作用于系统中的多个对象上。实际上，序列化不仅作用于事务中涉及的特定对象，而且可以把整个系统当成一个整体—-操作可以作用于谓词之上（<em>比如where</em>），比如所有的cat。</p>
<p>适用于<em>整个系统</em>-操作可能作用于谓词，如“所有cat的集合”。</p>
<p>It is also a <em>multi-object</em> property: operations can act on multiple objects in the system. Indeed, serializability applies not only to the <em>particular</em> objects involved in a transaction, but to <em>the system as a whole</em>—operations may act on predicates, like “the set of all cats”.</p>
<p>序列化无法达到完全或者粘性可用。如果发生了网络分区，那部分或所有节点将无法继续执行操作。</p>
<p>Serializability cannot be totally or sticky available; in the event of a network partition, some or all nodes will be unable to make progress.</p>
<p>序列化意味着可重复读，快照隔离等。但是它并在real-time上并无约束，即使在单进程上也没有。如果进程A完成了写操作w，然后进程B开始读操作r，那r不一定读到w的结果。如果想要real-time上的约束，则需要强序列化。</p>
<p>Serializability implies <a href="https://jepsen.io/consistency/models/repeatable-read">repeatable read</a>, <a href="https://jepsen.io/consistency/models/snapshot-isolation">snapshot isolation</a>, etc. However, it does not impose any real-time, or even per-process constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. For those kinds of real-time guarantees, see <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializable</a>.</p>
<p>进一步而言，序列化也不保证单个进程上的事务间的顺序。一个进程可能先观察到一个写操作，然后在后续的事务中再次观察同一个写操作时却也可能失败。实际上，如果写操作发生在不同的事务上，那进程可能无法正确观察到自己之前的写操作。</p>
<p>Moreover, serializability does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.</p>
<p>序列化对于总的顺序有强约束，但是它允许乱序。比如，一个序列化的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</p>
<p>The requirement for a total order of transactions is strong—but still allows pathological orderings. For instance, a serializable database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very <em>end</em> of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations don’t seem to take advantage of these optimization opportunities.</p>
<h4 id="Formally"><a href="#Formally" class="headerlink" title="Formally"></a>Formally</h4><p>The <a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999 spec</a> says:</p>
<blockquote>
<p>隔离级别在SERIALIZABLE时，并行执行的事务保证是串行化的。串行化执行的定义，就是并行执行的事务产生的结果，与那些相同事务串行执行的结果是相同的。串行执行就是下一个事务在上一个事务执行完成之后才能执行。</p>
</blockquote>
<blockquote>
<p>The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.</p>
</blockquote>
<p>然后接着用禁止的异常情况来定义其隔离级别：序列化是读已提交的，但是不会出现P3（幻读）现象：</p>
<blockquote>
<p>P3（“幻读”）：事务T1读取满足某些搜索条件的行。事务T2执行SQL语句，这些语句会生成满足事务T1使用的搜索条件的一行或多行。如果事务T1随后以相同的搜索条件重复再次读取，它将获得不同的行集合。</p>
</blockquote>
<p>… and goes on to define its isolation levels in terms of proscribed anomalies: serializable is <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>, but without phenomenon P3:</p>
<blockquote>
<p>P3 (“Phantom”): SQL-transaction T1 reads the set of rows N that satisfy some <search condition>. SQL-transaction T2 then executes SQL-statements that generate one or more rows that satisfy the <search condition> used by SQL-transaction T1. If SQL-transaction T1 then repeats the initial read with the same <search condition>, it obtains a different collection of rows.</p>
</blockquote>
<p>然而，正如<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson、Bernstein等人观察到的</a>，ANSI规范有多种解释，其中一种解释（“异常解释”）允许非序列化历史。</p>
<p>However, as <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson, Bernstein, et al observed</a>, the ANSI specification allows multiple intepretations, and one of those interpretations (the “anomaly interpretation”) admits nonserializable histories.</p>
<p><a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya对事务隔离级别的正式定义</a>对ANSI隔离级别的预防性解释进行了更全面的总结，其将序列化定义为没有四种异常现象。即序列化禁止：</p>
<ul>
<li>P0（脏写）：w1（x）…w2（x）</li>
<li>P1（脏读）：w1（x）…r2（x）</li>
<li>P2（不可重复读）：r1（x）…w2（x）</li>
<li>P3（幻读）：r1（P）…w2（P中的y）</li>
</ul>
<p>这里w表示写，r表示读，下标表示执行操作的事务。…表示除了commit或abort之外的子操作。p表示谓词。</p>
<p><a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s formalization of transactional isolation levels</a> provides a more thorough summary of the preventative interpretation of the ANSI levels, defining serializability as the absence of four phenomena. Serializability prohibits:</p>
<ul>
<li>P0 (Dirty Write): w1(x) … w2(x)</li>
<li>P1 (Dirty Read): w1(x) … r2(x)</li>
<li>P2 (Fuzzy Read): r1(x) … w2(x)</li>
<li>P3 (Phantom): r1(P) … w2(y in P)</li>
</ul>
<p>Here <em>w</em> denotes a write, <em>r</em> denotes a read, and subscripts indicate the transaction which executed that operation. The notation “…” indicates a series of micro-operations <em>except</em> for a commit or abort. P indicates a predicate.</p>
<p>正如Adya所指出的，ANSI规范的预防性解释是“过度限制的”：它排除了一些满足序列化的操作历史。</p>
<p>As Adya notes, the preventative interpretation of the ANSI specification is <em>overly restrictive</em>: it rules out some histories which are legally serializable.</p>
<p>对于序列化，基于抽象执行的合理直观的正式定义，可以参阅Cerone、Bernardi和Gotsman的<a href="http://drops.dagstuhl.de/opus/volltexte/2015/5375/pdf/15.pdf">具有原子可见性的事务一致性模型框架</a>，它将可序列化性指定为三个属性的组合：</p>
<ul>
<li>内部一致性：在事务内，读操作观察该事务最近的写操作（如果有）</li>
<li>外部一致性：事务T1中的读操作（该事务中之前没有写操作）必须观察事务T0中写操作的结果，即T0对T1是可见的，并且除了T0外没有其他最近写入该对象的事务。</li>
<li>完全可见性：可见性关系必须是全序的（total order）。</li>
</ul>
<p>有关基于状态的正式定义，请参阅Crooks、Pu、Alvisi和Clement：<a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seing-tr.pdf">眼见为实：以Client为中心的数据库隔离规范</a></p>
<p>For a reasonably intuitive formalization based on abstract executions, see Cerone, Bernardi, &amp; Gotsman’s <a href="http://drops.dagstuhl.de/opus/volltexte/2015/5375/pdf/15.pdf">A Framework for Transactional Consistency Models with Atomic Visibility</a>, which specifies serializability as a combination of three properties:</p>
<ul>
<li>Internal consistency: within a transaction, reads observe that transaction’s most recent writes (if any)</li>
<li>External consistency: reads <em>without</em> a preceding write in transaction T1 must observe the state written by a transaction T0, such that T0 is visible to T1, and no more recent transaction wrote to that object.</li>
<li>Total visibility: the visibility relationship must be a <em>total</em> order.</li>
</ul>
<p>For a state-based formalization, see Crooks, Pu, Alvisi, &amp; Clement: <a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seeing-tr.pdf">Seeing is Believing: A Client-Centric Specification of Database Isolation</a>.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>12Repeatable Read</title>
    <url>/2022/05/27/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/12Repeatable%20Read/</url>
    <content><![CDATA[<h3 id="Repeatable-Read"><a href="#Repeatable-Read" class="headerlink" title="Repeatable Read"></a>Repeatable Read</h3><p>可重复读类似于序列化，但是与之不同的是，它允许幻读：事务T1使用谓词进行读取，比如“所有名为Dikembe”的人员集合，另一个事务T2在T1提交之前，新增了或者修改了名为”Dikembe”的人员信息。虽然单个对象在读取时是稳定的，但谓词本身可能不是。</p>
<span id="more"></span>
<p>Repeatable read is closely related to <a href="https://jepsen.io/consistency/models/serializable">serializability</a>, but unlike serializable, it allows <a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">phantoms</a>: if a transaction T1 reads a predicate, like “the set of all people with the name “Dikembe”, then another transaction T2 may create or modify a person with the name “Dikembe” before T1 commits. Individual <em>objects</em> are stable once read, but the <em>predicate itself</em> may not be.</p>
<p>可重复读是一种“事务”模型：操作（通常称为“事务”）可以包括按顺序执行的几个子操作。它也是一个“多对象”属性：操作可以作用于系统中的多个对象。</p>
<p>Repeatable read is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. It is also a <em>multi-object</em> property: operations can act on multiple objects in the system.</p>
<p>可重复读在网络分区的场景下也不是完全可用；在存在网络分区的情况下，一些或所有节点可能无法取得进展。为了获得完全可用性，需要以允许不可重复读为代价，考虑<a href="https://jepsen.io/consistency/models/read-committed">读已提交</a>.</p>
<p>Repeatable read cannot be totally available; in the presence of network partitions, some or all nodes may be unable to make progress. For total availability, at the cost of allowing fuzzy reads, consider <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>.</p>
<p>可重复读取意味着<a href="https://jepsen.io/consistency/models/cursor-stability">游标稳定性</a>，<a href="https://jepsen.io/consistency/models/read-committed">读已提交</a>等。</p>
<p>Repeatable read implies <a href="https://jepsen.io/consistency/models/cursor-stability">cursor stability</a>, <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>, etc.</p>
<p>需要注意的是，可重复读不会有实际时间上的约束。如果进程A完成了写操作w，然后进程B开始读操作r，r不一定能观察到w。如果想要事务模型有实际时间上的约束，考虑<a href="https://jepsen.io/consistency/models/strict-serializable">严格序列化</a>。</p>
<p>Note that repeatable read does not impose any real-time constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. For a transactional model that provides real-time constraints, consider <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>.</p>
<p>此外，可重复读也不需要保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</p>
<p>Moreover, repeatable read does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.</p>
<p>像串行化一样，可重复读允许重排序。比如，一个可重复读的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</p>
<p>Like <a href="https://jepsen.io/consistency/models/serializable">serializability</a>, repeatable read allows pathological orderings. For instance, a repeatable-read database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very <em>end</em> of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations don’t seem to take advantage of these optimization opportunities.</p>
<h4 id="Formally-正式定义"><a href="#Formally-正式定义" class="headerlink" title="Formally 正式定义"></a>Formally 正式定义</h4><p><a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999规范</a>将可重复读取指定为<a href="https://jepsen.io/consistency/models/read-committed">读已提交</a>但不允许现象P2：</p>
<blockquote>
<p>P2（“不可重复读取”）：SQL事务T1读取一行。然后，SQL事务T2修改或删除该行并执行提交。如果T1随后尝试重新读取该行，它可能会读到修改后的值或发现该行已被删除。</p>
</blockquote>
<p>The <a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999 spec</a> specifies repeatable read as <a href="https://jepsen.io/consistency/models/read-committed">read committed</a> but disallowing phenomenon P2:</p>
<blockquote>
<p>P2 (“Non-repeatable read”): SQL-transaction T1 reads a row. SQL-transaction T2 then modifies or deletes that row and performs a COMMIT. If T1 then attempts to reread the row, it may receive the modified value or discover that the row has been deleted.</p>
</blockquote>
<p>然而，正如<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson、Bernstein等人观察到的</a>，ANSI规范允许有多种解释，其中一种解释（“异常解释”）仍然允许“可串行化”系统的非串行化历史。相反，我们更喜欢<a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya对事务隔离级别的形式化</a>，它提供了预防性解释的简明定义。在此模型中，可重复读取禁止：</p>
<ul>
<li>P0（脏写）：w1（x）…w2（x）</li>
<li>P1（脏读）：w1（x）…r2（x）</li>
<li>P2（不可重复读）：r1（x）…w2（x）</li>
</ul>
<p>但允许：</p>
<ul>
<li>P3（幻读）：r1（P）…w2（P中的y）</li>
</ul>
<p>这里w表示写，r表示读，下标表示执行操作的事务。”…”表示除了commit或abort之外的子操作。p表示谓词。请注意，与ANSI SQL规范不同，T1不一定执行第二次读取；对谓词的修改就足够了。</p>
<p>However, as <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson, Bernstein, et al observed</a>, the ANSI specification allows multiple intepretations, and one of those interpretations (the “anomaly interpretation) still admits nonserializable histories for “serializable” systems. Instead, we prefer <a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s formalization of transactional isolation levels</a>, which provides a concise definition of the preventative interpretation. In this model, repeatable read prohibits:</p>
<ul>
<li>P0 (Dirty Write): w1(x) … w2(x)</li>
<li>P1 (Dirty Read): w1(x) … r2(x)</li>
<li>P2 (Fuzzy Read): r1(x) … w2(x)</li>
</ul>
<p>but allows:</p>
<ul>
<li>P3 (Phantom): r1(P) … w2(y in P)</li>
</ul>
<p>Here <em>w</em> denotes a write, <em>r</em> denotes a read, and subscripts indicate the transaction which executed that operation. The notation “…” indicates a series of micro-operations <em>except</em> for a commit or abort. P indicates a predicate. Note that unlike the ANSI SQL spec, T1 does not necessarily execute a second read; modification of the predicate is enough.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>13Snapshot Isolation</title>
    <url>/2022/05/29/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/13Snapshot%20Isolation/</url>
    <content><![CDATA[<h3 id="Snapshot-Isolation-快照隔离"><a href="#Snapshot-Isolation-快照隔离" class="headerlink" title="Snapshot Isolation     快照隔离"></a>Snapshot Isolation     快照隔离</h3><p>在快照隔离的系统中，每个事务像是在独立且一致的数据库快照上进行操作。在提交之前，其更改仅对本事务可见，提交后，该事务的所有修改对其之后开始的事务才是原子可见的。如果事务T1修改了对象x，而另一个事务T2在T1的快照开始之后、T1提交之前提交了对<em>x</em>的写入，则T1必须中止。</p>
<span id="more"></span>
<p>In a snapshot isolated system, each transaction appears to operate on an independent, consistent <em>snapshot</em> of the database. Its changes are visible only to that transaction until commit time, when all changes become visible atomically to any transaction which begins at a later time. If transaction T1 has modified an object <em>x</em>, and another transaction T2 committed a write to <em>x</em> after T1’s snapshot began, and before T1’s commit, then T1 must abort.</p>
<p>快照隔离是一种“事务性”模型：操作（通常称为“事务”）包括几个按顺序执行的基本子操作；它也是一个“多对象”属性：操作可以作用于系统中的多个对象。</p>
<p>Snapshot isolation is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. It is also a <em>multi-object</em> property: operations can act on multiple objects in the system.</p>
<p>快照隔离不能完全可用；存在网络分区的情况下，一些或所有节点可能无法取得进展。如果需要完全可用性，以允许长分叉异常为代价，可以考虑<a href="http://www.news.cs.nyu.edu/~%E9%87%91%E9%98%B3/%E9%85%92%E5%90%A7/%E6%B2%83%E5%B0%94%E7%89%B9-sosp11.pdf">并行快照隔离</a>，或者是更弱一些的，但是更广泛支持的<a href="https://jepsen.io/consistency/models/read-committed">读已提交</a>。</p>
<p>Snapshot isolation cannot be totally available; in the presence of network partitions, some or all nodes may be unable to make progress. For total availability, at the cost of allowing long-fork anomalies, consider <a href="http://www.news.cs.nyu.edu/~jinyang/pub/walter-sosp11.pdf">parallel snapshot isolation</a>, or (weaker, but more commonly supported) <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>.</p>
<p>与序列化要求事务之间的全序关系不同，快照隔离仅需要事务是偏序的：一个事务中的子操作可以与其他事务中的子操作交错执行。快照隔离允许的最显著现象是<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">写倾斜</a>，它允许事务读取重叠状态，修改不相交的对象集后提交；以及允许<a href="https://www.cs.umb.edu/~poneil/ROAnom.pdf">只读事务异常</a>，涉及部分不相交的写入集。</p>
<p>Unlike <a href="https://jepsen.io/consistency/models/serializable">serializability</a>, which enforces a total order of transactions, snapshot isolation only forces a <em>partial</em> order: sub-operations in one transaction may interleave with those from other transactions. The most notable phenomena allowed by snapshot isolation are <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">write skews</a>, which allow transactions to read overlapping state, modify disjoint sets of objects, then commit; and <a href="https://www.cs.umb.edu/~poneil/ROAnom.pdf">a read-only transaction anomaly</a>, involving partially disjoint write sets.</p>
<p>快照隔离意味着读已提交。但是它没有任何实际时间上的约束。如果进程A完成了写操作w，然后进程B开始读操作r，r不一定能观察到w。一些数据库提供快照隔离变体来支持实际时间上的约束。类似于<a href="https://jepsen.io/consistency/models/strict-serializable">严格序列化</a>，它提供了全序关系以及实际时间上的保证。</p>
<p>Snapshot isolation implies <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>. However, it does not impose any real-time constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. Some databases provide real-time variants of snapshot isolation. Compare with <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>, which provides a total order <em>and</em> real-time guarantees.</p>
<p>此外，快照隔离也不需要保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。如果需要强制同一进程中的事务按顺序执行，请考虑<a href="http://www.inf.usi.ch/faculty/pedone/Paper/2004/IC_TECH_REPORT_200421.pdf">前缀一致快照隔离</a>.</p>
<p>Moreover, snapshot isolation does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions. To enforce that transactions from the same process appear to execute in order, consider <a href="http://www.inf.usi.ch/faculty/pedone/Paper/2004/IC_TECH_REPORT_200421.pdf">prefix-consistent snapshot isolation</a>.</p>
<p>在其广义形式中，快照隔离允许重排序。比如，一个快照隔离的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</p>
<p>In its generalized form, snapshot isolation allows pathological orderings. For instance, a snapshot isolated database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very <em>end</em> of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations don’t seem to take advantage of these optimization opportunities.</p>
<h4 id="A-Range-of-Snapshot-Isolations-一系列快照隔离"><a href="#A-Range-of-Snapshot-Isolations-一系列快照隔离" class="headerlink" title="A Range of Snapshot Isolations     一系列快照隔离"></a><a href="https://jepsen.io/consistency/models/snapshot-isolation#a-range-of-snapshot-isolations">A Range of Snapshot Isolations</a>     一系列快照隔离</h4><p>不同的论文对快照隔离允许的操作历史的限制程度各不相同。<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson等人的原始论文</a>对于提交和开始时间戳与挂钟时间之间的关系，以及<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.1363&amp;rep=rep1&amp;type=pdf">Daudjee&amp;Salem</a>的后续工作建立了“经典&#x2F;强”和“广义&#x2F;弱”的快照隔离级别（SI levels），有些含糊不清。<a href="https://pmg.csail.mit.edu/papers/adya-phd.pdf">阿德亚</a>对SI正式定义假设我们将调度程序所做的特定选择视为给定：操作历史是否是SI的取决于调度程序所做的操作。这就会根据调度器的实现方式产生一系列可能的快照隔离。</p>
<p>Papers vary in how strictly they constrain snapshot isolation’s allowed histories. The original paper by <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson et al</a> is somewhat ambiguous about the relationship between commit and start timestamps and wall-clock time, and subsequent work by <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.1363&rep=rep1&type=pdf">Daudjee &amp; Salem</a> established “classic&#x2F;strong” and “generalized&#x2F;weak” SI levels. <a href="https://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya</a>’s formalism for SI assumes that we take as given specific choices made by the scheduler: whether a history is SI or not depends on what the scheduler does. This gives rise to a range of possible snapshot isolations depending on how the scheduler is implemented.</p>
<p>如果将SI解释为强制事务总是选择一个高于每个已提交事务的提交时间的开始时间，那么快照隔离显然比序列化有优势：它约束了实际时间，但是序列化没有。</p>
<p>If one interprets SI as enforcing that transactions always choose a start time higher than every committed transaction’s commit time, then snapshot isolation is clearly incomparable to serializability: it forces a real-time order which serializability does not.</p>
<p>在与Bailis、Crooks、Fekete、Hellerstein、Sutra和Shapiro进行了广泛的讨论之后，Jepsen采用更宽松的方式解释快照隔离：SI禁止事务见形成依赖环，这些事务包含多个相邻的读写反依赖边。在这种广义定义下，快照隔离严格弱于序列化。此定义与更广泛的实现集兼容。</p>
<p>Following extensive conversation with Bailis, Crooks, Fekete, Hellerstein, Sutra, and Shapiro, Jepsen interprets snapshot isolation in a more relaxed way: we take SI to prohibit dependency cycles between transactions which contain more than one adjacent read-write anti-dependency edge. Under this generalized definition, snapshot isolation is strictly weaker than serializability. This definition is compatible with a broader set of implementations.</p>
<h4 id="Formally"><a href="#Formally" class="headerlink" title="Formally"></a><a href="https://jepsen.io/consistency/models/snapshot-isolation#formally">Formally</a></h4><p>Berenson、Bernstein等<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">首次定义的快照隔离</a>：</p>
<blockquote>
<p>…将事务开始读取已提交的数据快照中的数据时的时间戳，作为开始时间戳。此时间戳可以是事务首次读取数据之前的任何时间。只要自开始时间戳以来的快照数据是可维护的，那运行在快照隔离中的事务的读操作将永远不会被阻塞。事务的写操作（更新、插入和删除）也会反映到快照中，如果事务再次访问（读取或更新）数据，则将再次读取该快照。开始时间戳之后的其他事务的更新操作，对于本事务是不可见的。</p>
<p>当事务T1准备提交时，它将获得一个提交时间戳，该时间戳将大于任何现有的开始时间戳或提交时间戳。只有不存在符合下面条件的事务T2的情况下，T1才能提交成功：T2的提交时间戳在T1执行期间，即[开始时间戳，提交时间戳]期间，并且T2和T1对同一份数据进行了写操作。如果存在这样的T2，那T1将会中止。这种被称为First-committer-wins的特性，防止了更新丢失的现象（现象P4）。T1提交后，其更新对于开始时间戳大于T1提交时间戳的所有事物都可见了。</p>
</blockquote>
<p>如果想要增加单个进程内的顺序，或者实际时间上的顺序，可以对开始时间戳和提交时间戳增加约束。</p>
<p>Berenson, Bernstein, et al <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">first defined snapshot isolation</a> in terms of an abstract algorithm:</p>
<blockquote>
<p>… each transaction reads reads data from a snapshot of the (committed) data as of the time the transaction started, called its Start-Timestamp. This time may be any time before the transaction’s first Read. A transaction running in Snapshot Isolation is never blocked attempting a read as long as the snapshot data from its Start-Timestamp can be maintained. The transaction’s writes (updates, inserts, and deletes) will also be reflected in this snapshot, to be read again if the transaction accesses (i.e., reads or updates) the data a second time. Updates by other transactions active after the transaction Start-Timestamp are invisible to the transaction.</p>
<p>When the transaction T1 is ready to commit, it gets a Commit-Timestamp, which is larger than any existing Start-Timestamp or Commit-Timestamp. The transaction successfully commits only if no other transaction T2 with a Commit-Timestamp in T1’s execution interval [StartTimestamp, Commit-Timestamp] wrote data that T1 also wrote. Otherwise, T1 will abort. This feature, called First-committer-wins prevents lost updates (phenomenon P4). When T1 commits, its changes become visible to all transactions whose Start-Timestamps are larger than T1‘s Commit-Timestamp.</p>
</blockquote>
<p>有关基于抽象执行的合理直观的正式定义，请参阅Cerone、Bernardi和Gotsman的<a href="http://drops.dagstuhl.de/opus/volltexte/2015/5375/pdf/15.pdf">具有原子可见性的事务一致性模型框架</a>，它将快照隔离指定为四个属性的组合：</p>
<ul>
<li>内部一致性：在事务内部，读操作能观察该事务最近的写操作（如果有）；</li>
<li>外部一致性：T1中针对某个对象的读操作（之前没有写操作）能够观察到T0写入的状态，即T0对T1是可见的（没有其他的事务对该对象进行了写入）；</li>
<li>前缀：事务能被所有节点以相同的顺序被观察到；</li>
<li>无冲突：如果另个事务写同一个对象，那其中一个必须对另一个是可见的。</li>
</ul>
<p>有关基于状态的正式定义，请参阅Crooks、Pu、Alvisi和Clement：<a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seing-tr.pdf">眼见为实：以客户机为中心的数据库隔离规范</a>。</p>
<p>For a reasonably intuitive formalization based on abstract executions, see Cerone, Bernardi, &amp; Gotsman’s <a href="http://drops.dagstuhl.de/opus/volltexte/2015/5375/pdf/15.pdf">A Framework for Transactional Consistency Models with Atomic Visibility</a>, which specifies snapshot isolation as a combination of four properties:</p>
<ul>
<li>Internal consistency: within a transaction, reads observe that transaction’s most recent writes (if any)</li>
<li>External consistency: reads <em>without</em> a preceding write in transaction T1 must observe the state written by a transaction T0, such that T0 is visible to T1, and no more recent transaction wrote to that object.</li>
<li>Prefix: transactions become visible to all nodes in the same order</li>
<li>NoConflict: if two transactions write to the same object, one must be visible to the other.</li>
</ul>
<p>For a state-based formalization, see Crooks, Pu, Alvisi, &amp; Clement: <a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seeing-tr.pdf">Seeing is Believing: A Client-Centric Specification of Database Isolation</a>.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>14Cursor Stability</title>
    <url>/2022/06/02/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/14Cursor%20Stability/</url>
    <content><![CDATA[<h3 id="Cursor-Stability"><a href="#Cursor-Stability" class="headerlink" title="Cursor Stability"></a>Cursor Stability</h3><p>游标稳定性是一种一致性模型，它是读已提交的增强版，可以防止updates的丢失。它引入了游标（cursor）的概念，用于指向事务访问的特定对象。事务中可以包含多个游标，当事务中使用游标读取某个对象时，在释放游标或提交事务之前，任何其他事务都不能修改该对象。</p>
<span id="more"></span>
<p>Cursor stability is a consistency model which strengthens <a href="https://jepsen.io/consistency/models/read-committed">read committed</a> by preventing <em>lost updates</em>. It introduces the concept of a <em>cursor</em>, which refers to a particular object being accessed by a transaction. Transactions may have multiple cursors. When a transaction reads an object using a cursor, that object cannot be modified by any other transaction until the cursor is released, or the transaction commits.</p>
<p>游标稳定性可以防止更新丢失，即事务T1读取、修改和写回对象x，但另一个事务T2在T1读取x后更新x，这就导致T2的更新丢失了。</p>
<p>This prevents lost updates, where transaction T1 reads, modifies, and writes back an object <em>x</em>, but a different transaction T2 transaction also updates <em>x</em> after T1 read <em>x</em>—causing T2’s update to be effectively lost.</p>
<p>游标稳定性是一种“事务性”模型：操作（通常称为“事务”）可以包括按顺序执行的几个基本子操作。它也是一个“多对象”属性：操作可以作用于系统中的多个对象。</p>
<p>Cursor stability is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. It is also a <em>multi-object</em> property: operations can act on multiple objects in the system.</p>
<p>游标稳定性不是完全可用的；在网络分区的情况下，一些或所有节点可能无法取得进展。为了获得完全可用性，以允许更新丢失为代价，可以考虑<a href="https://jepsen.io/consistency/models/read-committed">读已提交</a>. 如果需要更强的一致性，即确保每个记录读取对象的稳定性，而非特定的游标的稳定性，可以考虑<a href="https://jepsen.io/consistency/models/repeatable-read">可重复读</a>.</p>
<p>Cursor stability cannot be totally available; in the presence of network partitions, some or all nodes may be unable to make progress. For total availability, at the cost of allowing lost updates, consider <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>. For a stronger level, which ensures the stability of every record read, rather than cursors, consider <a href="https://jepsen.io/consistency/models/repeatable-read">repeatable read</a>.</p>
<p>需要注意的是，游标稳定性没有任何的实际时间上的约束。如果进程A完成了写操作w，然后进程B开始读操作r，r不一定能观察到w。如果想要实际时间上的约束，考虑<a href="https://jepsen.io/consistency/models/strict-serializable">严格序列化</a>。</p>
<p>Note that cursor stability does not impose any real-time constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. For a transactional model that provides real-time constraints, consider <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>.</p>
<p>此外，游标稳定性也不保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</p>
<p>Moreover, cursor stability does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.</p>
<h4 id="Formally"><a href="#Formally" class="headerlink" title="Formally"></a><a href="https://jepsen.io/consistency/models/cursor-stability#formally">Formally</a></h4><p><a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya对事务隔离级别的正式定义中</a>，通过禁止两种现象来定义光标稳定性：</p>
<ul>
<li>G1：见下面；</li>
<li>G-cursor(x)：针对单个对象x的有向序列图中，必须包含一个反依赖环，以及至少一个写依赖的边；</li>
</ul>
<p><a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s formalization of transactional isolation levels</a> defines cursor stability in terms of two prohibited phenomena:</p>
<ul>
<li>G1: See below.</li>
<li>G-cursor(x): the directed serialization graph, restricted to a single object <em>x</em>, contains an anti-dependency cycle and at least one write-dependency edge.</li>
</ul>
<p>G1包含了三种禁止的现象：</p>
<ul>
<li>G1a（中止读取）：事务A中能观察被中止事务B修改的对象（可能通过谓词）。说白了，就是事务必须“提交”后，我们才能读取它们。</li>
<li>G1b（中间读取）：事务A中能观察到其他事务B修改的对象（可能通过谓词），而且事务B中对该对象还会有后续的修改。说白了，就是事务必须在完成之后，我们才能读取它；</li>
<li>G1c（环形信息流）：事务的有向序列化图包含完全由依赖边组成的有向环。直白的说，就是如果T1收到了T2的影响，那T2就不能受T1的影响；</li>
</ul>
<p>G1 encompasses three disallowed phenomena:</p>
<ul>
<li>G1a (Aborted Reads): A transaction observes an object (perhaps via a predicate) modified by an aborted transaction. Intuitively, transactions have to <em>commit</em> for us to read them.</li>
<li>G1b (Intermediate Reads): A transaction observes an object (perhaps via a predicate) modified by a transaction which was not that transaction’s final modification of that object. Intuitively, transactions have to <em>finish</em> before we can read them,</li>
<li>G1c (Circular Information Flow): the Directed Serialization Graph of transactions contains a directed cylce consisting entirely of dependency edges. Intuitively, if transaction T1 is affected by T2, T2 can’t be affected by T1.</li>
</ul>
<p>因为游标稳定性要强于读已提交，所以它还禁止ANSI定义的如下现象：</p>
<ul>
<li>P0（脏写）：w1（x）…w2（x）</li>
<li>P1（脏读）：w1（x）…r2（x）</li>
</ul>
<p>但是允许下面的现象：</p>
<ul>
<li>P2（不可重复读）：r1（x）…w2（x）</li>
<li>P3（幻读）：r1（P）…w2（P中的y）</li>
</ul>
<p>这里w表示写，r表示读，下标表示执行操作的事务。…表示除了commit或abort之外的子操作。p表示谓词。对于不使用游标的对象，有可能会发生“不可重复读”。</p>
<p>Since cursor stability is strictly stronger than read committed, it also prohibits the ANSI phenomena:</p>
<ul>
<li>P0 (Dirty Write): w1(x) … w2(x)</li>
<li>P1 (Dirty Read): w1(x) … r2(x)</li>
</ul>
<p>but allows:</p>
<ul>
<li>P2 (Fuzzy Read): r1(x) … w2(x)</li>
<li>P3 (Phantom): r1(P) … w2(y in P)</li>
</ul>
<p>In this notation, <em>w</em> denotes a write, <em>r</em> denotes a read, and subscripts indicate the transaction which executed that operation. … indicates a series of micro-operations <em>except</em> for a commit or abort. P indicates a predicate. Fuzzy reads are possible where cursors are not used.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>15Monotonic Atomic View</title>
    <url>/2022/06/11/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/15Monotonic%20Atomic%20View/</url>
    <content><![CDATA[<h3 id="Monotonic-Atomic-View-单调原子视图"><a href="#Monotonic-Atomic-View-单调原子视图" class="headerlink" title="Monotonic Atomic View     单调原子视图"></a>Monotonic Atomic View     单调原子视图</h3><p>单调原子视图是一种一致性模型，它比读已提交要强一些，防止事务读到之前提交的事务部分（非全部）结果。它表示ACID中的原子约束，即事务中的所有操作要么全生效，要么全不生效。只要T1中的某个写操作能被T2观察到，那T1的所有操作的结果都对T2都是可见的。这在强制外键约束和确保索引和物化视图反映其底层对象方面尤其有用。</p>
<span id="more"></span>
<p>Monotonic atomic view is a consistency model which strengthens <a href="https://jepsen.io/consistency/models/read-committed">read committed</a> by preventing transactions from observing some, but not all, of a previously committed transaction’s effects. It <a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">expresses</a> the atomic constraint from ACID that all (or none) of a transaction’s effects should take place. Once a write from transaction T1 is observed by transaction T2, then <em>all</em> effects of T1 should be visible to T2. This is particularly helpful in enforcing foreign key constraints and ensuring indices and materialized views reflect their underlying objects.</p>
<p>单调原子视图是一种“事务”模型：操作（通常称为“事务”）可以包括按顺序执行的几个基本子操作。它也是一个“多对象”属性：操作可以作用于系统中的多个对象。</p>
<p>Monotonic atomic view is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. It is also a <em>multi-object</em> property: operations can act on multiple objects in the system.</p>
<p>单调原子视图可以<a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">完全可用</a>：在存在网络分区的情况下，每个节点都可以取得进展。如果愿意牺牲完全可用性，考虑使用<a href="https://jepsen.io/consistency/models/repeatable-read">可重复读取</a>和<a href="https://jepsen.io/consistency/models/snapshot-isolation">快照隔离</a>提供更有力的保障。</p>
<p>Monotonic atomic view can be <a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">totally available</a>: in the presence of network partitions, every node can make progress. If you are willing to sacrifice total availability, both <a href="https://jepsen.io/consistency/models/repeatable-read">repeatable read</a> and <a href="https://jepsen.io/consistency/models/snapshot-isolation">snapshot isolation</a> offer stronger guarantees.</p>
<p>注意，单调原子视图不会施加任何实际时间上的约束。如果进程A完成写操作w，进程B开始读操作r，r不一定保证能看到w的结果。如果需要提供实时约束的事务模型，请考虑<a href="https://jepsen.io/consistency/models/strict-serializable">严格序列化</a>.</p>
<p>Note that monotonic atomic view does not impose any real-time constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. For a transactional model that provides real-time constraints, consider <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>.</p>
<p>此外，单调原子视图不保证单个进程内的事务顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</p>
<p>Moreover, monotonic atomic view does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.</p>
<p>类似于<a href="https://jepsen.io/consistency/models/serializable">串行化</a>，单调原子视图允许重排序。比如，一个单调原子视图的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</p>
<p>Like <a href="https://jepsen.io/consistency/models/serializable">serializability</a>, monotonic atomic view allows pathological orderings. For instance, a monotonic atomic view database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very <em>end</em> of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations don’t seem to take advantage of these optimization opportunities.</p>
<h4 id="Formally"><a href="#Formally" class="headerlink" title="Formally"></a><a href="https://jepsen.io/consistency/models/monotonic-atomic-view#formally">Formally</a></h4><p>单调原子视图在文献中并不常见，这个概念由Bailis、Davidson、Fekete等人在<a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">《高可用交易：优点和局限性》</a>中提出. 论文中将其描述为：</p>
<blockquote>
<p>在MAV（Monotonic Atomic View）下，一旦一个事务Ti的一些操作结果被另一个事务Tj观察到，此后，Ti的所有操作结果都能被Tj观察到。也就是说，如果事务Tj读取事务Ti写入的对象的某个版本，则Tj稍后读取的对象无法返回Ti应用了更高版本的值。</p>
</blockquote>
<p>Monotonic Atomic View is not commonly discussed in the literature, but was coined by Bailis, Davidson, Fekete, et al in <a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">Highly Available Transactions: Virtues and Limitations</a>. They describe it as:</p>
<blockquote>
<p>Under MAV, once some of the effects of a transaction Ti are observed by another transaction Tj, thereafter, all effects of Ti are observed by Tj. That is, if a transaction Tj reads a version of an object that transaction Ti wrote, then a later read by Tj cannot return a value whose later version is installed by Ti.</p>
</blockquote>
<p>根据<a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya对事务隔离级别的正式定义</a>，单调原子视图禁止：</p>
<ul>
<li>G1b（中间读取）：事务A（可能通过谓词）观察到由事务B修改的对象，而事务B后续还会对该对象进行修改。说白了就是，在能读取之前，事务必须是已完成的。</li>
</ul>
<p>由于单调原子视图强于读已提交，因此它也禁止了下面的ANSI现象：</p>
<ul>
<li>P0（脏写）：w1（x）…w2（x）</li>
<li>P1（脏读）：w1（x）…r2（x）</li>
</ul>
<p>但允许：</p>
<ul>
<li>P2（不可重复读）：r1（x）…w2（x）</li>
<li>P3（幻读）：r1（P）…w2（P中的y）</li>
</ul>
<p>这里w表示写，r表示读，下标表示执行操作的事务。”…”表示除了commit或abort之外的子操作。p表示谓词。</p>
<p>In terms of <a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s formalization of transactional isolation levels</a>, monotonic atomic view prohibits:</p>
<ul>
<li>G1b (Intermediate Reads): A transaction observes an object (perhaps via a predicate) modified by a transaction which was not that transaction’s final modification of that object. Intuitively, transactions have to <em>finish</em> before we can read them,</li>
</ul>
<p>And since monotonic atomic view is strictly stronger than read committed, it also prohibits the ANSI phenomena…</p>
<ul>
<li>P0 (Dirty Write): w1(x) … w2(x)</li>
<li>P1 (Dirty Read): w1(x) … r2(x)</li>
</ul>
<p>but allows:</p>
<ul>
<li>P2 (Fuzzy Read): r1(x) … w2(x)</li>
<li>P3 (Phantom): r1(P) … w2(y in P)</li>
</ul>
<p>Here <em>w</em> denotes a write, <em>r</em> denotes a read, and subscripts indicate the transaction which executed that operation. The notation “…” indicates a series of micro-operations <em>except</em> for a commit or abort. P indicates a predicate.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>16Read Committed</title>
    <url>/2022/06/15/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/16Read%20Committed/</url>
    <content><![CDATA[<h3 id="Read-Committed-读已提交"><a href="#Read-Committed-读已提交" class="headerlink" title="Read Committed     读已提交"></a>Read Committed     读已提交</h3><p>读已提交是这样的一致性模型，它加强了<a href="https://jepsen.io/consistency/models/read-uncommitted">Read uncommitted（读未提交）</a>，防止了脏读：不允许事务观察到未提交事务的写操作。</p>
<span id="more"></span>
<p>Read committed is a consistency model which strengthens <a href="https://jepsen.io/consistency/models/read-uncommitted">read uncommitted</a> by preventing <em>dirty reads</em>: transactions are not allowed to observe writes from transactions which do not commit.</p>
<p>读已提交是一种“事务”模型：操作（通常称为“事务”）可以包括按顺序执行的几个原语子操作。它也是一个“多对象”属性：操作可以作用于系统中的多个对象。</p>
<p>Read committed is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. It is also a <em>multi-object</em> property: operations can act on multiple objects in the system.</p>
<p>Read committed可以是<a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">完全可用</a>的：存在网络分区的情况下，每个节点都可以取得进展。在不牺牲可用性的情况下，还可以通过选择更强的单调原子视图来确保能同时观察到事务的效果. 如果想提高性能从而允许脏读，可以考虑<a href="https://jepsen.io/consistency/models/read-uncommitted">读未提交</a>.</p>
<p>Read committed can be <a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">totally available</a>: in the presence of network partitions, every node can make progress. Without sacrificing availability, you can also ensure that transactions effects are observed <em>together</em> by choosing the stronger <a href="https://jepsen.io/consistency/models/monotonic-atomic-view">monotonic atomic view</a>. For performance, you may wish to allow dirty reads, and relax to <a href="https://jepsen.io/consistency/models/read-uncommitted">read uncommitted</a>.</p>
<p>请注意，读已提交没有任何实际时间上的约束。如果进程A完成写操作w，然后进程B开始读操作r，则r不一定保证能观察到w。如果需要实际时间约束的事务模型，可以考虑<a href="https://jepsen.io/consistency/models/strict-serializable">严格序列化</a>.</p>
<p>Note that read committed does not impose any real-time constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. For a transactional model that provides real-time constraints, consider <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>.</p>
<p>此外，读已提交也不保证单个进程上的多个事务之间的顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</p>
<p>Moreover, read committed does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.</p>
<p>类似于<a href="https://jepsen.io/consistency/models/serializable">串行化</a>，读已提交允许重排序。比如，一个读已提交的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</p>
<p>Like <a href="https://jepsen.io/consistency/models/serializable">serializability</a>, read committed allows pathological orderings. For instance, a read commmitted database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very <em>end</em> of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations don’t seem to take advantage of these optimization opportunities.</p>
<h4 id="Formally"><a href="#Formally" class="headerlink" title="Formally"></a><a href="https://jepsen.io/consistency/models/read-committed#formally">Formally</a></h4><p><a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999规范</a>通过禁止现象P1来指定读已提交：</p>
<blockquote>
<p>P1（“脏读”）：SQL事务T1修改了某行。然后，SQL事务T2在T1执行提交之前读取该行。如果T1随后执行回滚，T2将读取到从未提交过的行，可以认为该行从未存在过。</p>
</blockquote>
<p>The <a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999 spec</a> specifies repeatable read by disallowing phenomenon P1:</p>
<blockquote>
<p>P1 (“Dirty read”): SQL-transaction T1 modifies a row. SQL-transaction T2 then reads that row before T1 performs a COMMIT. If T1 then performs a ROLLBACK, T2 will have read a row that was never committed and that may thus be considered to have never existed.</p>
</blockquote>
<p>然而，正如<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson、Bernstein等人观察到的</a>，ANSI规范允许多种解释，其中一种解释（“异常解释”）仍然允许“可串行化”系统的非串行化历史。相反，我们更喜欢<a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya对事务隔离级别的正式定义</a>，它提供了预防性解释的简明定义。在此模型中，读已提交禁止：</p>
<ul>
<li>P0（脏写）：w1（x）…w2（x）</li>
<li>P1（脏读）：w1（x）…r2（x）</li>
</ul>
<p>但允许：</p>
<ul>
<li>P2（不可重复读）：r1（x）…w2（x）</li>
<li>P3（幻读）：r1（P）…w2（P中的y）</li>
</ul>
<p>这里w表示写，r表示读，下标表示执行操作的事务。”…”表示除了commit或abort之外的子操作。p表示谓词。</p>
<p>基于状态的正式定义，请参阅Crooks、Pu、Alvisi和Clement：<a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seing-tr.pdf">眼见为实：以Client为中心的数据库隔离规范</a>。</p>
<p>However, as <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson, Bernstein, et al observed</a>, the ANSI specification allows multiple intepretations, and one of those interpretations (the “anomaly interpretation) still admits nonserializable histories for “serializable” systems. Instead, we prefer <a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s formalization of transactional isolation levels</a>, which provides a concise definition of the preventative interpretation. In this model, read committed prohibits:</p>
<ul>
<li>P0 (Dirty Write): w1(x) … w2(x)</li>
<li>P1 (Dirty Read): w1(x) … r2(x)</li>
</ul>
<p>but allows:</p>
<ul>
<li>P2 (Fuzzy Read): r1(x) … w2(x)</li>
<li>P3 (Phantom): r1(P) … w2(y in P)</li>
</ul>
<p>Here <em>w</em> denotes a write, <em>r</em> denotes a read, and subscripts indicate the transaction which executed that operation. The notation “…” indicates a series of micro-operations <em>except</em> for a commit or abort. P indicates a predicate.</p>
<p>For a state-based formalization, see Crooks, Pu, Alvisi, &amp; Clement: <a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seeing-tr.pdf">Seeing is Believing: A Client-Centric Specification of Database Isolation</a>.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>17Read Uncommitted</title>
    <url>/2022/06/25/%E5%88%86%E5%B8%83%E5%BC%8F/%E4%B8%80%E8%87%B4%E6%80%A7/jepsen/17Read%20Uncommitted/</url>
    <content><![CDATA[<h3 id="Read-Uncommitted-读未提交"><a href="#Read-Uncommitted-读未提交" class="headerlink" title="Read Uncommitted    读未提交"></a>Read Uncommitted    读未提交</h3><p>读未提交是这样的一致性模型，它禁止脏写，即两个事务在提交之前同时修改同一对象。在ANSI SQL规范中，读未提交是默认的，也是最宽松的一致性模型，它允许所有行为。然而，基于Berenson等人的<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">讨论</a>，它实际上禁止了脏写。</p>
<span id="more"></span>
<p>Read uncommitted is a consistency model which prohibits <em>dirty writes</em>, where two transactions modify the same object concurrently before committing. In the ANSI SQL specification, read uncommitted is presumed to be the default, most permissive consistency model, allowing all behaviors; however, Berenson et al <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">argued</a> that it should, in fact, prohibit dirty writes.</p>
<p>读未提交是一种“事务性”模型：操作（通常称为“事务”）可能涉及按顺序执行的几个基本子操作。它也是一个“多对象”属性：操作可以作用于系统中的多个对象。</p>
<p>Read uncommitted is a <em>transactional</em> model: operations (usually termed “transactions”) can involve several primitive sub-operations performed in order. It is also a <em>multi-object</em> property: operations can act on multiple objects in the system.</p>
<p>读未提交可以是<a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">完全可用</a>：存在网络分区的情况下，每个节点都可以取得进展。在不牺牲可用性的情况下，还可以通过选择更强的读取已提交来确保事务不会读取未提交状态。</p>
<p>Read uncommitted can be <a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/10/hat-vldb2014.pdf">totally available</a>: in the presence of network partitions, every node can make progress. Without sacrificing availability, you can also ensure that transactions do not read uncommitted state by choosing the stronger <a href="https://jepsen.io/consistency/models/read-committed">read committed</a>.</p>
<p>请注意，读未提交没有任何实际时间上的约束。如果进程A完成写操作w，然后进程B开始读操作r，则r不一定保证能观察到w。如果需要实际时间约束的事务模型，可以考虑<a href="https://jepsen.io/consistency/models/strict-serializable">严格序列化</a>.</p>
<p>Note that read uncommitted does not impose any real-time constraints. If process A completes write <em>w</em>, then process B begins a read <em>r</em>, <em>r</em> is not necessarily guaranteed to observe <em>w</em>. For a transactional model that provides real-time constraints, consider <a href="https://jepsen.io/consistency/models/strict-serializable">strict serializability</a>.</p>
<p>此外，读未提交也不保证单个进程上的多个事务之间的顺序。一个进程可能之前还能观察到一个写操作，而在后续的事务中却无法再次观察到同一个写操作。事实上，如果发生在不同的事务中，那进程都可能无法观察到自己之前的写操作。</p>
<p>Moreover, read uncommitted does not require a per-process order between transactions. A process can observe a write, then <em>fail</em> to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.</p>
<p>类似于<a href="https://jepsen.io/consistency/models/serializable">串行化</a>，读未提交允许重排序。比如，一个读已提交的数据库中，如果读操作发生在time 0上，那这些读操作可能返回的是空状态；它可能会丢弃只写事务，因为允许把这些事务排在操作历史的最后，即排在所有读操作的后面；像自增这样的操作也可以被丢弃，即允许自增的结果永远无法被观察到。好在大多数实现一般不会再用这种程度的优化措施。</p>
<p>Like <a href="https://jepsen.io/consistency/models/serializable">serializability</a>, read uncommitted allows pathological orderings. For instance, a read uncommmitted database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very <em>end</em> of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations don’t seem to take advantage of these optimization opportunities.</p>
<h4 id="Formally"><a href="#Formally" class="headerlink" title="Formally"></a><a href="https://jepsen.io/consistency/models/read-uncommitted#formally">Formally</a></h4><p><a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999规范</a>中，读未提交的行为基本上没有任何限制。任何奇怪的行为都是允许的。</p>
<p>然而，正如<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson、Bernstein等人观察到的</a>，ANSI规范允许多种解释，其中一种解释（“异常解释”）仍然允许“可串行化”系统的非串行化历史。相反，我们更喜欢<a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya对事务隔离级别的正式定义</a>，它提供了预防性解释的简明定义。在此模型中，读未提交禁止：</p>
<ul>
<li>P0（脏写）：w1（x）…w2（x）</li>
</ul>
<p>但允许：</p>
<ul>
<li>P1（脏读）：w1（x）…r2（x）</li>
<li>P2（不可重复读）：r1（x）…w2（x）</li>
<li>P3（幻读）：r1（P）…w2（P中的y）</li>
</ul>
<p>这里w表示写，r表示读，下标表示执行操作的事务。”…”表示除了commit或abort之外的子操作。p表示谓词。</p>
<p>基于状态的正式定义，请参阅Crooks、Pu、Alvisi和Clement：<a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seing-tr.pdf">眼见为实：以Client为中心的数据库隔离规范</a>。</p>
<p>The <a href="http://web.cecs.pdx.edu/~len/sql1999.pdf">ANSI SQL 1999 spec</a> places essentially no constraints on the behavior of read uncommitted. Any and all weird behavior is fair game.</p>
<p>However, as <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf">Berenson, Bernstein, et al observed</a>, the ANSI specification allows multiple intepretations, and one of those interpretations (the “anomaly interpretation) still admits nonserializable histories for “serializable” systems. Instead, we prefer <a href="http://pmg.csail.mit.edu/papers/adya-phd.pdf">Adya’s formalization of transactional isolation levels</a>, which provides a concise definition of the preventative interpretation. In this model, read uncommitted prohibits:</p>
<ul>
<li>P0 (Dirty Write): w1(x) … w2(x)</li>
</ul>
<p>but allows:</p>
<ul>
<li>P1 (Dirty Read): w1(x) … r2(x)</li>
<li>P2 (Fuzzy Read): r1(x) … w2(x)</li>
<li>P3 (Phantom): r1(P) … w2(y in P)</li>
</ul>
<p>Here <em>w</em> denotes a write, <em>r</em> denotes a read, and subscripts indicate the transaction which executed that operation. The notation “…” indicates a series of micro-operations <em>except</em> for a commit or abort. P indicates a predicate.</p>
<p>For a state-based formalization, see Crooks, Pu, Alvisi, &amp; Clement: <a href="http://www.cs.utexas.edu/~ncrooks/2017-podc-seeing-tr.pdf">Seeing is Believing: A Client-Centric Specification of Database Isolation</a>.</p>
]]></content>
      <categories>
        <category>分布式</category>
        <category>一致性</category>
        <category>jepsen</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>DDIA笔记-01可靠可扩展与可维护的应用系统</title>
    <url>/2022/09/23/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/01%E5%8F%AF%E9%9D%A0%E5%8F%AF%E6%89%A9%E5%B1%95%E4%B8%8E%E5%8F%AF%E7%BB%B4%E6%8A%A4%E7%9A%84%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>设计一个数据密集型系统的核心目标是：可靠、可扩展和可维护。</p>
<ul>
<li>可靠性：即使发生了某些错误，系统仍可以继续正常工作。需要设计<font color=red>容错</font>机制来避免从<font color=red>故障</font>引发系统<font color=red>失效</font>。要保证可靠性：<ul>
<li>提供合理的抽象，仔细设计 API；</li>
<li>对最容易出错的地方、容易引发故障的接口进行隔离；</li>
<li>充分的测试；</li>
<li>出现故障时有快速的恢复机制；</li>
<li>日常运维有详细清晰的监控子系统；</li>
<li>科学的培训和管理流程；</li>
</ul>
</li>
<li>可扩展性用于描述系统应对负载增加能力。在描述应对负载的能力时，首先要能描述负载和描述能力（性能），然后才去思考如何应对负载：<ul>
<li>描述负载：比如每秒向 Web 服务器发出的请求数；数据库中的读写比率；聊天室中同时活跃的用户数量等；</li>
<li>描述性能：<ul>
<li>吞吐量（throughput）：每秒可处理的单位数据量，通常记为QPS。</li>
<li>响应时间（response time）：从用户侧观察到的发出请求到收到回复的时间。</li>
<li>延迟（latency）：延迟常和响应时间混用指代响应时间；但严格来说，<font color=red>延迟只是指请求过程中排队等待时间</font>，虽然其在响应时间中一般占大头；但只有处理请求是瞬时完成时，延迟才能等同于响应时间。</li>
<li>针对响应时间而言，<font color=red>以百分位点而非平均值来衡量更合理。</font>比如p95意味着95％的请求能在阈值内完成。</li>
</ul>
</li>
<li>应对负载：<ul>
<li>对于快速增长的系统，应考虑如何设计，才能应对多出一个数量级的负载；</li>
<li>应对负载通常需要在垂直扩展（升级到更强大的机器）和水平扩展（将负载分布到多个小的机器）做取舍；</li>
<li>通常做法是：先采用垂直扩展策略，直到高扩展性或高可用性的要求迫使不得不做水平扩展。</li>
<li>没有通用架构设计来应对负载。对于特定应用，扩展能力好的架构通常会<font color=red>做出某些假设</font>，然后有针对性地优化设计。</li>
</ul>
</li>
</ul>
</li>
<li>可维护性：可以从软件设计时开始考虑，尽可能较少维护期间的麻烦。可维护性要关注三个设计原则：<ul>
<li>可运维性：方便运维团队来保持系统平稳运行。比如：提供对系统运行时行为和内部的可观测性，方便监控；提供良好的文档和易于理解的操作模式；提供良好的默认配置，允许管理员在需要时方便地修改默认值；尝试自我修复，在需要时让管理员手动控制系统状态；行为可预测，减少意外发生。</li>
<li>简单性：简化系统设计。主要意味着<font color=red>消除意外方面的复杂性</font>。消除意外复杂性最好手段之一是<font color=red>抽象</font>。一个好的设计抽象可以隐藏大量的实现细节，并对外<font color=red>提供干净、易懂的接口</font>。</li>
<li>可演化性：能轻松地对系统进行改进，并根据需求变化将其适配到非典型场景。项目管理上的敏捷开发适用于小系统；可演化和简单性与抽象性密切相关：简单易懂的系统往往比复杂的系统更容易修改。</li>
</ul>
</li>
</ul>
<hr>
<p>当今许多新型应用都属于<font color=red>数据密集型</font>（data-intensive），而不是计算密集型（compute-intensive）。对于这些类型应用，CPU的处理能力往往不是第一限制性因素，关键在于数据量、数据的复杂度及数据的快速多变性。</p>
<p>数据密集型应用通常是基于标准模块构建而成，每个模块负责单一的常用功能。例如，许多应用系统都包含以下模块：</p>
<ul>
<li>数据库：存储数据，这样之后应用可以再次访问。</li>
<li>高速缓存：缓存那些复杂或操作代价昂贵的结果，以加快下一次访问。</li>
<li>索引：用户可以按关键字搜索数据并支持各种过滤。</li>
<li>流式处理：持续发送消息至另一个进程，处理采用异步方式。</li>
<li>批处理：定期处理大量的累积数据。</li>
</ul>
<p>构建某个特定应用时，我们总是需要弄清楚哪些组件、哪些方法最适合自己，并且当单个组件无法满足需求而必须组合使用时，总要面临更多的技术挑战。本章，我们将首先探讨所关注的<font color=red>核心设计目标：可靠、可扩展与可维护的数据系统。</font></p>
<h3 id="认识数据系统"><a href="#认识数据系统" class="headerlink" title="认识数据系统"></a>认识数据系统</h3><p>通常将数据库、队列、高速缓存等视为不同类型的系统。他们有着截然不同的访问模式，但它们可以归为一大类，即“数据系统”（data system）。</p>
<p><font color=red>应用系统组合使用多个组件来提供服务，而对外提供服务的界面或者API会隐藏内部实现细节。这样我们基于一个个较小的通用组件，构建成一个全新的专用数据系统。</font>所以，你既是一名应用开发者，也是一名数据系统设计师。</p>
<p>影响数据系统设计的因素有很多，本书将专注于对大多数软件系统都极为重要的几个问题：</p>
<ul>
<li>可靠性（Reliability）：当出现意外情况如硬件、软件故障、人为失误等，系统应可以继续正常运转，虽然性能可能有所降低，但确保功能正确。</li>
<li>可扩展性（Scalability）：随着规模的增长，例如数据量，流量或复杂性，系统应以合理的方式来匹配这种增长。</li>
<li>可维护性（Maintainability）：随着时间的推移，许多新人参与到系统开发和运维，以维护现有功能或适配新场景等，系统都应高效运转。</li>
</ul>
<h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>我们认为可靠性大致意味着：<font color=red>即使发生了某些错误，系统仍可以继续正常工作。</font></p>
<p>可能出错的事情称为错误（faults）或故障，系统可应对错误则称为容错（fault-tolerant）或者弹性（resilient）。</p>
<p>故障与失效（failure）不完全一致。故障通常被定义为组件偏离其正常规格，而失效意味系统作为一个整体停止，无法向用户提供所需的服务。我们不太可能将故障概率降低到零，因此通常<font color=red>设计容错机制来避免从故障引发系统失效。</font></p>
<h4 id="硬件故障"><a href="#硬件故障" class="headerlink" title="硬件故障"></a>硬件故障</h4><p>硬件故障比如硬盘崩溃，内存故障，电网停电，甚至有人误拔网线。</p>
<p>我们的第一反应通常是为硬件添加冗余来减少系统故障率。例如对磁盘配置RAID，服务器配备双电源等。当一个组件发生故障，冗余组件可以快速接管，之后再更换失效的组件。</p>
<p>但是，随着数据量和计算需求的增加，更多的应用可能运行在大规模机器上，随之而来的硬件故障率呈线性增长。因此，通过软件容错的方式来容忍多机失效成为新的手段，或者至少成为硬件容错的有力补充。</p>
<h4 id="软件错误"><a href="#软件错误" class="headerlink" title="软件错误"></a>软件错误</h4><p>我们通常认为硬件故障之间多是相互独立的：一台机器的磁盘故障并不意味着另一台机器的磁盘也要失效。</p>
<p>另一类故障则是系统内的软件问题。这些故障事先更加难以顶料，而且因为节点之间是由软件关联的，因而往往会导致更多的系统故障。</p>
<p>软件系统问题有时没有快速解决办法，而只能仔细考虑很多细节，包括认真检查依赖的假设条件与系统之间的交互，进行全面的测试，进程隔离，允许进程崩溃并自动重启，反复评沽，监控并分析生产环节的行为表现等。</p>
<h4 id="人为失误"><a href="#人为失误" class="headerlink" title="人为失误"></a>人为失误</h4><p><font color=red>我们假定人是不可靠的，那么该如何保证系统的可靠性呢？可以尝试结合以下多种方法：</font></p>
<ul>
<li>以最小出错的方式来设计系统。例如，精心设计的抽象层、API以及管理界面，使“做正确的事情”很轻松，但搞坏很复杂。但是，如果限制过多，人们就会想办法来绕过它，这会抵消其正面作用。因此解决之道在于很好的平衡。</li>
<li>想办法分离最容易出错的地方、容易引发故障的接口。特别是，提供一个功能齐全但非生产用的沙箱环境，使人们可以放心的尝试、体验，万一出现问题，不会影响真实用户。</li>
<li>充分的测试：从单元测试到全系统集成测试以及手动测试。自动化测试已被广泛使用，对于覆盖正常操作中很少出现的边界条件等尤为重要。</li>
<li>当出现人为失误时，提供快速的恢复机制以尽可能减少故障影响。例如，快速回滚配置改动，滚动发布新代码，并提供校验数据的工具。</li>
<li>设置详细清晰的监控子系统，包括性能指标和错误率。监控可以向我们发送告警信号并检查是否存在假设不成立或违反约束条件等。这些检测指标对于诊断问题也非常有用。</li>
<li>推行管理流程并加以培训。这很重要而且比较复杂。</li>
</ul>
<hr>
<h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>可扩展性是用来描述<font color=red>系统应对负载增加能力</font>的术语。讨论可扩展性通常要考虑这类问题：“如果系统以某种方式增长，我们应对这一增长的措施有哪些“，”我们该如何添加计算资源来处理额外的负载”。</p>
<h4 id="描述负载"><a href="#描述负载" class="headerlink" title="描述负载"></a>描述负载</h4><p>首先需要简洁地描述系统当前的负载，只有这样才能更好地讨论后续增长问题  (例如负载加倍会意味着什么)。负载可以用称为负载参数的若干数字来描述。参数的选择取决于系统的体系结构，它可能是WEB服务器的每秒请求处理次数，数据库中写入的比例，缓存命中率等。有时平均值很重要，而有时系统瓶颈来自于少数峰值。</p>
<h4 id="描述性能"><a href="#描述性能" class="headerlink" title="描述性能"></a>描述性能</h4><p>如何描述系统性能。在批处理系统如Hadoop中，我们通常关心吞吐量，即每秒可处理的记录条数，或者在某指定数据集上运行作业所需的总时间；而在线系统通常更看重服务的响应时间，即客户端从发送请求到接收响应之间的间隔。</p>
<p>即使是处理相同的请求，每次可能都会产生不同的响应时间。我们经常考察的是平均响应时间（严格来说，术语“平均值”通常被理解为算术平均值：给定n个值，将所有值相加，并除以n）。然而，如果想知道更典型的响应时间，<font color=red>平均值并不是合适的指标</font>，因为它掩盖了一些信息，无法告诉有多少用户实际经历了多少延迟。</p>
<p>最好<font color=red>使用百分位数</font>。如果将响应时间从最快到最慢排序，中位数就是列表中间的响应时间。例如，如果中位数响应时间为200ms，意味着有一半的请求响应不到200ms，而另一半请求响应需要更长的时间。中位数也称为50百分位数，有时缩写为p50。</p>
<p>为了弄清楚异常值有多糟糕，需要关注更大的百分位数如常见的第95, 99和99.9（缩写为p95, p99和p999）值。作为典型响应时间阈值，它们分别表示有95%, 99%或99.9%的请求响应时间快于阈值。</p>
<p><font color=red>排队延迟</font>往往在高百分数响应时间中影响很大。由于服务器并行处理的请求有限（例如，CPU内核数的限制），正在处理的少数请求可能会阻挡后续请求，这种情况有时被称为队头阻塞。即使后续请求可能处理很简单，但它阻塞在等待先前请求的完成，客户端将会观察到极慢的响应时间。因此，很重要的一点是要在客户端来测量响应时间。</p>
<p>因此，当测试可扩展性而人为产生负载时，负载生成端要独立于响应时间来持续发送请求。如果客户端在发送请求之前总是等待先前请求的完成，就会在测试中人为地缩短了服务器端的累计队列深度，这就带来了测试偏差。</p>
<h4 id="应对负载增加的方法"><a href="#应对负载增加的方法" class="headerlink" title="应对负载增加的方法"></a>应对负载增加的方法</h4><p>针对特定级别负载而设计的架构不太可能应付超出预设10倍的实际负载。如果服务处于快速增长阶段，那么需要认真考虑<font color=red>每增加一个数量级的负载，架构应如何设计。</font></p>
<p>现在谈论更多的是如何在<font color=red>垂直扩展（即升级到更强大的机器）和水平扩展（即将负载分布到多个更小的机器）</font>之间做取舍。在多台机器上分配负载也被称为无共享体系结构。在单台机器上运行的系统通常更简单，然而高端机器可能非常昂贵，且扩展水平有限，最终往往还是无法避免需要水平扩展。</p>
<p>把无状态服务分布然后扩展至多台机器相对比较容易，而有状态服务从单个节点扩展到分布式多机环境的复杂性会增加。因此，直到最近通常的做法一直是，将数据库运行在一个节点上（采用垂直扩展策略），直到高扩展性或高可用性的要求迫使不得不做水平扩展。然而，随着相关分布式系统专门组件和编程接口越来越好，未来分布式数据系统将成为标配。</p>
<p>超大规模的系统往往针对特定应用而高度定制，很难有一种通用的架构。对于特定应用来说，扩展能力好的架构通常会<font color=red>做出某些假设，然后有针对性地优化设计</font>，如哪些操作是最频繁的，哪些负载是少数情况。</p>
<h3 id="可维护性"><a href="#可维护性" class="headerlink" title="可维护性"></a>可维护性</h3><p>众所周知，软件的大部分成本并不在最初的开发阶段，而是在于整个生命周期内持续的投入，这包括维护与缺陷修复，监控系统来保持正常运行、故障排查、适配新平台、搭配新场景、技术缺陷的完善以及增加新功能等。</p>
<p>但是，换个角度，<font color=red>可以从软件设计时开始考虑，尽可能较少维护期间的麻烦</font>，避免造出容易过期的系统。为此，我们将特别关注软件系统的三个设计原则：</p>
<ul>
<li>可运维性：方便<font color=red>运维团队</font>来保持系统平稳运行。</li>
<li>简单性：简化系统复杂性，使<font color=red>新工程师</font>能够轻松理解系统。注意这与用户界面的简单性并不一样。</li>
<li>可演化性：<font color=red>后续工程师</font>能够轻松地对系统进行改进，并根据需求变化将其适配到非典型场景。</li>
</ul>
<p>实现上述这些目标也没有简单的解决方案。接下来，首先建立对这三个特性的理解。</p>
<h4 id="可运维性：运维更轻松"><a href="#可运维性：运维更轻松" class="headerlink" title="可运维性：运维更轻松"></a>可运维性：运维更轻松</h4><p>一个优秀的运维团队通常至少负责以下内容：</p>
<ul>
<li>监视系统的健康状况，并在服务出现异常状态时快速恢复服务。</li>
<li>追踪问题的原因，例如系统故障或性能下降。</li>
<li>保持软件和平台至最新状态，例如安全补丁方面。</li>
<li>了解不同系统如何相互影响，避免执行带有破坏性的操作。</li>
<li>预测未来可能的问题，并在问题发生之前解决（例如容量规划）。</li>
<li>建立用于部署、配置管理等良好的实践规范和工具包。</li>
<li>执行复杂的维护任务，例如将应用程序从一个平台迁移到另一个平台。</li>
<li>当配置更改时，维护系统的安全稳健。</li>
<li>制定流程来规范操作行为，并保持生产环境稳定。</li>
<li>保持相关知识的传承（如对系统理解），例如人员离职或者新员工加入等。</li>
</ul>
<p>良好的可操作性意味着使日常工作变得简单，使运维团队能够专注于高附加值的任务。数据系统设计可以在这方面贡献很多，包括：</p>
<ul>
<li>提供对系统运行时行为和内部的可观测性，方便监控。</li>
<li>支持自动化，与标准工具集成。</li>
<li>避免绑定特定的机器，这样在整个系统不间断运行的同时，允许机器停机维护。</li>
<li>提供良好的文档和易于理解的操作模式，诸如“如果我做了X，会发生Y”。</li>
<li>提供良好的默认配置，且允许管理员在需要时方便地修改默认值。</li>
<li>尝试自我修复，在需要时让管理员手动控制系统状态。</li>
<li>行为可预测，减少意外发生。</li>
</ul>
<h4 id="简单性：简化复杂度"><a href="#简单性：简化复杂度" class="headerlink" title="简单性：简化复杂度"></a>简单性：简化复杂度</h4><p>复杂性有各种各样的表现方式：状态空间的膨胀，模块紧耦合，令人纠结的相互依赖关系，不一致的命名和术语，为了性能而采取的特殊处理，为解决某特定问题而引入的特殊框架等。</p>
<p>复杂性使得维护变得越来越困难，最终会导致预算超支和开发进度滞后。对于复杂的软件系统，变更而进入</p>
]]></content>
      <categories>
        <category>数据密集型应用系统设计读书笔记</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>一致性</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
</search>
